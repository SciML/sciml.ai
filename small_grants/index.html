<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta property="og:title" content="SciML: Open Source Software for Scientific Machine Learning"> <meta property="og:description" content="Open Source Software for Scientific Machine Learning"> <meta property="og:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta property="og:url" content="https://sciml.ai"> <meta name="twitter:title" content=SciML > <meta name="twitter:description" content="Open Source Software for Scientific Machine Learning"> <meta name="twitter:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <!-- Favicon--> <link rel=icon  type="image/x-icon" href="assets/favicon.png" /> <!-- Bootstrap icons--> <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel=stylesheet  type="text/css" /> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css"> <!-- Google fonts--> <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet  type="text/css" /> <!-- Core theme CSS (includes Bootstrap)--> <link href="./css/styles.css" rel=stylesheet  /> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/styles.css"> <link rel=stylesheet  href="/css/hypertext.css"> <link rel=icon  href="/assets/favicon.png"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-2"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-90474609-2'); </script> <title>SciML Small Grants Program Current Project List</title> <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.3/dist/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin=anonymous ></script> <div class=container-fluid  id=top-alert > <div class="alert alert-dark alert-dismissible mb-0" role=alert > <p class=text-center > <a href="https://youtu.be/yHiyJQdWBY8">Check out the latest talk: "The Continuing Advancements of Scientific Machine Learning (SciML)"</a> </p> <!-- <button type=button  class=close  data-dismiss=alert  aria-label=Close > <span aria-hidden=true >&times;</span> --> </button> </div> </div> <header> <nav class="navbar navbar-expand-lg navbar-light"> <a class=navbar-brand  href="/">Home</a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class="nav-item active"> <a class=nav-link  href="https://docs.sciml.ai/">Documentation</a> <li class=nav-item > <a class=nav-link  href="/news/">News</a> <li class=nav-item > <a class=nav-link  href="/roadmap/">Roadmap</a> <li class=nav-item > <a class=nav-link  href="/citing/">Citing</a> <li class=nav-item > <a class=nav-link  href="/showcase/">Showcase</a> <li class=nav-item > <a class=nav-link  href="https://benchmarks.sciml.ai/">Benchmarks</a> <li class=nav-item > <a class=nav-link  href="https://github.com/SciML/">GitHub</a> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id=navbarDropdownMenuLink  role=button  data-toggle=dropdown  aria-haspopup=true  aria-expanded=false > Community </a> <div class=dropdown-menu  aria-labelledby=navbarDropdownMenuLink > <a class=dropdown-item  href="/community/">Community Home</a> <a class=dropdown-item  href="/governance/">Governance</a> <a class=dropdown-item  href="/coc/">Code of Conduct</a> <a class=dropdown-item  href="/challenge/">Challenges</a> <a class=dropdown-item  href="/dev/">Developer Programs</a> </div> <li class=nav-item > <a class=nav-link  href="https://juliahub.com/company/contact-us-sciml"> Commercial Support </a> <li class=nav-item > <a class=nav-link  href="https://numfocus.org/donate-to-sciml"><i class="bi bi-heart"></i> Donate</a> </ul> </div> </nav> </header> <div class=franklin-content ><h1 id=sciml_small_grants_program_current_project_list ><a href="#sciml_small_grants_program_current_project_list" class=header-anchor >SciML Small Grants Program Current Project List</a></h1> <p>The following is the current project list for the SciML Small Grants Program.</p> <h2 id=rules_and_regulations ><a href="#rules_and_regulations" class=header-anchor >Rules and Regulations</a></h2> <p>The small grant projects are decided by the SciML Steering Council and candidates can choose to take on projects from the project list. This is similar to &quot;bounty programs&quot; seen in other open source environments, though it is driven by SciML in a manner that is designed to give better outcomes to both contributors and maintainers of the project. In order to remove a hostile competitive atmosphere, <strong>candidates must declare to the committee interest before solving the chosen issue</strong> and, upon approval by the selection committee, are given an exclusive time interval &#40;defaulting to one month&#41; to solve the issue. Payout is done upon completion of the chosen project and acceptance by the steering council.</p> <p>All projects are expected to contribute the code to repositories in the SciML Github organization. Code which is not contributed to the open source repositories will not be considered in the approval evaluations.</p> <h2 id=declaring_for_a_project ><a href="#declaring_for_a_project" class=header-anchor >Declaring for a Project</a></h2> <p>To declare for a small grant program, open a pull request on the Github repo for <a href="https://github.com/SciML/sciml.ai">https://github.com/SciML/sciml.ai</a> in the <code>small_grants.md</code> file with a declaration for the grant in the section of the project of interest. For example:</p> <blockquote> <p><strong>In Progress</strong>: Claimed by XXXXXX for the time period of &#40;today&#41; - &#40;one month later&#41;.</p> </blockquote> <p>In your pull request, please share:</p> <ul> <li><p>Full legal name</p> <li><p>CV</p> <li><p>Short bio / background about your experience in the topic</p> <li><p>Description of which project you&#39;re interested in</p> </ul> <p>If some anonymity is requested, you may instead send this information to sciml@julialang.org.</p> <p>The potential reviewers will discuss the current-ness of the small grant project, the availability of reviewers, and the willingness to review. The small grant project is accepted when a majority of the steering council approves of the project and the PR is merged. At that point the work will commence under the supervision of the reviewer. When the reviewer accepts and merges the appropriate PRs, the grant will be determined as completed and the payout will commence. The grants are project based and payout will only occur upon acceptance by the reviewer.</p> <p>Note that information about ongoing projects will be tracked as public information below in order to give clear information to other contributors about what projects are taken. Completed projects will be added to a projects archive.</p> <h2 id=going_over_the_one-month_time_budget ><a href="#going_over_the_one-month_time_budget" class=header-anchor >Going Over the One-Month Time Budget</a></h2> <p>Each project is given a one month exclusive period for the purpose of decreasing competition. The difference between the small grants program and a standard bounty program is that we wish to discourage &quot;sniping&quot; bounties from others, i.e. someone working for a few weeks on a project before suddenly someone else appears with a complete solution and takes the payout. While this is not impossible to prevent, as an open source community there may be others who happen to be working the same project independently, to reduce the occurrences of this phenomena we have a policy that payouts will only occur for declared projects. This policy can be overridden on special circumstances by a majority vote of the steering council.</p> <p>In order to discourage &quot;spotting&quot;, i.e. claiming projects to sit on them indefinitely and thus blocking other contributors, only one month exclusive periods are granted. If the project seems to be going beyond this one month timeframe, the contributor should ask for an extension by making a new pull request to bump the time window, which would then need to pass by a majority vote of the steering council. If a new claim PR is opened after the time window has passed without the contributor formally bumping the time window, then the new claim PR will be reviewed under the assumption that the project is not currently active.</p> <p>Note that unsuccessful small grants projects, i.e. claims of potential projects where the contributor disappears and does not end up with a PR, will be used in the criteria of whether further requests are accepted.</p> <h2 id=getting_paid_out ><a href="#getting_paid_out" class=header-anchor >Getting Paid Out</a></h2> <p>After a successful project, a pull request to <a href="https://github.com/SciML/sciml.ai">https://github.com/SciML/sciml.ai</a> must be made that updates the Small Grants project page declaring the project as completed by moving it to the completed section. This pull request then must be merged by the designated reviewer. Once this is merged, the project is declared to be successfully completed unless comments by the reviewer in the pull request suggest otherwise. Upon completion, open an expense report at <a href="https://opencollective.com/sciml/expenses/new">https://opencollective.com/sciml/expenses/new</a> and follow the instructions to submit an expense. In the expense report,</p> <p>We need a PDF invoice on file. Please submit an invoice using an invoice template &#40;for example: https://create.microsoft.com/en-us/templates/invoices&#41; that contains the following information:</p> <ul> <li><p>Current Date</p> <li><p>Unique Invoice Number</p> <li><p>Name and Contact Information</p> <li><p>Bill To Information &#40;NumFOCUS&#41;</p> <li><p>Project Name and Grant Information &#40;if relevant&#41;</p> <li><p>Period of Work Performed</p> <li><p>Itemized List of Work Performed</p> <li><p>Short description</p> <li><p>Hourly rate</p> <li><p>Total for Invoice</p> <li><p>Payment Terms &#40;e.g., Net 30&#41;</p> </ul> <p>Lastly, in order to process payment, we the following should be added as a comment to the report filing:</p> <ul> <li><p>SWIFT/BIC#</p> <li><p>IFSC</p> <li><p>Account Number</p> </ul> <p>A steering council member will review that the expense matches the approved grant and, if so, approve the expense. This finalizes the process and the payment will be sent.</p> <h2 id=donating_to_the_program ><a href="#donating_to_the_program" class=header-anchor >Donating to the Program</a></h2> <p>If you wish to donate to the SciML Small Grants Program, <a href="https://numfocus.org/donate-to-sciml">please donate via NumFOCUS</a>. SciML via NumFOCUS is a 501&#40;c&#41;&#40;3&#41; public charity in the United States. Your donation is tax-deductible to the extent provided by US law. General donated funds are used for the developer programs such as the small grants program and the SciML Fellowship.</p> <p>Donations can be earmarked towards specific projects. In order for an earmarked project to become a part of the SciML Small Grants program it must be approved by the SciML Steering Council. The reason why all chosen projects must be vetted by the Steering Council is that we want to be fair to the potential contributors, and this means we must ensure that the suggested projects will have a prompt review process and proper maintenance beyond the timeframe of the project. For this reason, we suggest discussing with maintainers in the official Slack/Zulip before making an earmarked donation.</p> <h2 id=adding_projects_to_the_list ><a href="#adding_projects_to_the_list" class=header-anchor >Adding Projects to the List</a></h2> <p>If you wish to add a project to the small grants list, open a pull request in the <a href="https://github.com/SciML/sciml.ai">https://github.com/SciML/sciml.ai</a> in the <code>small_grants.md</code> file with a full description of the project and the suspected monetary payout. New projects require approval by a steering committee member. The justification for the funding is required. If it is a personal donation, it is recommended that personal donation is made prior to the PR, though for known members of the community this requirement can be waived. Some of the projects can be covered through prior grant funds and other donations made to the SciML project, though it should not be assumed that this will be the case for any project without review by a steering committee member.</p> <h2 id=commitments_from_reviewers ><a href="#commitments_from_reviewers" class=header-anchor >Commitments from Reviewers</a></h2> <p>Reviewers are committed to giving timely feedback and reviews. It is expected that you keep in constant touch with the reviewer during the duration of the project, discussing on one of the community chat channels at least a few times a week until project completion. Discussions should take place in the SciML Slack or Zulip channels &#40;#diffeq-bridged or #sciml-bridged&#41; to allow for other contributors to give feedback. Reviews of PRs should be expected within a few days to ensure the contributor can complete the project in the allotted time frame.</p> <p>However, it is also expected that the contributor can work fairly independently with guidance from the reviewer. Contributors are expected to be comfortable enough in the area of expertise to work through the errors and test failures on their own. The SciML Small Grants Program is not a training program like Google Summer of Code or the SciML Fellowship, and thus the reviewer is not expected to mentor or teach the contributor how to solve the problem. The reviewer is in no obligation to help the contributor work through tests, bug fixing, etc. as these projects were chosen due to the fact that the maintainers have not been able to find the time to cover these areas. The obligation of the reviewer is to give a timely feedback on what the requirements for merging would be &#40;i.e. what tests are required to be added, whether certain code meets style demands, etc.&#41; so that the contributor can achieve a mergable PR within the time frame, but there is no expectation that the reviewer will &quot;go the extra mile&quot; to teach the contributor how the package or mathematics works.</p> <h1 id=list_of_current_projects ><a href="#list_of_current_projects" class=header-anchor >List of Current Projects</a></h1> <h2 id=fix_datainterpolations_bspline_derivatives_100 ><a href="#fix_datainterpolations_bspline_derivatives_100" class=header-anchor >Fix <code>DataInterpolations</code> Bspline derivatives &#40;&#36;100&#41;</a></h2> <p><em>In Progress: Claimed by Divyansh Goyal for the time period of September 11th 2025 - October 11th 2025. Extended from October 11 2025 - November 12 2025, in support for a fully mature PR.</em></p> <p><code>DataInterpolations.jl</code> is a SciML repository for interpolating 1D data. It supports a wide number of interpolation types, as well as taking first and second derivatives of the interpolations. Specifically, the BSplineInterpoation has a few bugs with regards to where it puts the control points, and how it calculates derivatives.</p> <p><strong>Information to Get Started</strong>: See the issue https://github.com/SciML/DataInterpolations.jl/issues/419 describes the issue and a proposed solution. Specifically, this work will likely start by mirroring https://github.com/SciML/DataInterpolationsND.jl/pull/20 and re-enabling the derviative tests for BSpline interpolations.</p> <p><strong>Related Issues</strong>: https://github.com/SciML/Optimization.jl/issues/917</p> <p><strong>Success Criteria</strong>: Merged pull request which adds a new OptimizationSciPy.jl to the Optimization.jl repository.</p> <p><strong>Recommended Skills</strong>: Basic &#40;undergrad-level&#41; knowledge of calculus</p> <p><strong>Reviewers</strong>: Chris Rackauckas and Oscar Smith</p> <h2 id=update_cutestjl_to_the_optimizationjl_interface_and_add_to_scimlbenchmarks_200 ><a href="#update_cutestjl_to_the_optimizationjl_interface_and_add_to_scimlbenchmarks_200" class=header-anchor >Update CUTEst.jl to the Optimization.jl Interface and Add to SciMLBenchmarks &#40;&#36;200&#41;</a></h2> <p><em>In Progress: Claimed by Raunak Narang for the time period of August 6th 2025 - September 6th 2025.</em></p> <p><a href="https://github.com/JuliaSmoothOptimizers/CUTEst.jl">CUTEst.jl</a> is a repository of constrained and unconstrained nonlinear programming problems for testing and comparing optimization algorithms. We would like to be able to repurpose this work for improving Optimization.jl&#39;s performance and tracking the performance of optimizers. It would be useful to the community if this set of benchmarks was updated to the modern SciML interfaces and benchmarking tools so it can make use of the full set of methods in Optimization.jl and drive further developments and recommendations to users.</p> <p>This would likely turn into either contributions to CUTEst or wrappers to CUTEst &#40;hosted in SciML&#41; which transform the NLPModels form into Optimization.jl, and a benchmarking script that loops over all optimization problems and applies a set of optimizers to each of them, computing summary statistics at the bottom.</p> <p><strong>Information to Get Started</strong>: The <a href="https://github.com/SciML/SciMLBenchmarks.jl?tab&#61;readme-ov-file#contributing">Contributing Section of the SciMLBenchmarks README</a> describes how to contribute to the benchmarks. The benchmark results are generated using the benchmark server. It is expected that the benchmarks are updated to use the <a href="https://docs.sciml.ai/Optimization/stable/">Optimization.jl</a> interface, which is an interface over most optimizers in Julia. Not all of the optimizers are covered in this interface: simply remove the optimizers which are not wrapped into Optimization.jl</p> <p><strong>Related Issues</strong>: <a href="https://github.com/SciML/SciMLBenchmarks.jl/issues/935">https://github.com/SciML/SciMLBenchmarks.jl/issues/935</a></p> <p><strong>Success Criteria</strong>: The benchmarks should be turned into a loop over Optimization.jl solvers in a standard SciMLBenchmarks benchmark build.</p> <p><strong>Recommended Skills</strong>: Basic &#40;undergrad-level&#41; knowledge of using numerical optimizers</p> <p><strong>Reviewers</strong>: Chris Rackauckas and Vaibhav Dixit</p> <h2 id=dae_problem_benchmarks_100_benchmark ><a href="#dae_problem_benchmarks_100_benchmark" class=header-anchor >DAE Problem Benchmarks &#40;&#36;100 / Benchmark&#41;</a></h2> <p><strong>In Progress</strong>: Claimed by Jayant Pranjal for the time period of 24th July 2025 - 24th Aug 2025.</p> <p>New benchmarks for differential-algebraic equation &#40;DAE&#41; systems would greatly improve our ability to better tune solvers across problems. However, we are currently lacking in the number of such benchmarks that exist. The goal would be to add standard benchmarks from <a href="https://github.com/SciML/SciMLBenchmarks.jl/issues/359">this issue</a> to the SciMLBenchmarks system so that they can be performance tracked over time.</p> <p><strong>Information to Get Started</strong>: <a href="https://github.com/SciML/SciMLBenchmarks.jl?tab&#61;readme-ov-file#contributing">Contributing Section of the SciMLBenchmarks README</a> describes how to contribute to the benchmarks. The benchmark results are generated using the benchmark server. The <a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/372">transition amplifier benchmark</a> and <a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/373">slider crank benchmark</a> were old PRs to add a few of the problems. These could be used as starting points to solve two problems. One would likely need to modify the structural simplification to turn dummy derivative off as well, that can be discussed with Chris in the PR review.</p> <p><strong>Related Issues</strong>: <a href="https://github.com/SciML/OrdinaryDiffEq.jl/issues/2177">https://github.com/SciML/OrdinaryDiffEq.jl/issues/2177</a></p> <p><strong>Success Criteria</strong>: New benchmarks with the DAE systems.</p> <p><strong>Recommended Skills</strong>: Prior knowledge in modeling with differential-algebraic equations would be helpful for debugging.</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=refactor_ordinarydiffeqjl_solver_sets_to_reuse_perform_step_implementations_via_tableaus_100solver_set ><a href="#refactor_ordinarydiffeqjl_solver_sets_to_reuse_perform_step_implementations_via_tableaus_100solver_set" class=header-anchor >Refactor OrdinaryDiffEq.jl Solver Sets to Reuse perform_step&#33; Implementations via Tableaus &#40;&#36;100/solver set&#41;</a></h2> <p><hr />In Progress:** Claimed for the SDIRK set by Krish Gaur for the time period of July 4th 2025 - August 4th, 2025*</p> <p>The perform_step&#33; implementations per solver in OrdinaryDiffEq.jl are often &quot;bespoke&quot;, i.e. one step implementation per solver. The reason is because the package code grew organically over time and this is the easiest way to ensure performance and write out a new method. However, many of the methods can be collapsed by class into a single solver set using a tableau implementation that loops over coefficients. Because of the nuances in performance and implementation, we have avoided doing this refactoring until a few more pieces were set in stone.</p> <p>Note that this should be done based on classes of solvers, as documented in the code as files in the <code>perform_step&#33;</code> implementations &#40;though the explicit Runge-Kutta methods are split across a few files and should all be a single tableau&#41;. Solver set should be discussed before starting the project.</p> <p>It is recommended that implicit methods such as Rosenbrock and SDIRK integrators are done first, as the extra intricacies of their algorithm make this refactor simpler because the nuances of the implementation are less likely to noticeably impact performance.</p> <p><strong>Information to Get Started</strong>: The OrdinaryDiffEq.jl solvers are all found in <a href="https://github.com/SciML/OrdinaryDiffEq.jl">the Github repository</a> and the format of the package is documented in the <a href="https://docs.sciml.ai/DiffEqDevDocs/stable/">developer documentation</a>. The key to doing this right is to note that it is just a refactor, so all of the methods are there in the package already. However, note that some methods can be a bit nuanced, for example, BS5 and DP8 use fairly non-standard error estimators for an explicit Runge-Kutta method, while Verner methods have a twist with laziness. Because of this, the key is to be careful to add points to dispatch to alternative based on the nuances of the given algorithms.</p> <p><strong>Related Issues</strong>: <a href="https://github.com/SciML/OrdinaryDiffEq.jl/issues/233">https://github.com/SciML/OrdinaryDiffEq.jl/issues/233</a></p> <p><strong>Success Criteria</strong>: The independent solver packages are registered and released, and a breaking update to OrdinaryDiffEq.jl is released which reduces the loading time by not including all solvers by default. This success also requires updating package documentation to reflect these changes.</p> <p><strong>Recommended Skills</strong>: Since all of the code for the solvers exists and this a refactor, no prior knowledge of numerical differential equations is required. Only standard software development skills and test-driven development of a large code base is required.</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h1 id=successful_projects_archive ><a href="#successful_projects_archive" class=header-anchor >Successful Projects Archive</a></h1> <p>These are the previous SciML small grants projects which have successfully concluded and paid out.</p> <h2 id=update_loopvectorization_to_support_changes_in_julia_v112_200 ><a href="#update_loopvectorization_to_support_changes_in_julia_v112_200" class=header-anchor >Update LoopVectorization to Support Changes in Julia v1.12 &#40;&#36;200&#41;</a></h2> <p><em>Completed by Maximilian Pochapski Oct 8th, 2025</em></p> <p><a href="https://github.com/JuliaSIMD/LoopVectorization.jl">LoopVectorization.jl</a> is a central package for the performance of many Julia packages. Its internals make use of many low-level features and manual SIMD that can make it require significant maintenance to be optimized for new versions of the compiler.</p> <p><strong>Information to Get Started</strong>:</p> <p>With Julia v1.12:</p> <ul> <li><p><a href="https://releases.llvm.org/17.0.1/docs/OpaquePointers.html">opaque pointer mode</a> is now the default</p> <li><p><a href="https://github.com/JuliaLang/julia/pull/53687">Julia pointers are now LLVM pointers</a> instead of integers, as they were in earlier Julia versions.</p> </ul> <p>The purpose of this project is to update LoopVectorization.jl, VectorizationBase.jl, SLEEFPirates.jl and the rest of the JuliaSIMD ecosystem so that all the <code>llvmcall</code>s use opaque pointers, and any <code>Ptr</code> arguments or returns are llvm <code>ptr</code>s instead of integers. LoopVectorization.jl tests should pass under <code>--depwarn&#61;error</code>.</p> <p>Note that the funds for this project as given by earmarked donations to the JuliaLang project which SciML will help administer through the small grants program.</p> <p><strong>Success Criteria</strong>: LoopVectorization.jl runs on v1.12&#39;s latest release.</p> <p><strong>Recommended Skills</strong>: This requires some low-level knowledge of LLVM IR and familiarity with <code>llvmcall</code>. The changes should be routine.</p> <p><strong>Reviewers</strong>: Chris Elrod</p> <h2 id=dae_problem_benchmarks_100_benchmark__2 ><a href="#dae_problem_benchmarks_100_benchmark__2" class=header-anchor >DAE Problem Benchmarks &#40;&#36;100 / Benchmark&#41;</a></h2> <p>Completed by <strong>Jayant Pranjal</strong></p> <p><strong>Benchmarks added:</strong> NAND Gate Problem benchmark</p> <p>New benchmarks for differential-algebraic equation &#40;DAE&#41; systems would greatly improve our ability to better tune solvers across problems. However, we are currently lacking in the number of such benchmarks that exist. The goal would be to add standard benchmarks from <a href="https://github.com/SciML/SciMLBenchmarks.jl/issues/359">this issue</a> to the SciMLBenchmarks system so that they can be performance tracked over time.</p> <p><strong>Information to Get Started</strong>: <a href="https://github.com/SciML/SciMLBenchmarks.jl?tab&#61;readme-ov-file#contributing">Contributing Section of the SciMLBenchmarks README</a> describes how to contribute to the benchmarks. The benchmark results are generated using the benchmark server. The <a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/372">transition amplifier benchmark</a> and <a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/373">slider crank benchmark</a> were old PRs to add a few of the problems. These could be used as starting points to solve two problems. One would likely need to modify the structural simplification to turn dummy derivative off as well, that can be discussed with Chris in the PR review.</p> <p><strong>Related Issues</strong>: <a href="https://github.com/SciML/OrdinaryDiffEq.jl/issues/2177">https://github.com/SciML/OrdinaryDiffEq.jl/issues/2177</a></p> <p><strong>Success Criteria</strong>: New benchmarks with the DAE systems.</p> <p><strong>Recommended Skills</strong>: Prior knowledge in modeling with differential-algebraic equations would be helpful for debugging.</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=fix_and_update_the_simple_handwritten_pdes_as_odes_benchmark_set_400 ><a href="#fix_and_update_the_simple_handwritten_pdes_as_odes_benchmark_set_400" class=header-anchor >Fix and Update the &quot;Simple Handwritten PDEs as ODEs&quot; Benchmark Set &#40;&#36;400&#41;</a></h2> <blockquote> <p>Completed by Arjit Seth on August 1, 2025.</p> </blockquote> <p>The &quot;Simple Handwritten PDEs as ODEs&quot; benchmarks have been failing for a while. They need to be updated to the &quot;new&quot; linear solve syntax introduced in 2022. When updated, these benchmarks should serve as a canonical development point for PDE-specific methods, such as implicit-explicit &#40;IMEX&#41; and exponential integrators.</p> <p><strong>Information to Get Started</strong>: The <a href="https://github.com/SciML/SciMLBenchmarks.jl?tab&#61;readme-ov-file#contributing">Contributing Section of the SciMLBenchmarks README</a> describes how to contribute to the benchmarks. The benchmark results are generated using the benchmark server. Half of the benchmarks are set up using hand-discretized finite difference stencils for the PDE, the other half use ApproxFun.jl in order to do a pseudospectral discretization. A direct pseudospectral discretization via manual FFTs and operator construction would also be fine.</p> <p><strong>Related Issues</strong>: <a href="https://github.com/SciML/SciMLBenchmarks.jl/issues/929">https://github.com/SciML/SciMLBenchmarks.jl/issues/929</a></p> <p><strong>Success Criteria</strong>: Pull requests which <a href="https://github.com/SciML/SciMLBenchmarks.jl/tree/master/benchmarks/SimpleHandwrittenPDE">update the benchmarks in the folder</a> to be successful with the current Julia and package version &#40;v1.10&#41; without erroring, generating work-precision diagrams. In addition, these should be updated to give a clearer definition of the PDE being solved, adding a LaTeX description of the equations to the top of the file.</p> <p><strong>Recommended Skills</strong>: Basic &#40;undergrad-level&#41; knowledge of finite difference and pseudospectral PDE discretizations.</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=wrap_pycma_into_the_optimizationjl_interface_100 ><a href="#wrap_pycma_into_the_optimizationjl_interface_100" class=header-anchor >Wrap PyCMA into the Optimization.jl Interface &#40;&#36;100&#41;</a></h2> <p><em>Completed by Maximilian Pochapski June 25th, 2025</em></p> <p>PyCMA is a very good global optimizer written in Python. It did very well in early editions of the BlackboxOptimizationBenchmarking.jl tests &#40;see for example https://github.com/jonathanBieler/BlackBoxOptimizationBenchmarking.jl/tree/v0.1.0&#41; and thus it would be good to have available for users to call and for benchmarking new global optimization algorithms against. The goal of this project is to use PythonCall.jl to setup the wrapper subpackage OptimizationPyCMA.jl with the bells and whistles to make such benchmarking and usage straightforward and simple.</p> <p><strong>Information to Get Started</strong>: See the issue https://github.com/SciML/Optimization.jl/issues/918 which has links to starter code. PythonCall.jl is a well-documented library for calling Python code from Julia and thus its documentation is a good starting point as well.</p> <p><strong>Related Issues</strong>: https://github.com/SciML/Optimization.jl/issues/918</p> <p><strong>Success Criteria</strong>: Merged pull request which adds a new OptimizationPyCMA.jl to the Optimization.jl repository.</p> <p><strong>Recommended Skills</strong>: Basic &#40;undergrad-level&#41; knowledge of calculus and Python</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=wrap_scipyoptimize_into_the_optimizationjl_interface_300 ><a href="#wrap_scipyoptimize_into_the_optimizationjl_interface_300" class=header-anchor >Wrap <code>scipy.optimize</code> into the Optimization.jl Interface &#40;&#36;300&#41;</a></h2> <p><strong>Completed by Aditya Pandey on June 23rd, 2025</strong></p> <p><code>scipy.optimize</code> is a standard in Python with lots of different methods, both local and global optimizers, that are well-tested and robust. Thus in order to improve the benchmarking and development of native Julia solvers, it would be helpful to have these algorithms more easily accessible on the standard optimization interface. Additionally, it can help users who are transitioning projects to and from Julia to have a direct way to call the previous code in order to double check the translation. The goal of this project is to use PythonCall.jl to setup the wrapper subpackage OptimizationSciPy.jl with the bells and whistles to make such benchmarking and usage straightforward and simple.</p> <p><strong>Information to Get Started</strong>: See the issue https://github.com/SciML/Optimization.jl/issues/917 which has links to starter code. PythonCall.jl is a well-documented library for calling Python code from Julia and thus its documentation is a good starting point as well.</p> <p><strong>Related Issues</strong>: https://github.com/SciML/Optimization.jl/issues/917</p> <p><strong>Success Criteria</strong>: Merged pull request which adds a new OptimizationSciPy.jl to the Optimization.jl repository.</p> <p><strong>Recommended Skills</strong>: Basic &#40;undergrad-level&#41; knowledge of calculus and Python</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=add_sympyjl_as_an_alternative_backend_for_symbolicsjl_300 ><a href="#add_sympyjl_as_an_alternative_backend_for_symbolicsjl_300" class=header-anchor >Add SymPy.jl as an Alternative Backend for Symbolics.jl &#40;&#36;300&#41;</a></h2> <p><strong>Completed by Jash Ambaliya on June 20th, 2025.</strong></p> <p>The Symbolics.jl symbolic solver covers a wide range of cases, but adding a fallback to SymPy&#39;s solver provides broader coverage with minimal effort. This project implemented a backend integration using SymPy.jl via automatic conversion between Symbolics and SymPy representations. It introduced helper functions for converting expressions, solving them using SymPy, and converting results back.</p> <p><strong>Goals Achieved</strong> :</p> <ul> <li><p>Documented and tested the Symbolics &lt;-&gt; SymPy roundtrip conversion.</p> <li><p>Implemented wrappers in <code>SymbolicsSymPyExt.jl</code> for the following SymPy-based operations:</p> <ul> <li><p><code>linear_solve</code></p> <li><p><code>algebraic_solve</code></p> <li><p><code>integrate</code></p> <li><p><code>limit</code></p> <li><p><code>simplify</code></p> </ul> </ul> <p>This enhancement expands Symbolics.jl’s solving capabilities by leveraging the mature SymPy backend when native solutions are insufficient.</p> <p><strong>Information to Get Started</strong> :</p> <p>The project used the existing <code>symbolics_to_sympy</code> function and the SymPy exchange mechanism. Reference discussions included:</p> <ul> <li><p><a href="https://github.com/JuliaSymbolics/Symbolics.jl/issues/1223">Symbolics.jl #1223</a></p> <li><p><a href="https://github.com/jverzani/SymPyCore.jl/pull/88">SymPyCore.jl #88</a></p> <li><p><a href="https://github.com/JuliaSymbolics/Symbolics.jl/issues/1551">Symbolics.jl #1551</a></p> </ul> <p><strong>Success Criteria</strong> : Pull requests adding the five requested functions to the <a href="https://github.com/JuliaSymbolics/Symbolics.jl/blob/master/ext/SymbolicsSymPyExt.jl">SymbolicsSymPyExt.jl</a>.</p> <p><strong>Recommended Skills</strong> : Basic &#40;undergrad-level&#41; knowledge of calculus, symbolic computation, and Python interop in Julia.</p> <p><strong>Reviewers</strong> : Chris Rackauckas</p> <h2 id=refactor_ordinarydiffeqjl_to_use_sub-packages_of_solvers_600 ><a href="#refactor_ordinarydiffeqjl_to_use_sub-packages_of_solvers_600" class=header-anchor >Refactor OrdinaryDiffEq.jl to use Sub-Packages of Solvers &#40;&#36;600&#41;</a></h2> <h4 id=note_bounty_increase_to_600_from_300_7202024 ><a href="#note_bounty_increase_to_600_from_300_7202024" class=header-anchor >Note: Bounty increase to &#36;600 from &#36;300 &#40;7/20/2024&#41;</a></h4> <p><strong>Completed by Param Umesh Thakkar for the time period of June 18th, 2024 - August 18th 2024. Extended due to scope and cost extension.</strong></p> <p>It&#39;s no surprise to anyone to hear that DifferentialEquations.jl, in particular the OrdinaryDiffEq.jl solver package, is very large and takes a long time to precompile. However, this is because there are a lot of solvers in the package. The goal would be to refactor this package so that sets of solvers are instead held in subpackages that are only loaded on-demand. Since many of the solvers are only used in more niche applications, this allows for them to be easily maintained in the same repo while not imposing a loading cost on the more standard applications.</p> <p><strong>Information to Get Started</strong>: The OrdinaryDiffEq.jl solvers are all found in <a href="https://github.com/SciML/OrdinaryDiffEq.jl">the Github repository</a> and the format of the package is documented in the <a href="https://docs.sciml.ai/DiffEqDevDocs/stable/">developer documentation</a></p> <p><strong>Related Issues</strong>: <a href="https://github.com/SciML/OrdinaryDiffEq.jl/issues/2177">https://github.com/SciML/OrdinaryDiffEq.jl/issues/2177</a></p> <p><strong>Success Criteria</strong>: The independent solver packages are registered and released, and a breaking update to OrdinaryDiffEq.jl is released which reduces the loading time by not including all solvers by default. This success also requires updating package documentation to reflect these changes.</p> <p><strong>Recommended Skills</strong>: Since all of the code for the solvers exists and this a refactor, no prior knowledge of numerical differential equations is required. Only standard software development skills and test-driven development of a large code base is required.</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=dae_problem_benchmarks_100_benchmark__3 ><a href="#dae_problem_benchmarks_100_benchmark__3" class=header-anchor >DAE Problem Benchmarks &#40;&#36;100 / Benchmark&#41;</a></h2> <p><strong>Completed by Marko Polic in the time period of June 18th, 2024 - July 18th 2024.</strong> The transistor amplifier benchmark was added <a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1007">https://github.com/SciML/SciMLBenchmarks.jl/pull/1007</a>. Project is kept open for other benchmarks.</p> <h2 id=scimlbenchmarks_compatibility_bump_for_benchmark_sets_100_each_set ><a href="#scimlbenchmarks_compatibility_bump_for_benchmark_sets_100_each_set" class=header-anchor >SciMLBenchmarks Compatibility Bump for Benchmark Sets &#40;&#36;100 each set&#41;</a></h2> <p><strong>Completed by Param Umesh Thakkar for the time period of January 21st - February 21st. Extended from February 21st to March 21st. Extended due to the project&#39;s complexity and final refinements.</strong></p> <p>The <a href="https://github.com/SciML/SciMLBenchmarks.jl">SciMLBenchmarks</a> are a large set of benchmarks maintained by the SciML organization. As such, keeping these benchmarks up-to-date can be a time-consuming task. In many cases, we can end up in a situation where there are many package bumps that need to happen. Sometimes no code needs to be updated, in other cases the benchmark code does need to be updated. The only way to tell is to start the update process, bump the project and manifest tomls, and start digging into the results.</p> <p>These bumps are done in subsets. The currently identified subsets are:</p> <h4 id=parameterestimation ><a href="#parameterestimation" class=header-anchor >ParameterEstimation</a></h4> <ul> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/799">https://github.com/SciML/SciMLBenchmarks.jl/pull/799</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1063">https://github.com/SciML/SciMLBenchmarks.jl/pull/1063</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1064">https://github.com/SciML/SciMLBenchmarks.jl/pull/1064</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1065">https://github.com/SciML/SciMLBenchmarks.jl/pull/1065</a></p> </ul> <h4 id=pinns ><a href="#pinns" class=header-anchor >PINNs</a></h4> <ul> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1062">https://github.com/SciML/SciMLBenchmarks.jl/pull/1062</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1061">https://github.com/SciML/SciMLBenchmarks.jl/pull/1061</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1044">https://github.com/SciML/SciMLBenchmarks.jl/pull/1044</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/877">https://github.com/SciML/SciMLBenchmarks.jl/pull/877</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1060">https://github.com/SciML/SciMLBenchmarks.jl/pull/1060</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1059">https://github.com/SciML/SciMLBenchmarks.jl/pull/1059</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/1043">https://github.com/SciML/SciMLBenchmarks.jl/pull/1043</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/876">https://github.com/SciML/SciMLBenchmarks.jl/pull/876</a></p> <li><p><a href="https://github.com/SciML/SciMLBenchmarks.jl/pull/605">https://github.com/SciML/SciMLBenchmarks.jl/pull/605</a></p> </ul> <p><strong>Information to Get Started</strong>: The <a href="https://github.com/SciML/SciMLBenchmarks.jl?tab&#61;readme-ov-file#contributing">Contributing Section of the SciMLBenchmarks README</a> describes how to contribute to the benchmarks. The benchmark results are generated using the benchmark server. It is expected that the developer checks that the benchmarks are appropriately ran and generating correct graphs when updated, and highlight any performance regressions found through the update process.</p> <p><strong>Related Issues</strong>: See the linked pull requests.</p> <p><strong>Success Criteria</strong>: The benchmarks should run and give similar results to the pre-updated benchmarks, and any regressions should be identified with an issue opened in the appropriate repository.</p> <p><strong>Recommended Skills</strong>: Willingness to roll up some sleeves and figure out what changed in breaking updates.</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=update_scimloperatorsjl_to_allow_for_different_defining_vectors_from_actions_500 ><a href="#update_scimloperatorsjl_to_allow_for_different_defining_vectors_from_actions_500" class=header-anchor >Update SciMLOperators.jl to allow for different defining vectors from actions &#40;&#36;500&#41;</a></h2> <p><strong>Completed by Divyansh Goyal on May 17th, 2025.</strong></p> <p>SciMLOperators.jl is a package for defining lazy operators <code>A&#40;u,p,t&#41;*v</code> which can be used throughout the ecosystem. However, many of the operators incorrectly make the assumption that <code>u &#61; v</code>, i.e. <code>A&#40;u,p,t&#41;*u</code> is the operation. While this is the only case required for some ODE integrators, this oversimplification limits the full usage of the library. It is expected that this is a breaking change &#40;with a major release bump&#41; and is the major change required for the v1.0 release.</p> <p><strong>Information to Get Started</strong>: The documentation of https://github.com/SciML/SciMLOperators.jl should be sufficient.</p> <p><strong>Recommended Skills</strong>: Basic &#40;undergrad-level&#41; knowledge of linear operators and multiple dispatch in Julia.</p> <p><strong>Reviewers</strong>: Chris Rackauckas</p> <h2 id=improve_training_performance_of_gpu_backend_in_evotreesjl_2000 ><a href="#improve_training_performance_of_gpu_backend_in_evotreesjl_2000" class=header-anchor >Improve training performance of GPU backend in EvoTrees.jl &#40;&#36;2000&#41;</a></h2> <h4 id=note_bounty_was_decided_to_be_2250_due_to_partial_kajl_work ><a href="#note_bounty_was_decided_to_be_2250_due_to_partial_kajl_work" class=header-anchor >Note: Bounty was decided to be &#36;2250 due to partial KA.jl work</a></h4> <p><strong>Completed by Aditya Pandey on October 25th, 2025.</strong> &#40;Extended from September 1,2025 due to integration of work with main branch and adding features&#41;</p> <p>EvoTrees.jl&#91;https://github.com/Evovest/EvoTrees.jl&#93; is an efficient pure-Julia implementation of boosted trees. Performance on CPU is competitive and even superior to peers such as XGBoost. However, the GPU backend is lagging.</p> <p>The objective of this project is to improve the GPU backend to bring the training benchmarks to in a competitive range to XGBoost. A premium of &#36;500 will be awarded if the solution is implemented with <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions.jl</a>, allowing the support for AMD gpus.</p> <p><strong>Information to Get Started</strong>: A key bottleneck is assumed to be the important overhead from the large number of kernels launched as the depth of the tree grows. Also, only the gradients and histograms are computed on the GPU, while gains and best node split could also be computed on the GPU and reduce the GPU to CPU communications. Potential solution paths and preliminary work initiative is discussed in this <a href="https://github.com/Evovest/EvoTrees.jl/issues/288">issue</a>.</p> <p><strong>Success Criteria</strong>: A PR is merged to EvoTrees.jl which brings the benchmarked GPU training time to less than 125&#37; that of XGBoost for the 1M and 10M observations benchmarks as discussed in the core <a href="https://github.com/Evovest/EvoTrees.jl/issues/288">issue</a>.</p> <p>It should be reproducible on either a 3090, 4090 or a RTX A4000. The solution should be purely Julia based, and not result in a significant increase in code complexity / LoCs.</p> <p><strong>Recommended Skills</strong>: Experience in kernel development on GPU, preferably with CUDA.jl or <a href="https://github.com/JuliaGPU/KernelAbstractions.jl">KernelAbstractions.jl</a>. General performance optimization and multi-threading.</p> <p><strong>Reviewers</strong>: <a href="https://github.com/jeremiedb">Jeremie Desgagne-Bouchard</a></p> <!-- Footer--> <footer class="footer bg-light"> <div class=container > <div class=row > <div class="col-lg-6 h-100 text-center text-lg-start my-auto"> <ul class="list-inline mb-2"> <li class=list-inline-item ><a href="/community">Contact</a> <!-- <li class=list-inline-item ><a href="#!">Terms of Use</a> <li class=list-inline-item ><a href="#!">Privacy Policy</a> --> </ul> <p class="text-muted small mb-4 mb-lg-0">Website powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia</a> programming language. &copy; SciML 2022. All Rights Reserved.</p> <p class="text-muted small mb-4 mb-lg-0">Edit on <a href="https://github.com/SciML/sciml.ai">GitHub</a></p> </div> <div class="col-lg-6 h-100 text-center text-lg-end my-auto"> <ul class="list-inline mb-0"> <li class="list-inline-item me-4"> <a href="https://github.com/SciML"><i class="bi-github fs-3"></i></a> <li class="list-inline-item me-4"> <a href="https://twitter.com/SciML_Org"><i class="bi-twitter fs-3"></i></a> <li class=list-inline-item > <a href="https://www.linkedin.com/company/the-julia-language"><i class="bi-linkedin fs-3"></i></a> </ul> </div> </div> </div> </footer> </div>