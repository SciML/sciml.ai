<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta property="og:title" content="SciML: Open Source Software for Scientific Machine Learning"> <meta property="og:description" content="Open Source Software for Scientific Machine Learning"> <meta property="og:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta property="og:url" content="https://sciml.ai"> <meta name="twitter:title" content=SciML > <meta name="twitter:description" content="Open Source Software for Scientific Machine Learning"> <meta name="twitter:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <!-- Favicon--> <link rel=icon  type="image/x-icon" href="assets/favicon.png" /> <!-- Bootstrap icons--> <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel=stylesheet  type="text/css" /> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css"> <!-- Google fonts--> <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet  type="text/css" /> <!-- Core theme CSS (includes Bootstrap)--> <link href="./css/styles.css" rel=stylesheet  /> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/styles.css"> <link rel=stylesheet  href="/css/hypertext.css"> <link rel=icon  href="/assets/favicon.png"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-2"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-90474609-2'); </script> <title>How Julia ODE Solve Compile Time Was Reduced From 30 Seconds to 0.1</title> <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.3/dist/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin=anonymous ></script> <div class=container-fluid  id=top-alert > <div class="alert alert-dark alert-dismissible mb-0" role=alert > <p class=text-center > <a href="https://youtu.be/yHiyJQdWBY8">Check out the latest talk: "The Continuing Advancements of Scientific Machine Learning (SciML)"</a> </p> <!-- <button type=button  class=close  data-dismiss=alert  aria-label=Close > <span aria-hidden=true >&times;</span> --> </button> </div> </div> <header> <nav class="navbar navbar-expand-lg navbar-light"> <a class=navbar-brand  href="/">Home</a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class="nav-item active"> <a class=nav-link  href="https://docs.sciml.ai/">Documentation</a> <li class=nav-item > <a class=nav-link  href="/news/">News</a> <li class=nav-item > <a class=nav-link  href="/roadmap/">Roadmap</a> <li class=nav-item > <a class=nav-link  href="/citing/">Citing</a> <li class=nav-item > <a class=nav-link  href="/showcase/">Showcase</a> <li class=nav-item > <a class=nav-link  href="https://benchmarks.sciml.ai/">Benchmarks</a> <li class=nav-item > <a class=nav-link  href="https://github.com/SciML/">GitHub</a> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id=navbarDropdownMenuLink  role=button  data-toggle=dropdown  aria-haspopup=true  aria-expanded=false > Community </a> <div class=dropdown-menu  aria-labelledby=navbarDropdownMenuLink > <a class=dropdown-item  href="/community/">Community Home</a> <a class=dropdown-item  href="/governance/">Governance</a> <a class=dropdown-item  href="/coc/">Code of Conduct</a> <a class=dropdown-item  href="/challenge/">Challenges</a> <a class=dropdown-item  href="/dev/">Developer Programs</a> </div> <li class=nav-item > <a class=nav-link  href="https://juliahub.com/company/contact-us-sciml"> Commercial Support </a> <li class=nav-item > <a class=nav-link  href="https://numfocus.org/donate-to-sciml"><i class="bi bi-heart"></i> Donate</a> </ul> </div> </nav> </header> <div class=franklin-content ><h1 id=how_julia_ode_solve_compile_time_was_reduced_from_30_seconds_to_01 ><a href="#how_julia_ode_solve_compile_time_was_reduced_from_30_seconds_to_01" class=header-anchor >How Julia ODE Solve Compile Time Was Reduced From 30 Seconds to 0.1</a></h1> <p>We did it. We got control of our compile times in a large-scale &gt;100,000 line of code Julia library. The end result looks like:</p> <p><img src="https://user-images.githubusercontent.com/1814174/185794444-34a99f53-646a-4cbb-81a3-678bb2e13a17.gif" alt="" /></p> <p>However, the most important thing is the friends we made along the way. In this blog post we will go through a step-by-step explanation of the challenges to compile times, ways to understand and debug compile-time issues, how to directly control specialization to avoid recompilation, and finally how to setup snoop precompilation on packages to enable easy system image building. We will describe the changes made to Julia in v1.8 which were necessary in order for this win, and the underlying trade-offs made with these changes. With an understanding of what we have done and why, this process for reducing Julia package compile times is easily reproducible to the rest of the ecosystem. So let&#39;s get started&#33;</p> <p>Note: this is meant to be a human-readable summary of the <a href="https://github.com/SciML/DifferentialEquations.jl/issues/786">original thread on compile times found in the DifferentialEquations.jl repository</a></p> <h4 id=edit_9182023 ><a href="#edit_9182023" class=header-anchor >Edit: 9/18/2023</a></h4> <p>The release of Julia v1.9 added caching of native code to precompilation. This means that the system image step is no longer necessary, see <a href="https://julialang.org/blog/2023/04/julia-1.9-highlights/">the v1.9 release blog post for details</a>. In particular, on v1.10 we with out of the both Julia we see the following result for the first solve time:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq, SnoopCompile
<span class=hljs-keyword >function</span> lorenz(du, u, p, t)
    du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>] - u[<span class=hljs-number >1</span>])
    du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>] * (<span class=hljs-number >28.0</span> - u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
    du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>] * u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span> / <span class=hljs-number >3</span>) * u[<span class=hljs-number >3</span>]
<span class=hljs-keyword >end</span>

<span class=hljs-meta >@time</span> <span class=hljs-keyword >begin</span>
    lorenzprob = ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.AutoSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), <span class=hljs-built_in >Float64</span>[])
    sol = solve(lorenzprob, Rosenbrock23())
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># 0.096844 seconds (106.87 k allocations: 7.286 MiB, 99.59% compilation time)</span></code></pre> <p>so now less than 0.1 seconds right out of the box. This is a fantastic improvement by the compiler team&#33;</p> <p>However, note that the system image can still be useful in reducing load times, i.e. the time for doing <code>using OrdinaryDiffEq</code> which on v1.10 is about <code>1.784398 seconds &#40;2.76 M allocations: 168.908 MiB, 1.58&#37; gc time, 1.95&#37; compilation time&#41;</code>. There are some further improvements to this in the works, but a system image will bring this to effectively zero.</p> <p>Now back to the show, with the understanding that this blog post and its numbers reflect the environment of Julia v1.8. All of the same tricks are still useful, or even more useful now that precompilation always builds binaries.</p> <h2 id=starting_the_process_profiling_why_ordinarydiffeq_first_solve_time_was_30_second_compilation ><a href="#starting_the_process_profiling_why_ordinarydiffeq_first_solve_time_was_30_second_compilation" class=header-anchor >Starting the Process: Profiling Why OrdinaryDiffEq First Solve Time Was 30 Second Compilation</a></h2> <p>First let&#39;s introduce our challenger. Up at bat and standing strong at 12 lines of code is a formidable opponent: a stiff ODE solve. The code looks like this:</p> <pre><code class="julia hljs"><span class=hljs-meta >@time</span> <span class=hljs-keyword >begin</span>
  <span class=hljs-keyword >using</span> OrdinaryDiffEq
  <span class=hljs-keyword >function</span> lorenz(du,u,p,t)
      du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>]-u[<span class=hljs-number >1</span>])
      du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>]*(<span class=hljs-number >28.0</span>-u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
      du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>]*u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span>/<span class=hljs-number >3</span>)*u[<span class=hljs-number >3</span>]
  <span class=hljs-keyword >end</span>
  u0 = [<span class=hljs-number >1.0</span>;<span class=hljs-number >0.0</span>;<span class=hljs-number >0.0</span>]; tspan = (<span class=hljs-number >0.0</span>,<span class=hljs-number >100.0</span>)
  prob = ODEProblem(lorenz,u0,tspan)
  solve(prob,Rodas5())
<span class=hljs-keyword >end</span></code></pre> <p>When we <a href="https://github.com/SciML/DifferentialEquations.jl/issues/786">started out compile-time journey on August 13, 2021</a>, this small and widely used code took nearly 22 seconds for the first <code>solve</code> call. Note that in the early phase we did not track <code>using OrdinaryDiffEq</code> time, which together brings the time for this code chunk to around 30 seconds.</p> <p>But why? The major improvement which came to the Julia language in the summer of 2021, which kicked off this project, was the ability to profile compile times. To do this, one uses a mixture of the package <a href="https://github.com/timholy/SnoopCompile.jl">SnoopCompile.jl</a> with the flamegraph viewing package <a href="https://github.com/timholy/ProfileView.jl">ProfileView.jl</a>. When we did that back in the early phase of this project, the profiling code was:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq, SnoopCompile

<span class=hljs-keyword >function</span> lorenz(du,u,p,t)
 du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>]-u[<span class=hljs-number >1</span>])
 du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>]*(<span class=hljs-number >28.0</span>-u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
 du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>]*u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span>/<span class=hljs-number >3</span>)*u[<span class=hljs-number >3</span>]
<span class=hljs-keyword >end</span>

u0 = [<span class=hljs-number >1.0</span>;<span class=hljs-number >0.0</span>;<span class=hljs-number >0.0</span>]
tspan = (<span class=hljs-number >0.0</span>,<span class=hljs-number >100.0</span>)
prob = ODEProblem(lorenz,u0,tspan)
alg = Rodas5()
tinf = <span class=hljs-meta >@snoopi_deep</span> solve(prob,alg)</code></pre> <pre><code class="julia hljs"><span class=hljs-meta >@show</span> tinf
InferenceTimingNode: <span class=hljs-number >1.460777</span>/<span class=hljs-number >16.030597</span> on Core.Compiler.Timings.ROOT() with <span class=hljs-number >46</span> direct children</code></pre> <p>The way to read this is that there was <code>1.460777</code> seconds of LLVM code generation time and <code>16.030597</code> seconds of inference time with 46 inference gaps &#40;due to some uninferred portion&#41; of the code. &#91;Note that this is the result after some optimizations were already in place&#93;. This tells us that there are potentially 16 seconds of inference time that could be precompiled away.</p> <p>Additionally, we can get a flamegraph of the compile-time profile as follows:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> ProfileView
ProfileView.view(flamegraph(tinf))</code></pre> <p><img src="https://user-images.githubusercontent.com/1814174/129282082-ac51270f-5843-4bcc-a452-8aa663c458b8.png" alt="" /></p> <p>A flamegraph is a representation of a profile where every bar is a line of code, moving vertically moves down the call stack &#40;for example, <code>g&#40;f&#40;x&#41;&#41;</code> would have the time for <code>g&#40;y&#41;</code> over the time for <code>f&#40;x&#41;</code>&#41;, and the horizontal bar represents the percentage of the time taken by a given function. From this you can see two things. First of all, most of the time is in one large sized chunk whose function is labelled <code>linear_nonlinear.jl</code>: we will go into this piece in detail. Then there is a small set of chunks to the right which have a repeated structure, that points to a function in <a href="https://github.com/YingboMa/FastBroadcast.jl">FastBroadcast.jl</a> for <code>@..</code> lowering, and finally there are some decently-sized gaps in the graph. The gaps correspond to things which are not measured. The snooping process only profiles the Julia inference time, not the LLVM compile time. Our goal will be to get &quot;everything we can into a gap&quot;.</p> <h2 id=interlude_on_the_biggest_chunk_of_compile-time_recursivefactorization ><a href="#interlude_on_the_biggest_chunk_of_compile-time_recursivefactorization" class=header-anchor >Interlude on the Biggest Chunk of Compile-Time: RecursiveFactorization</a></h2> <p>So first let&#39;s answer how the compile time got to an absurd 30 seconds. Stiff ODE solvers are much more complex than methods for non-stiff equations because they need to do things like solve nonlinear equations. Solving nonlinear equations requires the repeated solving of linear equations, i.e. solving <code>Ax &#61; b</code> for <code>x</code>. In most programming languages, the linear algebra handling for these kinds of standard operations is performed by underlying libraries called the BLAS and LAPACK library. Most open source projects use <a href="https://github.com/xianyi/OpenBLAS">an implementation called OpenBLAS</a>, a C implementation of BLAS/LAPACK which does many of the tricks required for getting much higher performance than &quot;simple&quot; codes by using CPU-specialized kernels based on the sizes of the CPU&#39;s caches. Open source projects like R and SciPy also ship with OpenBLAS because of its generally good performance and open licensing, though it&#39;s known that OpenBLAS is handily outperformed by <a href="https://www.intel.com/content/www/us/en/develop/documentation/get-started-with-mkl-for-dpcpp/top.html">Intel MKL</a> which is a vendor-optimized BLAS/LAPACK implementation for Intel CPUs &#40;which works on AMD CPUs as well&#41;. Given its licensing, most open source projects cannot &#40;and thus do not&#41; ship with a binary of MKL even though it is known to perform better in many circumstances.</p> <p>In the Julia world and juiced with a need for speed, a package wrapping Intel&#39;s MKL, <a href="https://github.com/JuliaLinearAlgebra/MKL.jl">MKL.jl</a>, exists and does a global swap of the BLAS/LAPACK bindings from the current library &#40;default OpenBLAS&#41; to MKL. The underlying mechanism of this swap, <a href="https://github.com/JuliaLinearAlgebra/libblastrampoline">libblastrampoline</a>, has a fantastic <a href="https://www.youtube.com/watch?v&#61;t6hptekOR7s">video by Elliot Saba from JuliaCon 2021</a>. However, as a maintainer of one of the widest used Julia package organizations out there &#40;with ~25&#37; of all unique IPs downloading the SciMLBase.jl package in some form according to the package server&#41;, it would be very disruptive to simply add <code>using MKL</code> to our codebase and do a global preference swap for the user. Telling users to do this in documentation would mean that one has &quot;bad performance by default&quot; unless they read deep into the documentation, an unsettling result for any Julia developer. So we looked to build an alternative.</p> <p>Because the stiff ODE solvers only required a single LAPACK operation to solve <code>Ax &#61; b</code>, the LU-factorization performed within the call <code>A\b</code>, we developed a pure Julia implementation of the LU-factorization as <a href="https://github.com/JuliaLinearAlgebra/RecursiveFactorization.jl">RecursiveFactorization.jl</a>. </p> <p>&#40;Okay I lied, after the LU-factorization you need to do a backsolve, which is performed by <a href="https://github.com/JuliaSIMD/TriangularSolve.jl">TriangularSolve.jl</a>, but that&#39;s a small detail so let&#39;s get back to the main story&#41;</p> <p>This RecursiveFactorization.jl used tools from the <a href="https://github.com/JuliaSIMD">JuliaSIMD</a> stack, mainly <a href="https://github.com/JuliaSIMD/LoopVectorization.jl">LoopVectorization.jl</a> and <a href="https://github.com/JuliaSIMD/Polyester.jl">Polyester.jl&#39;s low-overhead threading model</a>, to generate architecture-specific compute kernels with efficient multithreading. After a great lift done by @chriselrod and @yingboma on this front, the results became very clear that this new LU-factorization codebase completely stomped OpenBLAS out of the water, achieving more than a 2x performance boost for matrices smaller than 500x500. But surprisingly, on some CPU architectures RecursiveFactorization.jl was seeing up to 50&#37; over the well-optimized MKL library &#40;and interestingly, there&#39;s a heavy correlation between &quot;seeing a really good result&quot; and &quot;having benchmarked on an AMD CPU&quot;&#41;. More details about this can be found <a href="https://github.com/JuliaLinearAlgebra/RecursiveFactorization.jl/pull/28">in a pull request</a> and in <a href="https://www.youtube.com/watch?v&#61;KQ8nvlURX4M">Chris Elrod&#39;s JuliaCon 2021 talk on pure Julia linear algebra functions</a>.</p> <p><img src="https://user-images.githubusercontent.com/8043603/124346090-dafeff80-dbaa-11eb-839a-c110cede6d34.png" alt="" /> <img src="https://user-images.githubusercontent.com/8043603/124346095-e2260d80-dbaa-11eb-97b4-062b0470150b.png" alt="" /></p> <p>With us now seeing results like:</p> <pre><code class="julia hljs">Progress:   <span class=hljs-number >6</span>%
█████████                                                                                                                           |  ETA: <span class=hljs-number >0</span>:<span class=hljs-number >49</span>:<span class=hljs-number >51</span>
  Size:                    (<span class=hljs-number >17</span>, <span class=hljs-number >17</span>)
  RecursiveFactorization:  (MedianGFLOPS = <span class=hljs-number >3.053</span>, MaxGFLOPS = <span class=hljs-number >5.323</span>)
  MKL:                     (MedianGFLOPS = <span class=hljs-number >2.047</span>, MaxGFLOPS = <span class=hljs-number >2.198</span>)
  OpenBLAS:                (MedianGFLOPS = <span class=hljs-number >2.509</span>, MaxGFLOPS = <span class=hljs-number >2.762</span>)
  
Progress:   <span class=hljs-number >6</span>%
████████                                                                                                                            |  ETA: <span class=hljs-number >0</span>:<span class=hljs-number >50</span>:<span class=hljs-number >05</span>
  Size:                    (<span class=hljs-number >486</span>, <span class=hljs-number >486</span>)
  RecursiveFactorization:  (MedianGFLOPS = <span class=hljs-number >61.48</span>, MaxGFLOPS = <span class=hljs-number >63.66</span>)
  MKL:                     (MedianGFLOPS = <span class=hljs-number >44.45</span>, MaxGFLOPS = <span class=hljs-number >46.02</span>)
  OpenBLAS:                (MedianGFLOPS = <span class=hljs-number >30.56</span>, MaxGFLOPS = <span class=hljs-number >31.42</span>)</code></pre> <p>meaning about 1.5x-2x faster than what we had before, it was a no-brainer to incorporate this into the ODE solver stack. Stiff ODEs are very LU-factorization bound, and therefore 2x faster LU-factorizations can mean about a 2x performance improvement.</p> <p>But of course, we now have replaced a prebuilt C binary with a just-in-time &#40;JIT&#41; compiled Julia code and thus had to pay the JIT price in new sessions. This JIT price, with its automated CPU architecture detection and specialization, cost over 17 seconds, and was thus the major player in our 22 seconds first <code>solve</code> time. Ouch.</p> <h2 id=solving_the_recursivefactorization_compile-times_taking_control_of_precompilation ><a href="#solving_the_recursivefactorization_compile-times_taking_control_of_precompilation" class=header-anchor >Solving the RecursiveFactorization Compile-Times: Taking Control of Precompilation</a></h2> <p>Instead of backing down from this challenge, we decided to just figure out how to make Julia&#39;s precompilation system better and work for us. With a <a href="https://sciml.ai/news/2021/08/31/czi/">newly received CZI grant to the SciML organization</a>, we called in the help of <a href="https://neuroscience.wustl.edu/people/timothy-holy-phd/">Dr. Tim Holy</a>, one of Julia&#39;s core compiler engineers behind the precompilation tooling, to help us untangle this mess. Our goal was to make as few performance comprimises as possible but achieve 0.1 seconds of compile-time. Luckily, our goal was in reach. Let&#39;s take another look at that compilation profile:</p> <p><img src="https://user-images.githubusercontent.com/1814174/129282082-ac51270f-5843-4bcc-a452-8aa663c458b8.png" alt="" /></p> <p>The largest chunk was the DiffEqBase.jl linear solver code, which we now know is almost entirely due to the compilation of RecursiveFactorization.jl. However, the process <code>A\b</code> on 64-bit floating point numbers is a very standard thing which could in theory be compiled once and reused in all sessions. There were two major questions to solve:</p> <ul> <li><p>Why is Julia&#39;s precompilation mechanism not storing this LU-factorization call?</p> <li><p>How do we improve the compile-times of things which are not precompiled?</p> </ul> <p>Let&#39;s dig in.</p> <h3 id=why_is_julias_precompilation_mechanism_not_storing_this_lu-factorization_call ><a href="#why_is_julias_precompilation_mechanism_not_storing_this_lu-factorization_call" class=header-anchor >Why is Julia&#39;s precompilation mechanism not storing this LU-factorization call?</a></h3> <p>The answer to the first question comes down to the interaction between package precompilation and multiple dispatch. Let&#39;s assume we had the package:</p> <pre><code class="julia hljs"><span class=hljs-keyword >module</span> MyPackage
f(x,y) = x * y
<span class=hljs-keyword >end</span></code></pre> <p>How long should precompilation of this package take? On one hand the answer is &quot;that should be quick, it&#39;s simple&#33;&quot;. However, <code>f&#40;x,y&#41;</code> is unbounded on the types that it can take, so therefore you <em>could</em> precompile a whole lot of different methods of <code>f&#40;x,y&#41;</code>. And indeed if you want to fully cover all of the possibilities for what <code>f&#40;x::T,y::T2&#41;</code> could call, you have a lot of possibilities. You have <code>f&#40;::Float64,::Float64&#41;</code>, <code>f&#40;::Int32, ::Int32&#41;</code>, etc. we were thinking about numbers, but <code>&quot;hi &quot; * &quot;there&quot;</code> is also valid since <code>*</code> in Julia is string concatenation. <code>f&#40;::Dict,::Dict&#41;</code> is a method too: it throws an error, but it&#39;s a valid method of <code>f</code>. And so on. If you have <code>n</code> types in your Julia setup, then <code>f</code> has <code>n^2</code> possible methods. Precompilation doesn&#39;t sound so simple anymore?</p> <p>Thus in order to prevent a combinatoric explosion in compile times, Julia does not eagerly compile every possible method to <code>f</code> that could be called. Instead, it chooses to precompile functions based on the methods of <code>f</code> that it actually sees used. Because precompilation occurs at the time of <code>using</code>, i.e. when the user first calls <code>using MyPackage</code> or <code>import MyPackage</code> &#40;note in v1.7&#43; that&#39;s now moved to the package installation time in order to be performed in parallel&#41;, the methods of <code>f</code> that are precompiled are the methods of <code>f</code> which are called in the top-level of the module during <code>using</code> time. </p> <p>In other words, since in our module <code>MyPackage</code> <code>f</code> is defined but no methods of <code>f</code> are used, there are no methods of <code>f</code> which are precompiled. Yay, we stopped combinatorial precompilation growth&#33;</p> <p>This alludes to a simple fix: just call the methods that you need. For example, in the example <code>MyPackage</code>, we can force the precompilation mechanism to precompile <code>f</code> on many standard number types by doing:</p> <pre><code class="julia hljs"><span class=hljs-keyword >module</span> MyPackage
f(x,y) = x * y

<span class=hljs-keyword >let</span>
  f(<span class=hljs-number >1.0</span>,<span class=hljs-number >1.0</span>)
  f(<span class=hljs-number >1</span>,<span class=hljs-number >1</span>)
  f(<span class=hljs-built_in >Int32</span>(<span class=hljs-number >1</span>),<span class=hljs-number >1</span>)
  f(<span class=hljs-number >1.0</span>,<span class=hljs-built_in >Int128</span>(<span class=hljs-number >1</span>))
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>and etc. You can make a loop over the types you want and do the combination of all calls. Of course this has two downsides. One has to be semi-explicit about what to precompile. By semi-explicit I mean that you do not necessarily have to call <code>f</code> on every type combination, but you do need to call some function which calls <code>f</code> on that type combination. Because most packages tend to have a large number of commonly reused functions, this means that a few top-level calls will cause &quot;most&quot; of the useful parts of the package to precompile, so it&#39;s not that much of a limitation, but still it&#39;s something to consider. And secondly, this requires the function to be run at <code>using</code> time. </p> <p>To solve this second problem, on July 25th 2022 Tim Holy released a new package, <a href="https://discourse.julialang.org/t/ann-new-package-snoopprecompile/84778">SnoopPrecompile.jl</a>, which allows the internal calls of such a block to be &quot;snooped&quot;, making it so the function calls do not have to be run at <code>using</code> time. Thus the &quot;proper&quot; form of <code>MyPackage</code> to force compilation now looks like:</p> <pre><code class="julia hljs"><span class=hljs-keyword >module</span> MyPackage
f(x,y) = x * y

<span class=hljs-keyword >import</span> SnoopPrecompile
SnoopPrecompile.<span class=hljs-meta >@precompile_all_calls</span> <span class=hljs-keyword >begin</span>
  f(<span class=hljs-number >1.0</span>,<span class=hljs-number >1.0</span>)
  f(<span class=hljs-number >1</span>,<span class=hljs-number >1</span>)
  f(<span class=hljs-built_in >Int32</span>(<span class=hljs-number >1</span>),<span class=hljs-number >1</span>)
  f(<span class=hljs-number >1.0</span>,<span class=hljs-built_in >Int128</span>(<span class=hljs-number >1</span>))
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>which will trace the function calls at precompilation time but &quot;turn off&quot; the calls for normal usings.</p> <p>So what was the answer to the first question:</p> <blockquote> <p>Why is Julia&#39;s precompilation mechanism not storing this LU-factorization call?</p> </blockquote> <p>The answer was that nobody told it to. So now, you see a precompile snoop <a href="https://github.com/SciML/OrdinaryDiffEq.jl/blob/v6.27.1/src/OrdinaryDiffEq.jl#L207-L333">at the top level of the OrdinaryDiffEq.jl module</a> which covers the standard ODE solver calls, which then causes the internals such as RecursiveFactorization.jl to be snooped and thus be precompiled. Therefore, problem solved... on Julia v1.8.</p> <h3 id=why_only_on_julia_v18_what_changed_to_allow_for_more_precompilation ><a href="#why_only_on_julia_v18_what_changed_to_allow_for_more_precompilation" class=header-anchor >Why only on Julia v1.8? What changed to allow for &quot;more&quot; precompilation?</a></h3> <p>I&#39;m glad you asked. The reason is because a major change in the Julia compiler stack from Tim Holy which was introduced in Julia v1.8 is required in order to allow for almost all &#40;I&#39;ll describe &quot;almost all&quot;&#41; calls to precompile. If you look at the <a href="https://docs.julialang.org/en/v1/NEWS/#Compiler/Runtime-improvements">Julia v1.8 release notes</a> you&#39;ll see an obscure mention of a change in the Julia compiler:</p> <blockquote> <p>Precompilation &#40;with explicit precompile directives or representative workloads&#41; now saves more type-inferred code, resulting in reduced time-to-first task for packages that use precompilation. This change also eliminates the runtime performance degradation occasionally triggered by precompilation on older Julia versions. More specifically, any newly-inferred method/type combinations needed by your package–regardless of where those methods were defined–can now be cached in the precompile file, as long as they are inferrably called by a method owned by your package &#40;#43990&#41;.</p> </blockquote> <p>Let&#39;s break down what <a href="https://github.com/JuliaLang/julia/pull/43990">this pull request</a> is actually doing. Assume we have the <code>MyPackage</code> package from before:</p> <pre><code class="julia hljs"><span class=hljs-keyword >module</span> MyPackage
f(x,y) = x * y

<span class=hljs-keyword >import</span> SnoopPrecompile
SnoopPrecompile.<span class=hljs-meta >@precompile_all_calls</span> <span class=hljs-keyword >begin</span>
  f(<span class=hljs-number >1.0</span>,<span class=hljs-number >1.0</span>)
  f(<span class=hljs-number >1</span>,<span class=hljs-number >1</span>)
  f(<span class=hljs-built_in >Int32</span>(<span class=hljs-number >1</span>),<span class=hljs-number >1</span>)
  f(<span class=hljs-number >1.0</span>,<span class=hljs-built_in >Int128</span>(<span class=hljs-number >1</span>))
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>and now assume we build a package which builds on top of the functionality of <code>MyPackage</code>:</p> <pre><code class="julia hljs"><span class=hljs-keyword >module</span> MyPackage2
g(x,y) = f(x,y) + x
<span class=hljs-keyword >end</span></code></pre> <p>Now, just like before, I want to reduce the time to first calls of <code>g</code> by forcing precompilation. Let&#39;s do it the same way as on <code>MyPackage</code>:</p> <pre><code class="julia hljs"><span class=hljs-keyword >module</span> MyPackage2
g(x,y) = f(x,y) + x

<span class=hljs-keyword >import</span> SnoopPrecompile
SnoopPrecompile.<span class=hljs-meta >@precompile_all_calls</span> <span class=hljs-keyword >begin</span>
  g(<span class=hljs-number >1.0</span>,<span class=hljs-number >1</span>)
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre> <p>Before Julia v1.8, this will <em>not</em> cause <code>g&#40;::Float64, ::Int&#41;</code> to be a precompiled method, and the reason is ownership. Our <code>MyPackage</code> has precompiled the methods <code>f&#40;::Float64, ::Float64&#41;</code>, <code>f&#40;::Int, ::Int&#41;</code>, <code>f&#40;::Int32, ::Int&#41;</code>, and <code>f&#40;::Float64, ::Int128&#41;</code>. However, because <code>g&#40;::Float64, ::Int&#41;</code> needs the method <code>f&#40;::Float64, ::Int&#41;</code> which is not part of the <code>MyPackage</code> precompilation, this would mean it does not have the necessary components to precompile and would discard the precompilation. </p> <p>The reason for this potential ownership issue is because <code>f</code> belongs to <code>MyPackage</code>, while the types in the signature, <code>Float64</code> and <code>Int</code>, belongs to Base. Because none of these entities belong to <code>MyPackage2</code>, the precompiled function cannot belong to <code>MyPackage2</code>, and because <code>MyPackage2</code> was the first to request this precompiled function it would then be discarded. This was the rule before Julia v1.8. The reason for this rule is because if this precompilation is invoked later in the process at the <code>MyPackage2</code> time, in order to not invalidate the precompilation of <code>MyPackage</code>, the new precompiled code would need to live with <code>MyPackage2</code>. &quot;So just put it in <code>MyPackage2</code>?&quot; The reason to be a bit conservative here is because if this precompiled function only exists in <code>MyPackage2</code>, you could have methods which duplicate. For example, <code>MyPackage3</code> might depend on <code>MyPackage</code> &#40;and importantly, not depend on <code>MyPackage2</code>&#41; and might ask to precompile the same method <code>f&#40;::Float64, ::Int&#41;</code>. If the precompiled file is to live with the first package to request it, you have two choices: either don&#39;t precompile <code>f&#40;::Float64, ::Int&#41;</code> at all, or have the precompiled <code>f&#40;::Float64, ::Int&#41;</code> in both <code>MyPackage2</code> and <code>MyPackage3</code>.</p> <p>For the earlier versions of the Julia precompilation system, the conservative approach of simply discarding such methods was the right approach. Because the number of precompiled functions increases the time to precompile and the time to load &#40;<code>using</code>&#41; a package, who knows what the effect would be on the true first <code>solve</code> time? A priori it&#39;s impossible to predict because it depends on how the packages decide to ask for precompilation. However, in 2022 we have much deeper dependency stacks, some packages having Base functionality defined 30 packages down, and if one package in the system misses the method that is required, much of precompilation could then be discarded. The question is then an empirical one: in our current package environment, is discarding such methods beneficial or detrimental? Tim Holy implemented a mechanism for packages to hold onto such &quot;external CodeInstances&quot; &#40;methods whose types are owned by a separate package&#41; and <a href="https://github.com/JuliaLang/julia/pull/43990">performed an empirical analysis in the PR</a>.</p> <p><img src="https://user-images.githubusercontent.com/1525481/154699530-b22023c9-8ad8-4329-a14c-ba354564bb29.png" alt="" /></p> <p>What this shows is that by moving from the v1.7 master behavior &#40;&quot;master&quot;&#41; to either full precompilation &#40;&quot;full&quot;&#41; or a pruned version &#40;&quot;prune&quot;&#41;, more is precompiled with the package <code>.ji</code> files, the load times are increased, and the &quot;time to first x&quot; &#40;i.e. the time to the first significant call, so for example the total time for the first solve as is measured at the top of this post&#41; is decreased. In other words, while this does lead to an increase in load times, the existence of the precompilation is beneficial enough to that the total startup time is still very significantly decreased. For this reason, the ownership requirement was dropped and now on Julia v1.8 and above, <code>f&#40;::Float64, ::Int&#41;</code> will get precompiled by the downstream packages.</p> <p>This leads us to two important conclusions for this section of the conversion: </p> <ol> <li><p>Julia v1.8 is required for precompilation to easily have the wished upon effects, as with the older ownership issues one had to deduce which package is missing a given method and ensure that package accept the required method into the package.</p> <li><p>On Julia v1.8 and higher, load times can be reduced by ensuring that core packages &#40;packages used by many other packages&#41; snoop the precompilation of methods which are widely used downstream. This will help ensure that as few methods as possible are duplicated, which will ultimately decrease the package load times for the ecosystem.</p> </ol> <h2 id=the_next_step_improving_using_times_via_requiresjl_removal_and_package_splitting ><a href="#the_next_step_improving_using_times_via_requiresjl_removal_and_package_splitting" class=header-anchor >The Next Step: Improving Using Times via Requires.jl Removal and Package Splitting</a></h2> <p>With SnoopPrecompile and the new changes in Julia v1.8, the full RecursiveFactorization call, along with the FastBroadcast.jl dispatches, finally precompiled. This dropped the total first <code>solve</code> call dropped from 22 seconds to 3 seconds when not accounting for <code>using</code> times. But, given that the precompilation ownership changes greatly increased the amount of precompiled code, <code>using</code> times began to matter a lot more and thus this started entering the measurements. Thus the &quot;real&quot; time went from around 30 seconds to around 15 seconds. Thus if we were going to make the new precompilation improvements more useful, we needed to start focusing on the <code>using</code> times as well.</p> <p>Luckily &#40;or rather, it wasn&#39;t much of a coincidence&#41; as the using times became more important, a new feature landed on the Julia v1.8 master in order <a href="https://docs.julialang.org/en/v1/NEWS/#InteractiveUtils">to better profile the <code>using</code> times</a>:</p> <blockquote> <p>New macro @time_imports for reporting any time spent importing packages and their dependencies, highlighting compilation and recompilation time as percentages per import &#40;#41612,#45064&#41;.</p> </blockquote> <p>This is what it looked like on a small package <a href="https://github.com/SciML/RecursiveArrayTools.jl">RecursiveArrayTools.jl</a> which defines some important array types used in the differential equation solvers:</p> <pre><code class="julia hljs">julia&gt; <span class=hljs-meta >@time_imports</span> <span class=hljs-keyword >using</span> RecursiveArrayTools
     <span class=hljs-number >10.7</span> ms    ┌ MacroTools
     <span class=hljs-number >19.2</span> ms  ┌ ZygoteRules
      <span class=hljs-number >2.8</span> ms  ┌ Compat
      <span class=hljs-number >1.4</span> ms  ┌ Requires
    <span class=hljs-number >123.4</span> ms  ┌ FillArrays
    <span class=hljs-number >507.7</span> ms  ┌ StaticArrays
     <span class=hljs-number >17.8</span> ms      ┌ Preferences
     <span class=hljs-number >19.6</span> ms    ┌ JLLWrappers
    <span class=hljs-number >184.0</span> ms  ┌ LLVMExtra_jll
      <span class=hljs-number >5.1</span> ms      ┌ CEnum
    <span class=hljs-number >108.6</span> ms    ┌ LLVM
      <span class=hljs-number >1.9</span> ms    ┌ Adapt
    <span class=hljs-number >804.4</span> ms  ┌ GPUArrays
      <span class=hljs-number >5.8</span> ms  ┌ DocStringExtensions
      <span class=hljs-number >1.3</span> ms  ┌ IfElse
     <span class=hljs-number >39.8</span> ms  ┌ RecipesBase
     <span class=hljs-number >40.6</span> ms    ┌ Static
    <span class=hljs-number >504.1</span> ms  ┌ ArrayInterface
     <span class=hljs-number >73.6</span> ms  ┌ ChainRulesCore
   <span class=hljs-number >2332.6</span> ms  RecursiveArrayTools</code></pre> <p>Yes, over 2 seconds to load what was one of the &quot;small&quot; dependencies. But how did we get here?</p> <p>The major steps that led to this were of course the precompilation changes, but that&#39;s only part of the story. Some of these core packages like <a href="https://github.com/JuliaArrays/ArrayInterface.jl">ArrayInterface.jl</a> define interface functions which require many downstream dependencies. For example, &quot;does this array type have fast indexing?&quot; is a question that needs to be asked an answered on every array type you want to use the <code>fast_scalar_indexing&#40;T&#41;</code> function on, so therefore <code>ArrayInterface.jl</code> needs to have depenencies on all of the array types one might use, from <a href="https://github.com/JuliaMatrices/BlockBandedMatrices.jl">BlockBandedMatrices.jl</a> to <a href="https://github.com/jonniedie/ComponentArrays.jl">ComponentArrays.jl</a>. </p> <p>For this reason, <code>ArrayInterface.jl</code> used to make use of the <a href="https://github.com/JuliaPackaging/Requires.jl">Requires.jl</a> system for conditional dependencies. However, packages which are used in an <code>@requires</code> block are incompatible with precompilation, since their load does not occur at the <code>using</code> time of the given package but at the <code>using</code> time of the downstream package, something which has not been specialized in the precompilation system. The first step to improve precompilation was thus to remove all conditional module loading and make it explicit. </p> <p>However, this led to many &quot;unnecessary&quot; dependencies propagating downstream. For example, in the RecursiveArrayTools.jl example, you see <code>804.4 ms ┌ GPUArrays</code>, i.e. a large portion of the RecursiveArrayTools load time was due to needing to define one method:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Allow converting a VectorOfArray to a GPU-based Array</span>
Base.convert(T::<span class=hljs-built_in >Type</span>{&lt;:GPUArrays.AbstractGPUArray}, VA::AbstractVectorOfArray) = T(VA)</code></pre> <p>However, this led to the observation that there is no need for the abstract types like <code>GPUArrays.AbstractGPUArray</code> to live in the &quot;functionality&quot; package. In a sense, there is room for &quot;interface&quot; packages to define the core interfaces, functions like &quot;this is an abstract GPU array&quot;, which is separate from a package that defined &quot;this is how you do math on a GPU array&quot;. The former is would be a small package with almost 0 load time, while the latter is &quot;all of the hard work&quot; and only required by packages which want to do GPU computing. </p> <p>So without further ado, the great splitting of 2022 was commenced. The main packages which needed this treatment were:</p> <ul> <li><p>ArrayInterface.jl, which could split the interface definitions from its instantiations on downstream packages</p> <li><p>GPUArrays.jl, which could split the definition of a GPU array from the instantiation of GPU-based functionality. This would allow packages to be able to query &quot;is this a GPU array?&quot; with almost no load time penalty, allowing for easy separate GPU-safe code paths. This is important because GPUs do not support fast scalar indexing, i.e. <code>A&#91;1&#93;</code> is not a good operation on GPUs, so it&#39;s something you want to query and avoid.</p> <li><p>StaticArrays.jl, which could split the definition of a static array from the implementation of static array functionality. This would allow packages to be able to query for &quot;is this array a static array?&quot;, which is important because static arrays do not support <code>setindex&#33;</code>, i.e. <code>A&#91;1&#93; &#61; x</code>. The actual static array package load times are rather intense because it defines many size-specialized versions of arithmetic functions.</p> </ul> <p>This lead to the development of <a href="https://github.com/JuliaArrays/ArrayInterface.jl/tree/master/lib/ArrayInterfaceCore">ArrayInterfaceCore.jl</a> &#40;<a href="https://github.com/JuliaArrays/ArrayInterface.jl/issues/211">relevant issue</a>&#41;, <a href="https://github.com/JuliaGPU/GPUArrays.jl/tree/master/lib/GPUArraysCore">GPUArraysCore.jl</a> &#40;<a href="https://github.com/JuliaGPU/GPUArrays.jl/issues/409">relevant issue</a>&#41;, and <a href="https://github.com/JuliaArrays/StaticArraysCore.jl">StaticArraysCore.jl</a> &#40;<a href="https://github.com/JuliaArrays/StaticArrays.jl/issues/1023">relevant issue</a>&#41;.</p> <p>Now the overload became:</p> <pre><code class="julia hljs"><span class=hljs-keyword >import</span> GPUArraysCore
<span class=hljs-comment ># Allow converting a VectorOfArray to a GPU-based Array</span>
Base.convert(T::<span class=hljs-built_in >Type</span>{&lt;:GPUArraysCore.AbstractGPUArray}, VA::AbstractVectorOfArray) = T(VA)</code></pre> <p>As a result, the load time of RecursiveArrayTools.jl <a href="https://github.com/SciML/RecursiveArrayTools.jl/pull/217">decreased dramatically</a>:</p> <pre><code class="julia hljs"><span class=hljs-meta >@time_imports</span> <span class=hljs-keyword >using</span> SciMLBase
    <span class=hljs-number >10.4</span> ms    ┌ MacroTools
     <span class=hljs-number >19.0</span> ms  ┌ ZygoteRules
      <span class=hljs-number >3.8</span> ms  ┌ Compat
      <span class=hljs-number >1.5</span> ms    ┌ Adapt
      <span class=hljs-number >3.7</span> ms    ┌ ArrayInterfaceCore
      <span class=hljs-number >2.0</span> ms    ┌ StaticArraysCore
      <span class=hljs-number >9.7</span> ms  ┌ ArrayInterfaceStaticArraysCore
    <span class=hljs-number >123.1</span> ms  ┌ FillArrays
      <span class=hljs-number >5.0</span> ms  ┌ DocStringExtensions
     <span class=hljs-number >18.2</span> ms  ┌ RecipesBase
     <span class=hljs-number >51.3</span> ms  ┌ ChainRulesCore
      <span class=hljs-number >4.0</span> ms  ┌ GPUArraysCore
    <span class=hljs-number >292.7</span> ms  RecursiveArrayTools</code></pre> <p>There is still work to be done &#40;a FillArraysCore.jl is probably required&#41;, but one can see the massive effect this has on the ecosystem.</p> <h4 id=a_note_on_total_using_times ><a href="#a_note_on_total_using_times" class=header-anchor >A Note on Total Using Times</a></h4> <p>Now an astute reader may look at this and go &quot;but wait, if I am going to use static arrays, won&#39;t I still need to pay the full price of StaticArrays.jl loading at some time?&quot; Yes you will, but it turns out that delaying large overloading imports as late as possible leads to larger than expected loading time improvements. One big reason is because it reduces the amount of code that gets invalidated: we will get to invalidations right after this. But the second reason is simple. StaticArrays.jl adds a bunch of methods for <code>&#43;</code> between different array sizes. <code>&#43;&#40;::SVector&#123;Size N, T&#125;,::SVector&#123;Size N, T&#125;&#41;</code> for every size <code>N</code> is added, combinations between <code>MArray</code> and <code>SArray</code>, etc. are added. It adds overloads to <code>lu</code>, <code>qr</code>, ...: StaticArrays adds something to everything. Thus when the compiler is looking up what method to use for <code>lu</code> downstream, there ends up being a lot of &quot;StaticArrays junk&quot; to sift through, and this increases the compile times. This is thus reduced by not adding these extra methods until they are necessary.</p> <p>So with the move to using core interface packages separated from the functionality, one chunk of the using time gains was due to removing functionality that was only used by a small subset of users &#40;for example, the 1 second of extra using time on RecursiveArrayTools that was only due to allowing GPU support&#41;, while the other chunk was due to delaying large imports until further down in the process. good question good reader&#33; Now back to the regularly scheduled programming.</p> <h2 id=the_story_of_invalidations_how_you_accidentally_delete_all_of_your_good_precompilation_work ><a href="#the_story_of_invalidations_how_you_accidentally_delete_all_of_your_good_precompilation_work" class=header-anchor >The Story of Invalidations: How You Accidentally Delete All of Your Good Precompilation Work</a></h2> <p>Even with all of these changes, in some cases the cost of <code>using</code> could remain high. The reason is invalidations. To illustrate where invalidations can come from, let&#39;s look at the following example. Let&#39;s say in package MyPackage I define the function:</p> <pre><code class="julia hljs">f(x::<span class=hljs-built_in >Number</span>) = x <span class=hljs-keyword >isa</span> <span class=hljs-built_in >AbstractFloat</span></code></pre>
<p>and then I use it in a function <code>g</code>:</p>
<pre><code class="julia hljs">g(a::<span class=hljs-built_in >AbstractArray</span>) = sum(f,a)</code></pre>
<p><code>g</code> is thus a function that checks whether every element of some AbstractArray <code>a</code> is an <code>AbstractFloat</code>. Now, for strongly typed arrays <code>a</code>, such as <code>a::Array&#123;Float64,2&#125;</code>, <code>f&#40;x&#41;</code> always returns a <code>Bool</code> based on <code>eltype&#40;a&#41;</code>. and thus <code>f&#40;a&#91;i&#93;&#41;</code> always returns a <code>Bool</code>, and thus the <code>sum</code> function ends up iterating <code>tmp &#61; f&#40;a&#91;i&#93;&#41;</code> where <code>tmp::Int</code>, accumulating the booleans. But what if <code>a::Array&#123;Number,2&#125;</code>? In this case, the element type of <code>a</code> is not concrete, so with <code>a &#61; Number&#91;1.0, 1, 1f0&#93;</code>, one cannot deduce at compile time the element type of <code>a&#91;i&#93;</code>. However, even though all one knows is that <code>typeof&#40;a&#91;i&#93;&#41; &lt;: Number</code>, it turns out that for every <code>T &lt;: Number</code>, <code>T isa AbstractFloat</code> returns a Bool. Thus in some sense, at the compiler level, we can deduce that <code>f&#40;a::Number&#41;::Bool</code>. Given this fact, even without concrete type information on the elements of <code>a</code> we can still deduce the at <code>f&#40;a&#91;i&#93;&#41;::Bool</code> and make sure the return of <code>f</code> is unboxed, making this loop still be relatively fast by removing dynamic dispatch. </p>
<p>This assumption is known colloquially &#40;by very few people&#41; as the world-splitting optimization. Essentially, if the method table has 4 or fewer potential output types, then Julia&#39;s compiler can generate code that uses explicit branching instead of dynamic dispatch. This is important because dynamic dispatch involves checking the global method table for matching types and resolving dispatches at runtime, a very expensive process in comparison to <code>if T isa Number</code>. </p>
<p>However, the existence of this world-splitting leaves Julia&#39;s compiler open to having to invalidate old cached code if the assumptions change. One major example we have found is that the default <code>&#33;</code> function in Julia &#40;<code>&#33;x</code> is &quot;not x&quot;&#41; always expects <code>&#33;&#40;::Bool&#41;::Bool</code>, and thus the compiler specializes on the fact that the return should always be a <code>Bool</code>. But what happens if someone violates this assumption? For example, what if someone created a  <a href="https://github.com/SciML/Static.jl">Static.jl</a> with a static compile-time type-based <code>False</code> and <code>True</code> type? If that&#39;s the case then it would make sense that <code>&#33;&#40;::False&#41; &#61; True&#40;&#41;</code> and <code>&#33;&#40;::True&#41; &#61; False&#40;&#41;</code>. But what needs to be recompiled if we do this?</p>
<pre><code class="julia hljs">julia&gt; show(trees[<span class=hljs-keyword >end</span>-<span class=hljs-number >1</span>])
inserting !(::Static.False) <span class=hljs-keyword >in</span> Static at C:\Users\accou\.julia\packages\Static\sVI3g\src\Static.jl:<span class=hljs-number >427</span> invalidated:
   mt_backedges:   <span class=hljs-number >1</span>: signature <span class=hljs-built_in >Tuple</span>{typeof(!), <span class=hljs-built_in >Any</span>} triggered MethodInstance <span class=hljs-keyword >for</span> !=(::<span class=hljs-built_in >AbstractFloat</span>, ::<span class=hljs-built_in >AbstractFloat</span>) (<span class=hljs-number >0</span> children)
<span class=hljs-number >2</span>: signature <span class=hljs-built_in >Tuple</span>{typeof(!), <span class=hljs-built_in >Any</span>} triggered MethodInstance <span class=hljs-keyword >for</span> Base.isbadzero(::typeof(min), ::<span class=hljs-built_in >AbstractFloat</span>) (<span class=hljs-number >0</span> children)
<span class=hljs-number >3</span>: signature <span class=hljs-built_in >Tuple</span>{typeof(!), <span class=hljs-built_in >Any</span>} triggered MethodInstance <span class=hljs-keyword >for</span> Base.CoreLogging.<span class=hljs-string >var&quot;#handle_message#2&quot;</span>(::Base.Pairs{<span class=hljs-built_in >Symbol</span>, V, <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Vararg</span>{<span class=hljs-built_in >Symbol</span>, N}}, <span class=hljs-built_in >NamedTuple</span>{names, T}} <span class=hljs-keyword >where</span> {V, N, names, T&lt;:<span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Vararg</span>{<span class=hljs-built_in >Any</span>, N}}}, ::typeof(Base.CoreLogging.handle_message), ::Base.CoreLogging.SimpleLogger, ::Base.CoreLogging.LogLevel, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>) (<span class=hljs-number >0</span> children)
<span class=hljs-number >4</span>: signature <span class=hljs-built_in >Tuple</span>{typeof(!), <span class=hljs-built_in >Any</span>} triggered MethodInstance <span class=hljs-keyword >for</span> Base.CoreLogging.<span class=hljs-string >var&quot;#handle_message#2&quot;</span>(::Base.Pairs{<span class=hljs-built_in >Symbol</span>, _A, <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Symbol</span>}, <span class=hljs-built_in >NamedTuple</span>{names, T}} <span class=hljs-keyword >where</span> {_A, names, T&lt;:<span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Vararg</span>{<span class=hljs-built_in >Any</span>, N}}}, ::typeof(Base.CoreLogging.handle_message), ::Base.CoreLogging.SimpleLogger, ::Base.CoreLogging.LogLevel, ::LazyString, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Symbol</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>) (<span class=hljs-number >0</span> children)
<span class=hljs-number >5</span>: signature <span class=hljs-built_in >Tuple</span>{typeof(!), <span class=hljs-built_in >Any</span>} triggered MethodInstance <span class=hljs-keyword >for</span> Base.CoreLogging.<span class=hljs-string >var&quot;#handle_message#2&quot;</span>(::Base.Pairs{<span class=hljs-built_in >Symbol</span>, _A, <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Symbol</span>}, <span class=hljs-built_in >NamedTuple</span>{names, T}} <span class=hljs-keyword >where</span> {_A, names, T&lt;:<span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Vararg</span>{<span class=hljs-built_in >Any</span>, N}}}, ::typeof(Base.CoreLogging.handle_message), ::Base.CoreLogging.SimpleLogger, ::Base.CoreLogging.LogLevel, ::<span class=hljs-built_in >String</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Symbol</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>) (<span class=hljs-number >0</span> children)

.
.
.</code></pre>
<p>I cut this short because <a href="https://github.com/SciML/DifferentialEquations.jl/issues/786#issuecomment-1221515190">it ends up being &gt;180 method signatures which get invalidated</a> What this means is that after this method is added, the any method which specialized on the fact that that the output has to be <code>::Bool</code> now can no longer make this assumption, and has to be recompiled. That means that any precompilation cache which hits any of these methods needs to be discarded. Ouch&#33;</p>
<p>Note that this does not mean that every use of <code>&#33;</code> is discarded. Only cases where Julia&#39;s compiler could not infer the type of <code>&#33;</code> need to be invalidated, since if it can infer that <code>x::Bool</code>, then it can still know that <code>&#33;x::Bool</code> and thus no invalidation occurs. This is one principle to take away from this discussion:</p>
<p><strong>Invalidations and other bad compile-time things have a bigger chance of occuring on uninferred code</strong></p>
<p>In other words, making sure that code is type-stable and easy to infer can have many different compile-time benefits.</p>
<h4 id=quick_note_about_invalidation_sources ><a href="#quick_note_about_invalidation_sources" class=header-anchor >Quick Note About Invalidation Sources</a></h4>
<p>While we have found the world-splitting optimization to be one of the most common ways that large-scale invalidations can occur, it is by no means the only way. If someone implements a function <code>g&#40;x::Number,y::Number&#41; &#61; f&#40;x&#41; &#43; y</code> where <code>f&#40;x::Number&#41;</code> is the only definition that exists, and some  other package comes along and adds a dispatch <code>f&#40;x::Float64&#41;</code>, such a dispatch can invalidate the previous definitions of <code>g&#40;::Float64,::Number&#41;</code> by changing its behavior. This means another major source of invalidations is  <a href="https://docs.julialang.org/en/v1/manual/style-guide/#Avoid-type-piracy">type-piracy, which is something you shouldn&#39;t do</a>. Thus, avoid type-piracy and try to make code as well-inferred as possible and invalidations as an issue are fairly minimized. Now back to the show.</p>
<h3 id=profiling_and_fixing_sources_of_invalidations ><a href="#profiling_and_fixing_sources_of_invalidations" class=header-anchor >Profiling and Fixing Sources of Invalidations</a></h3>
<p>Now knowing that the effect of invalidations is to throw away for hardworking precompilation caches, fixing first call times definitely requires identifying if there are any invalidation sources which require removal. The following code uses SnoopCompile to profile the invalidation sources:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> SnoopCompile
invalidations = <span class=hljs-meta >@snoopr</span> <span class=hljs-keyword >begin</span>
    <span class=hljs-keyword >using</span> OrdinaryDiffEq

    <span class=hljs-keyword >function</span> lorenz(du, u, p, t)
        du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>] - u[<span class=hljs-number >1</span>])
        du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>] * (<span class=hljs-number >28.0</span> - u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
        du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>] * u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span> / <span class=hljs-number >3</span>) * u[<span class=hljs-number >3</span>]
    <span class=hljs-keyword >end</span>
    u0 = [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>]
    tspan = (<span class=hljs-number >0.0</span>, <span class=hljs-number >100.0</span>)
    prob = ODEProblem{<span class=hljs-literal >true</span>,<span class=hljs-literal >false</span>}(lorenz, u0, tspan)
    alg = Rodas5()
    tinf = solve(prob, alg)
<span class=hljs-keyword >end</span>;

trees = SnoopCompile.invalidation_trees(invalidations);

<span class=hljs-meta >@show</span> length(SnoopCompile.uinvalidated(invalidations)) <span class=hljs-comment ># show total invalidations</span>

show(trees[<span class=hljs-keyword >end</span>]) <span class=hljs-comment ># show the most invalidated method</span>

<span class=hljs-comment ># Count number of children (number of invalidations per invalidated method)</span>
n_invalidations = map(trees) <span class=hljs-keyword >do</span> methinvs
    SnoopCompile.countchildren(methinvs)
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >import</span> Plots
Plots.plot(
    <span class=hljs-number >1</span>:length(trees),
    n_invalidations;
    markershape=:circle,
    xlabel=<span class=hljs-string >&quot;i-th method invalidation&quot;</span>,
    label=<span class=hljs-string >&quot;Number of children per method invalidations&quot;</span>
)</code></pre>
<p>Here&#39;s the result of the two snapshots. December 24th, 2021:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/147303586-573258fc-cd0c-4548-b95d-b11dce55604a.png" alt="" /></p>
<p>August 21st, 2022:</p>
<p><img src="https://user-images.githubusercontent.com/1814174/185786190-28a1d8b5-2027-475c-8112-b9388230daaa.png" alt="" /></p>
<p>The invalidation reductions generally were as simple as removing a dispatch from some package. Generally these dispatches were so weird that we could find no code actually using the dispatch. Some cases were:</p>
<ul>
<li><p><a href="https://github.com/JuliaDiff/ChainRulesCore.jl/pull/524">An <code>&#61;&#61;</code> for a specific type that was only used in ChainRulesCore testing</a></p>

<li><p><a href="https://github.com/PainterQubits/Unitful.jl/pull/509">A Unitful dispatch that broke standard rules of element type promotion</a></p>

<li><p><a href="https://github.com/JuliaFolds/InitialValues.jl/pull/64">A <code>reduce_empty</code> overload that had a comment &quot;Not used in Transducers.jl ATM&quot;</a></p>

</ul>
<p>Generally, dispatches which do weird things are not useful because they break the convention of the function they are overloading, and thus are hard to actually make use of in generic code. Thus the biggest invalidators tend to be signs of bad coding anyways, and these fixes simply led to greener pastures along with better compile times. There are a few tricker ones, like  <a href="https://github.com/SciML/Static.jl/pull/78">the removal of <code>&#33;</code> overloads from Static.jl removing a key features</a> which was instead <a href="https://github.com/JuliaLang/julia/pull/46490">mitigated by PRs to Base</a> which <a href="https://github.com/JuliaLang/julia/pull/46491">add explicit <code>::Bool</code> type assertions where it is assumed</a> in <a href="https://github.com/JuliaLang/julia/pull/46481">order to make the dispatches</a> not recompile when <code>&#33;&#40;::True&#41;::Bool</code> is added. </p>
<p>However, for the most part, a few improvements and a few changes to Base made the biggest invalidators go away. While this didn&#39;t have a major effect on the test code from the start of this post, it did have a major effect on the related problem of automatic differentiation on the solver. In that case, if we wanted to take the gradient of the solution to the ODE:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq, SnoopCompile, ForwardDiff

lorenz = (du,u,p,t) -&gt; <span class=hljs-keyword >begin</span>
        du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>]-u[<span class=hljs-number >1</span>])
        du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>]*(<span class=hljs-number >28.0</span>-u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
        du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>]*u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span>/<span class=hljs-number >3</span>)*u[<span class=hljs-number >3</span>]
<span class=hljs-keyword >end</span>

u0 = [<span class=hljs-number >1.0</span>;<span class=hljs-number >0.0</span>;<span class=hljs-number >0.0</span>]; tspan = (<span class=hljs-number >0.0</span>,<span class=hljs-number >100.0</span>);
prob = ODEProblem(lorenz,u0,tspan); alg = Rodas5();
tinf = <span class=hljs-meta >@snoopi_deep</span> ForwardDiff.gradient(u0 -&gt; sum(solve(ODEProblem(lorenz,u0,tspan),alg)), u0)
tinf = <span class=hljs-meta >@snoopi_deep</span> ForwardDiff.gradient(u0 -&gt; sum(solve(ODEProblem(lorenz,u0,tspan),alg)), u0)</code></pre>
<p>then we saw that before handling the invalidations, almost all precompilation caches were discarded:</p>
<pre><code class="julia hljs"><span class=hljs-comment >#First</span>
InferenceTimingNode: <span class=hljs-number >1.849625</span>/<span class=hljs-number >14.538148</span> on Core.Compiler.Timings.ROOT() with <span class=hljs-number >32</span> direct children

<span class=hljs-comment >#Second</span>
InferenceTimingNode: <span class=hljs-number >1.531660</span>/<span class=hljs-number >4.170409</span> on Core.Compiler.Timings.ROOT() with <span class=hljs-number >12</span> direct children</code></pre>
<p>while after invalidations were handled, the precompilation caches were mostly kept and thus inference time dropped dramatically:</p>
<pre><code class="julia hljs"><span class=hljs-comment >#First</span>
InferenceTimingNode: <span class=hljs-number >1.181086</span>/<span class=hljs-number >3.320321</span> on Core.Compiler.Timings.ROOT() with <span class=hljs-number >32</span> direct children

<span class=hljs-comment >#Second</span>
InferenceTimingNode: <span class=hljs-number >0.998814</span>/<span class=hljs-number >1.650488</span> on Core.Compiler.Timings.ROOT() with <span class=hljs-number >11</span> direct children</code></pre>
<p>That&#39;s about 11 seconds chopped off the first solve time for the gradient case. But what about those &quot;direct children&quot;?</p>
<h4 id=quick_note_many_invalidations_dont_matter ><a href="#quick_note_many_invalidations_dont_matter" class=header-anchor >Quick Note: Many Invalidations Don&#39;t Matter</a></h4>
<p>Invalidations happen. If a single method is invalidated, it&#39;s not a big deal. If it&#39;s a core method that&#39;s then invalidating 1000&#39;s of children calls, that&#39;s a huge compile-time deal. Thus track down the major invalidations, but most cases are simply fine to leave alone.</p>
<h2 id=ambiguity_resolution ><a href="#ambiguity_resolution" class=header-anchor >Ambiguity Resolution</a></h2>
<p>Before we get to the direct children, I do want to add in a bit that  <a href="https://github.com/SciML/OrdinaryDiffEq.jl/issues/1750">ambiguity resolution can adversely effect compile times</a>. This doesn&#39;t seem to be documented anywhere &#40;this issue was opened by Jameson Nash, one of the core Julia developers, and this is the only case I know of which mentions this fact before this blog post&#41;,  but it makes sense  because finding and resolving ambiguities would require quite a bit of search code  to be ran, and thus simply avoiding any of these searches will lead to an improvement. </p>
<p>Method ambiguities arise when a potential function call is undefined in the sense of multiple dispatch.  The reason is because Julia always picks the &quot;most specific&quot; method available. For example, let&#39;s say we have:</p>
<pre><code class="julia hljs">f(x::<span class=hljs-built_in >Number</span>,y::<span class=hljs-built_in >Number</span>) = <span class=hljs-number >2</span>x + y
f(x::<span class=hljs-built_in >AbstractFloat</span>,y::<span class=hljs-built_in >Number</span>) = x*y</code></pre>
<p>then for <code>f&#40;2.0,3&#41;</code>, the code that will be called is <code>x*y</code> because <code>AbstractFloat &lt;: Number</code> is a more specific type choice. However, if there is no well-ordered form, an ambiguity occurs. This would occur for example if we add the method:</p>
<pre><code class="julia hljs">f(x::<span class=hljs-built_in >Number</span>,y::<span class=hljs-built_in >AbstractFloat</span>) = x/y</code></pre>
<p>Now if we call <code>f&#40;2.0,3.0&#41;</code>, is <code>f&#40;x::Number,y::AbstractFloat&#41;</code> or <code>f&#40;x::AbstractFloat,y::Number&#41;</code> a better fit? The former is a better fit in <code>y</code> while the latter is a better fit in <code>x</code>, so therefore it&#39;s ambiguous which method to choose. Thus if you actually call this in the REPL you will see:</p>
<pre><code class="julia hljs">julia&gt; f(<span class=hljs-number >2.0</span>,<span class=hljs-number >3.0</span>)
ERROR: <span class=hljs-built_in >MethodError</span>: f(::<span class=hljs-built_in >Float64</span>, ::<span class=hljs-built_in >Float64</span>) is ambiguous. Candidates:
  f(x::<span class=hljs-built_in >AbstractFloat</span>, y::<span class=hljs-built_in >Number</span>) <span class=hljs-keyword >in</span> Main at REPL[<span class=hljs-number >35</span>]:<span class=hljs-number >1</span>
  f(x::<span class=hljs-built_in >Number</span>, y::<span class=hljs-built_in >AbstractFloat</span>) <span class=hljs-keyword >in</span> Main at REPL[<span class=hljs-number >36</span>]:<span class=hljs-number >1</span>
Possible fix, define
  f(::<span class=hljs-built_in >AbstractFloat</span>, ::<span class=hljs-built_in >AbstractFloat</span>)
Stacktrace:
 [<span class=hljs-number >1</span>] top-level scope
   @ REPL[<span class=hljs-number >37</span>]:<span class=hljs-number >1</span></code></pre>
<p>Notice that there is nothing wrong with simply having these three method definitions if you know <code>f&#40;::Float64,::Float64&#41;</code> is never called. It&#39;s only an issue when the ambiguous case occurs. This means that ambiguous cases can exist within methods defined in a package and things can work just fine. However, if an uninferred code ever shows up and hits this function, the added code for resolving the ambiguity could in theory increase the compile times. </p>
<p>Thankfully, Julia&#39;s <code>Test</code> module has a method <code>detect_ambiguities</code> which returns all possible ambiguities of a module. When we first applied this to OrdinaryDiffEq, we got 1702 cases:</p>
<pre><code class="julia hljs">julia&gt; <span class=hljs-keyword >using</span> Test; Test.detect_ambiguities(OrdinaryDiffEq)
<span class=hljs-number >1702</span>-element <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Method</span>, <span class=hljs-built_in >Method</span>}}:
 (initialize!(nlsolver::OrdinaryDiffEq.NLSolver{&lt;:NLNewton, <span class=hljs-literal >false</span>}, integrator) @ OrdinaryDiffEq ~/.julia/packages/OrdinaryDiffEq/SmImO/src/nlsolve/newton.jl:<span class=hljs-number >3</span>, initialize!(integrator, cache::OrdinaryDiffEq.LowStorageRK5RPCache) @ OrdinaryDiffEq ~/.julia/packages/OrdinaryDiffEq/SmImO/src/perform_step/low_storage_rk_perform_step.jl:<span class=hljs-number >755</span>)
 (initialize!(nlsolver::OrdinaryDiffEq.NLSolver{&lt;:NLNewton, <span class=hljs-literal >true</span>}, integrator) @ OrdinaryDiffEq ~/.julia/packages/OrdinaryDiffEq/SmImO/src/nlsolve/newton.jl:<span class=hljs-number >13</span>, initialize!(integrator, cache::OrdinaryDiffEq.CG3Cache) @ OrdinaryDiffEq ~/.julia/packages/OrdinaryDiffEq/SmImO/src/perform_step/linear_perform_step.jl:<span class=hljs-number >156</span>)</code></pre>
<p>All of these were due to &quot;bad interface ideas&quot;, using the same function to mean a bunch of different  things. Just bad code in general. Thus we split functions which had &quot;different meanings&quot;, brought this  to zero, and added unit tests on ambiguities   <a href="https://github.com/SciML/OrdinaryDiffEq.jl/pull/1753">in one quick PR</a>. Easy peasy, lemon squeezy.</p>
<h2 id=improving_inference_and_connection_to_function_specialization ><a href="#improving_inference_and_connection_to_function_specialization" class=header-anchor >Improving Inference and Connection to Function Specialization</a></h2>
<p>Now with everything precompiling well and no longer invalidating &#40;as much&#41;, it&#39;s time to address the second question that we posed a few hundred lines earlier:</p>
<blockquote>
<p>How do we improve the compile-times of things which are not precompiled?</p>
</blockquote>
<p>While it is still currently hard to profile this direct question, it turns out that there is one major thing you can do to further improve your code: help make sure inference is specializing correctly. If we go back above to the SnoopCompile statistics:</p>
<pre><code class="julia hljs">InferenceTimingNode: <span class=hljs-number >1.460777</span>/<span class=hljs-number >16.030597</span> on Core.Compiler.Timings.ROOT() with <span class=hljs-number >46</span> direct children</code></pre>
<p>the number of &quot;direct children&quot; are the number of spots where a dynamic dispatch occurs. Recall that most invalidations only occur on code which is not fully inferred and note that&#39;s a sign of trouble. But now also bring in the fact uninferred calls don&#39;t precompile. Thus if you have major calls which are not inferred, this will further decrease the effectiveness of precompilation.</p>
<h4 id=quick_note_about_one_major_difference_from_v17 ><a href="#quick_note_about_one_major_difference_from_v17" class=header-anchor >Quick Note About One Major Difference From v1.7</a></h4>
<p>In the core <a href="https://github.com/SciML/DifferentialEquations.jl/issues/786">compile-time tracking thread</a> it was noted that lack of inference ends up disabling precompilation for downstream calls. Much of the work for improving compile times was thus centered around first improving inference so that RecursiveFactorization.jl could precompile. However,  <a href="https://discourse.julialang.org/t/ann-new-package-snoopprecompile/84778/4?u&#61;chrisrackauckas">this is one of the things that SnoopPrecompile solves</a>. Thus while it is still a good idea to improve inference to reduce invalidations and ensure more precompilation, it&#39;s not as major of an issue as before. tl;dr: before all code downstream of an inference issue would be discarded from precompilation, now with SnoopPrecompile calls downstream of an inference issue that are well-inferred are precompiled &#40;while the &quot;current&quot; method might be discarded for an inference issue&#41;.</p>
<p>Now back to fixing inference issues.</p>
<h3 id=the_most_common_easily_fixable_inference_issues ><a href="#the_most_common_easily_fixable_inference_issues" class=header-anchor >The Most Common Easily Fixable Inference Issues</a></h3>
<p>The <a href="https://docs.julialang.org/en/v1/manual/performance-tips/">standard Julia performance tips</a> lead to more statically inferred code, and thus those should be followed diligently for both good compile and run times. But pay special attention to <a href="https://docs.julialang.org/en/v1/manual/performance-tips/#Be-aware-of-when-Julia-avoids-specializing">Be aware of when Julia avoids specializing</a>. The tl;dr is that if you have a function which takes in a type, like:</p>
<pre><code class="julia hljs">f(T, x) = T(x)
f(<span class=hljs-built_in >Float32</span>, <span class=hljs-number >1.0</span>)</code></pre>
<p>then this function will specialize on <code>x</code> but not on <code>T</code> by default. Thus if you want inference to specialize on this function &#40;and thus infer the output type as T&#33;&#41;, you need to change DataType dispatches to the form:</p>
<pre><code class="julia hljs">f(::<span class=hljs-built_in >Type</span>{T}, x) <span class=hljs-keyword >where</span> T = T(x)</code></pre>
<p>A similar case arises with functions. If you have:</p>
<pre><code class="julia hljs">f(g::<span class=hljs-built_in >Function</span>, x) = g(x)</code></pre>
<p>By default Julia will attempt to reduce the amount of compilation by not specializing on the function <code>g</code>. However, if you are looking to improve the amount of precompilation that occurs, then you want this function to be specialized and compiled on the function <code>g</code>, and therefore:</p>
<pre><code class="julia hljs">f(g::G, x) <span class=hljs-keyword >where</span> G = g(x)</code></pre>
<p>will improve specialization, inference, and thus lead to more compilation. We will dig into this specific case in a little bit more detail, so hold onto your thoughts here&#33;</p>
<h3 id=okay_but_how_do_i_identify_what_methods_might_need_such_a_treatment ><a href="#okay_but_how_do_i_identify_what_methods_might_need_such_a_treatment" class=header-anchor >Okay, But How Do I Identify What Methods Might Need Such a Treatment?</a></h3>
<p>Good question. If you go back to the SnoopCompile call:</p>
<pre><code class="julia hljs"><span class=hljs-meta >@show</span> tinf
InferenceTimingNode: <span class=hljs-number >1.460777</span>/<span class=hljs-number >16.030597</span> on Core.Compiler.Timings.ROOT() with <span class=hljs-number >46</span> direct children</code></pre>
<p>You see that this counts the number of uninferred calls as &quot;direct children&quot;. You can query this via <code>inference_triggers</code> to figure out where those inference triggers occur. For example, in a <a href="https://github.com/SciML/DiffEqBase.jl/pull/698#issuecomment-896984234">much earlier version</a> we saw:</p>
<pre><code class="julia hljs">itrigs = inference_triggers(tinf)

itrigs[<span class=hljs-number >5</span>]

Inference triggered to call (::FiniteDiff.<span class=hljs-string >var&quot;#finite_difference_jacobian!##kw&quot;</span>)(::<span class=hljs-built_in >NamedTuple</span>{(:dir,), <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Bool</span>}}, ::typeof(FiniteDiff.finite_difference_jacobian!), ::<span class=hljs-built_in >Matrix</span>{<span class=hljs-built_in >Float64</span>}, ::<span class=hljs-built_in >Function</span>, ::<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, ::FiniteDiff.JacobianCache{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >UnitRange</span>{<span class=hljs-built_in >Int64</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Val</span>{:forward}(), <span class=hljs-built_in >Float64</span>}, ::<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}) from jacobian_finitediff_forward! (C:\Users\accou\.julia\dev\OrdinaryDiffEq\src\derivative_wrappers.jl:<span class=hljs-number >89</span>) with specialization OrdinaryDiffEq. jacobian_finitediff_forward!(::<span class=hljs-built_in >Matrix</span>{<span class=hljs-built_in >Float64</span>}, ::<span class=hljs-built_in >Function</span>, ::<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, ::FiniteDiff.JacobianCache{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >UnitRange</span>{<span class=hljs-built_in >Int64</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Val</span>{:forward}(), <span class=hljs-built_in >Float64</span>}, ::<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, ::OrdinaryDiffEq.ODEIntegrator{Rodas5{<span class=hljs-number >0</span>, <span class=hljs-literal >false</span>, DefaultLinSolve, <span class=hljs-built_in >Val</span>{:forward}}, <span class=hljs-literal >true</span>, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Float64</span>, SciMLBase.NullParameters, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}}, ODESolution{<span class=hljs-built_in >Float64</span>, <span class=hljs-number >2</span>, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}}}, ODEProblem{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>}, <span class=hljs-literal >true</span>, SciMLBase.NullParameters, ODEFunction{<span class=hljs-literal >true</span>, typeof(lorenz), LinearAlgebra.UniformScaling{<span class=hljs-built_in >Bool</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, typeof(SciMLBase.DEFAULT_OBSERVED), <span class=hljs-built_in >Nothing</span>}, Base.Pairs{<span class=hljs-built_in >Symbol</span>, <span class=hljs-built_in >Union</span>{}, <span class=hljs-built_in >Tuple</span>{}, <span class=hljs-built_in >NamedTuple</span>{(), <span class=hljs-built_in >Tuple</span>{}}}, SciMLBase.StandardODEProblem}, Rodas5{<span class=hljs-number >0</span>, <span class=hljs-literal >false</span>, DefaultLinSolve, <span class=hljs-built_in >Val</span>{:forward}}, OrdinaryDiffEq.InterpolationData{ODEFunction{<span class=hljs-literal >true</span>, typeof(lorenz), LinearAlgebra.UniformScaling{<span class=hljs-built_in >Bool</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, typeof(SciMLBase.DEFAULT_OBSERVED), <span class=hljs-built_in >Nothing</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}}}, OrdinaryDiffEq.Rosenbrock5Cache{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Matrix</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Matrix</span>{<span class=hljs-built_in >Float64</span>}, OrdinaryDiffEq.Rodas5Tableau{<span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>}, SciMLBase.TimeGradientWrapper{ODEFunction{<span class=hljs-literal >true</span>, typeof(lorenz), LinearAlgebra.UniformScaling{<span class=hljs-built_in >Bool</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, typeof(SciMLBase.DEFAULT_OBSERVED), <span class=hljs-built_in >Nothing</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, SciMLBase.NullParameters}, SciMLBase.UJacobianWrapper{ODEFunction{<span class=hljs-literal >true</span>, typeof(lorenz), LinearAlgebra.UniformScaling{<span class=hljs-built_in >Bool</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, typeof(SciMLBase.DEFAULT_OBSERVED), <span class=hljs-built_in >Nothing</span>}, <span class=hljs-built_in >Float64</span>, SciMLBase.NullParameters}, DefaultLinSolve, FiniteDiff.JacobianCache{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >UnitRange</span>{<span class=hljs-built_in >Int64</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Val</span>{:forward}(), <span class=hljs-built_in >Float64</span>}, FiniteDiff.GradientCache{<span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Val</span>{:forward}(), <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Val</span>{<span class=hljs-literal >true</span>}()}}}, DiffEqBase.DEStats}, ODEFunction{<span class=hljs-literal >true</span>, 
typeof(lorenz), LinearAlgebra.UniformScaling{<span class=hljs-built_in >Bool</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, typeof(SciMLBase.DEFAULT_OBSERVED), <span class=hljs-built_in >Nothing</span>}, OrdinaryDiffEq.Rosenbrock5Cache{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Matrix</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Matrix</span>{<span class=hljs-built_in >Float64</span>}, OrdinaryDiffEq.Rodas5Tableau{<span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>}, SciMLBase.TimeGradientWrapper{ODEFunction{<span class=hljs-literal >true</span>, typeof(lorenz), LinearAlgebra.UniformScaling{<span class=hljs-built_in >Bool</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, typeof(SciMLBase.DEFAULT_OBSERVED), <span class=hljs-built_in >Nothing</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, SciMLBase.NullParameters}, SciMLBase.UJacobianWrapper{ODEFunction{<span class=hljs-literal >true</span>, typeof(lorenz), LinearAlgebra.UniformScaling{<span class=hljs-built_in >Bool</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, typeof(SciMLBase.DEFAULT_OBSERVED), <span class=hljs-built_in >Nothing</span>}, <span class=hljs-built_in >Float64</span>, SciMLBase.NullParameters}, DefaultLinSolve, FiniteDiff.JacobianCache{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >UnitRange</span>{<span class=hljs-built_in >Int64</span>}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Val</span>{:forward}(), <span class=hljs-built_in >Float64</span>}, FiniteDiff.GradientCache{<span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Val</span>{:forward}(), 
<span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Val</span>{<span class=hljs-literal >true</span>}()}}, OrdinaryDiffEq.DEOptions{<span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Float64</span>, PIController{<span class=hljs-built_in >Rational</span>{<span class=hljs-built_in >Int64</span>}}, typeof(DiffEqBase.ODE_DEFAULT_NORM), typeof(LinearAlgebra.opnorm), <span class=hljs-built_in >Nothing</span>, CallbackSet{<span class=hljs-built_in >Tuple</span>{}, <span class=hljs-built_in >Tuple</span>{}}, typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), DataStructures.BinaryHeap{<span class=hljs-built_in >Float64</span>, DataStructures.FasterForward}, DataStructures.BinaryHeap{<span class=hljs-built_in >Float64</span>, DataStructures.FasterForward}, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Nothing</span>, <span class=hljs-built_in >Int64</span>, <span class=hljs-built_in >Tuple</span>{}, <span class=hljs-built_in >Tuple</span>{}, <span class=hljs-built_in >Tuple</span>{}}, <span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>}, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >Nothing</span>, OrdinaryDiffEq.DefaultInit})</code></pre>
<p>&#40;Note that this can be annoyingly long, but <code>itrigs</code> is just a <code>Vector</code>, so you can index it like <code>itrigs&#91;5&#93;</code> to only show the 5th inference trigger. Also, you can use  <a href="https://github.com/JuliaDebug/Cthulhu.jl">Cthulhu</a>  with <code>ascend&#40;itrigs&#91;5&#93;&#41;</code> to further debug the inference issue in detail, if you know Cthulhu&#41;</p>
<p>With some practice you can quickly read this and see:</p>
<pre><code class="julia hljs">OrdinaryDiffEq.jacobian_finitediff_forward!(::<span class=hljs-built_in >Matrix</span>{<span class=hljs-built_in >Float64</span>}, ::<span class=hljs-built_in >Function</span>, ...</code></pre>
<p>at <code>jacobian_finitediff_forward&#33; &#40;C:\Users\accou\.julia\dev\OrdinaryDiffEq\src\derivative_wrappers.jl:89&#41;</code>, oh wait a minute that code was missing a <code>::F&#41; where F</code> specialization on the function which is the second argument.</p>
<p><strong>Now if you handle those and did everything else before, you&#39;re in precompilation heaven. Congratualations, your inference time should be close to zero and you should only be left with LLVM time</strong></p>
<p>But wait a minute, we&#39;re still missing one last piece:</p>
<h2 id=handling_higher_order_functions_controlling_specialization ><a href="#handling_higher_order_functions_controlling_specialization" class=header-anchor >Handling Higher Order Functions: Controlling Specialization</a></h2>
<p>For most packages, you&#39;re done. This last piece is rather specific to codes like those in SciML which have higher order functions. Let&#39;s revisit one piece from the improving inference section. I mentioned that if you pass a function to another function, then Julia will not specialize by default.</p>
<pre><code class="julia hljs">f(g::<span class=hljs-built_in >Function</span>, x) = g(x)</code></pre>
<blockquote>
<p>By default Julia will attempt to reduce the amount of compilation by not specializing on the function <code>g</code>. However, if you are looking to improve the amount of precompilation that occurs, then you want this function to be specialized and compiled on the function <code>g</code>, and therefore:</p>
</blockquote>
<pre><code class="julia hljs">f(g::G, x) <span class=hljs-keyword >where</span> G = g(x)</code></pre>
<p>The reason for this behavior is because every function in Julia is a new type. </p>
<pre><code class="julia hljs">julia&gt; typeof(f)
typeof(f) (singleton type of <span class=hljs-keyword >function</span> f, subtype of <span class=hljs-built_in >Function</span>)

julia&gt; h = (x) -&gt; <span class=hljs-number >2</span>x
<span class=hljs-comment >#5 (generic function with 1 method)</span>

julia&gt; typeof(h)
<span class=hljs-string >var&quot;#5#6&quot;</span></code></pre>
<p>Here <code>#5</code> is just a counter &#40;using <code>gensym</code>&#41; saying this is the type for the 5th anonymous function created in my REPL. Every single one is a different type, and each of those types are subtypes of <code>::Function</code>. Thus <code>::Function</code> is a supertype, like <code>Number</code>, and is not the concrete type of functions.</p>
<p>This has some very important consequences. An astute reader may have already noticed the issue: before I mentioned how precompilation happens on the method signatures of the function, and the method signatures are defined by the input types. But if the input type is a function, then every unique function has a unique method, and thus while forcing specialization <code>f&#40;g::G, x&#41; where G &#61; g&#40;x&#41;</code> may allow for precompilation, the precompiled method will be <strong>only that specific <code>g</code>, not all possible functions</strong>. For example, that would be like precompiling the ODE solver only for the <code>f</code> you happened to put into the SnoopPrecompile.jl statement. Maybe that works for some use cases where the model does not change, but for an ODE solver, if you have to recompile every single method that touches <code>f</code> for every new ODE someone wants to solve, then you are still throwing away the vast majority of precompilation work.</p>
<p>The simplest answers are: you either fully specialize on higher order functions or you don&#39;t. An easy way to force this through a whole codebase is to simply wrap the function in a struct. The &quot;don&#39;t specialize it anywhere&quot; case looks like:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >struct</span> FWrap
  f
<span class=hljs-keyword >end</span>
(F::FWrap)(x...) = F.f(x...)

ff = FWrap(f)
<span class=hljs-comment >## ff now acts just like `f`, but its type is constant `FWrap`</span></code></pre>
<p>This minimizes the surface of which function specialization rules are applied, and can be an easy way to enforce no specialization. Since the types are always the same, the functions which only see <code>FWrap</code> will see constant types and precompile just fine. However, a major downside is that since <code>F.f</code> is not inferred, the output of <code>F.f</code> is not inferred, and thus <code>ff&#40;x&#41;</code> can easily be type-unstable. One way to make this easier to handle is to simply require an API of mutation, i.e. <code>ff&#33;&#40;out,x&#41;</code> which returns <code>nothing</code> can have <code>&#40;F::FWrap&#41;&#40;x...&#41; &#61; &#40;F.f&#40;x...&#41;; return nothing&#41;</code> enforce that the return is always <code>nothing</code> and thus world-splitting optimizations will ensure that calls to <code>ff</code> do not break inference. This is one of the easier ways to balance the trade-off of inference and specialization for higher order functions. </p>
<p>On the other hand, one can make a type that fully specializes:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >struct</span> FWrap{F}
  f::F
<span class=hljs-keyword >end</span>
(F::FWrap)(x...) = F.f(x...)

ff = FWrap(f)
<span class=hljs-comment >## ff now acts just like `f`, but its type is non-constant `FWrap{typeof(f)}`</span></code></pre>
<p>This will now fully specialize everywhere &#40;note: do not put <code>struct FWrap&#123;F&#125; &lt;: Function</code> since then the specialization rules for <code>Function</code>s will apply to <code>FWrap</code> as well&#33;&#41;. But is there a middle ground?</p>
<p>The middle ground would be to specialize on the input/output types: treating functions like function pointers in C. This can be done via a package known as  <a href="https://github.com/yuyichao/FunctionWrappers.jl">FunctionWrappers.jl</a>. For example, if we have the function <code>f&#40;x,y&#41; &#61; round&#40;x*y&#41;</code>, we can do <code>ff &#61; FunctionWrapper&#123;Int, Tuple&#123;Float64, Float32&#125;&#125;&#40;f&#41;</code> and this will make a <code>FunctionWrapper&#123;Int, Tuple&#123;Float64, Float32&#125;&#125;</code>. This is a type of function which only allows two inputs, &#40;x::Float64, y::Float32&#41;, and returns a single output <code>Int</code>. In other words, <code>ff</code> can be thought of as a function with only a single dispatch <code>ff&#40;x::Float64, y::Float32&#41;::Int</code>. It does not matter that <code>f</code> was compatible with more dispatches: once the function is wrapped its wrapped form can only call that specific type signature. All functions which are wrapped under the same signature share the same type, so <code>g&#40;x,y&#41; &#61; ceil&#40;x*y&#41;; gg &#61; FunctionWrapper&#123;Int, Tuple&#123;Float64, Float32&#125;&#125;&#40;g&#41;</code> has  that <code>typeof&#40;ff&#41; &#61;&#61;&#61; typeof&#40;gg&#41;</code>, even though ff&#40;x,y&#41; is not necessarily the same as <code>gg&#40;x,y&#41;</code>. Thus if one can ensure that all dispatches have the same constraint on the input/output types, a <code>FunctionWrapper</code> can be used to force specialization on the input/output types in a way that is not specific to a given function.</p>
<p>Note that FunctionWrappers.jl only supports single method dispatches, so therefore a wrapper package <a href="https://github.com/chriselrod/FunctionWrappersWrappers.jl">FunctionWrappersWrappers.jl</a> exists to allow for defining a <code>FunctionWrappersWrappers</code> which is a list of FunctionWrappers wrapped into a single function that performs a limited subset dispatch on the input arguments &#40;with inferred outputs&#41;. This looks like: <code>FunctionWrappersWrapper&#40;&#43;, &#40;Tuple&#123;Float64,Float64&#125;, Tuple&#123;Int,Int&#125;&#41;, &#40;Float64,Int&#41;&#41;</code>, i.e. you give a tuple of input argument tuples and output arguements. But the same applies in that this will now build functions with <code>n</code> pre-defined many dispatches in a way that specializes and thus allows for precompilation.</p>
<h3 id=automating_the_function_wrapping_process ><a href="#automating_the_function_wrapping_process" class=header-anchor >Automating the Function Wrapping Process</a></h3>
<p>While the user of the package could themselves wrap the function and thus achieve total precompilation with function specialization, we found that in our packages we could design the package so that the user did not have to muck with any <code>FunctionWrappersWrappers</code> nonsense but still achieve the full precompilation. To see how this was done, let&#39;s take a closer look at our stiff ODE solve example:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq
<span class=hljs-keyword >function</span> lorenz(du,u,p,t)
    du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>]-u[<span class=hljs-number >1</span>])
    du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>]*(<span class=hljs-number >28.0</span>-u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
    du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>]*u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span>/<span class=hljs-number >3</span>)*u[<span class=hljs-number >3</span>]
<span class=hljs-keyword >end</span>
u0 = [<span class=hljs-number >1.0</span>;<span class=hljs-number >0.0</span>;<span class=hljs-number >0.0</span>]; tspan = (<span class=hljs-number >0.0</span>,<span class=hljs-number >100.0</span>)
prob = ODEProblem(lorenz,u0,tspan)
solve(prob,Rodas5())</code></pre>
<p>In this case, the user provides us a model in the form of a function <code>lorenz</code>. This function is a mutating function, and from the ODE definition we have that <code>u0 isa Vector&#123;Float64&#125;</code>, <code>eltype&#40;tspan&#41; isa Float64</code>, and there are no parameters &#40;and thus <code>typeof&#40;p&#41; isa SciMLBase.NullParameters</code>&#41;. From these facts we know that internal to the ODE solver when automatic differentiation is not being used, the type of <code>u</code> and the type of <code>du</code> match the <code>typeof&#40;u0&#41;</code> &#40;we can also deduce the types required for automatic  differentiation, but that&#39;s a longer story which I will leave for the appendix&#41;. Therefore it is at the point of the <code>ODEProblem</code> construction that we have all of the information to do the function wrapping.</p>
<p>We can thus have the <code>ODEProblem</code> call itself specialize on the input function <code>lorenz</code>, but immediately compute the wrapper as follows:</p>
<pre><code class="julia hljs"><span class=hljs-comment ># Since the function mutates, make a wrapper that always throws away</span>
<span class=hljs-comment ># the return and gives nothing, just in case the user accidentally forgets!</span>
<span class=hljs-keyword >struct</span> Void{F}
  f::F
<span class=hljs-keyword >end</span>

<span class=hljs-keyword >function</span> (f::Void{F})(x) <span class=hljs-keyword >where</span> F
  f.f(x)
  <span class=hljs-literal >nothing</span>
<span class=hljs-keyword >end</span>

FunctionWrapper{<span class=hljs-built_in >Nothing</span>,<span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>},<span class=hljs-built_in >Vector</span>{<span class=hljs-built_in >Float64</span>},SciMLBase.NullParameters,<span class=hljs-built_in >Float64</span>}}(Void(lorenz))</code></pre>
<p>Thus the very first method of the call stack will have to recompile for every new ODE, but that&#39;s a trivial &lt;100 microseconds call short call. All of the real functionality is then behind the next call, like:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> ODEProblem(f,u0,tspan,p;kwargs...)
  _ODEProblem(wrapfunction(f),u0,tspan,p;kwargs...)
<span class=hljs-keyword >end</span></code></pre>
<p>so then all of the &quot;real work&quot; is precompiled. If this is done, then if <code>f</code> changes the <code>typeof&#40;prob&#41;</code> stays constant, and thus <code>solve</code> can be fully precompiled. In SciML we called this  <a href="https://scimlbase.sciml.ai/dev/interfaces/Problems/#Specialization-Levels"><code>SciMLBase.FunctionWrapperSpecialize</code></a>. However, the difficulty with this form is that we have to commit to the <code>FunctionWrapper</code> very early. If <code>ODEProblem</code> is then attempted to be solved with some new solver that uses some new automatic differentiation technique, it may break when it sees the <code>FunctionWrapper</code>, so you may need to manually unwrap &#40;<code>SciMLBase.unwrapped_f&#40;prob.f&#41;</code>&#41; and it becomes a nightmare to maintain. Thus the real question is, how much specialization do you really need to avoid?</p>
<p>Since the vast majority &#40;<code>&gt;99&#37;</code>&#41; of the compile time lives in the <code>solve&#40;prob,Rodas5&#40;&#41;&#41;</code> call, avoiding respecializing the rest of the <code>ODEProblem</code> call was simply over-engineering. Thus we found that a similar strategy could hold in the <code>solve</code> call itself. This looks like:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> solve(prob,alg)
  <span class=hljs-comment ># Pseudocode</span>
  <span class=hljs-keyword >if</span> alg is okay with having the <span class=hljs-keyword >function</span> wrapped
    _prob = wrapped_f_prob(prob)
  <span class=hljs-keyword >else</span>
    _prob = prob
  <span class=hljs-keyword >end</span>

  __solve(_prob,alg)
<span class=hljs-keyword >end</span></code></pre>
<p>In other words, we can wait to apply the function wrapping until we really know that we want it, allow for doing things like promoting <code>t</code> from <code>Int</code> etc., and thus have something very robust without ever forcing any other solver to be compatible with the <code>FunctionWrappersWrappers</code> types. This is what we implemented as the <code>AutoSpecialize</code> mode. And from  <a href="https://github.com/SciML/SciMLBase.jl/pull/242">a quick benchmark</a> we see that there&#39;s almost no  difference:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq, SnoopCompile, Profile, ProfileView
<span class=hljs-keyword >function</span> lorenz(du, u, p, t)
    du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>] - u[<span class=hljs-number >1</span>])
    du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>] * (<span class=hljs-number >28.0</span> - u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
    du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>] * u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span> / <span class=hljs-number >3</span>) * u[<span class=hljs-number >3</span>]
<span class=hljs-keyword >end</span>

<span class=hljs-meta >@time</span> <span class=hljs-keyword >begin</span>
    lorenzprob = ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.AutoSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), <span class=hljs-built_in >Float64</span>[])
    sol = solve(lorenzprob, Rosenbrock23())
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># FunctionWrapperSpecialize:</span>
<span class=hljs-comment ># 1.475326 seconds (83.83 k allocations: 3.442 MiB, 99.79% compilation time)</span>
<span class=hljs-comment ># 0.000184 seconds (458 allocations: 40.070 KiB)</span>
<span class=hljs-comment ># AutoSpecialize:</span>
<span class=hljs-comment ># 1.597643 seconds (958.02 k allocations: 49.979 MiB, 99.85% compilation time)</span>
<span class=hljs-comment ># 0.000182 seconds (467 allocations: 40.203 KiB</span></code></pre>
<p>&#40;that is stochastic from one run of each called twice. The difference is usually closer to 0.05  seconds, and the runtime is &quot;exactly&quot; the same&#41;.</p>
<p>Thus SciML defaults now to a strategy of delayed wrapping &#40;<code>AutoSpecialize</code>&#41; to make maintanance easy but avoid respecializing the solver unneccessarily. By default, the ODE solvers then precompile for <code>AutoSpecialize</code> with the standard <code>Float64</code> and <code>Vector&#123;Float64&#125;</code> types, so the entire solver precompiles. This gets the first solve time down to ~1.5 seconds, sans <code>using</code> time.</p>
<p><strong>Remember, we started this at 22 seconds, and now this is down to ~1.5 seconds&#33;</strong> &#40;both sans using time&#41;</p>
<h3 id=how_much_runtime_overhead_do_functionwrappers_add ><a href="#how_much_runtime_overhead_do_functionwrappers_add" class=header-anchor >How much runtime overhead do FunctionWrappers add?</a></h3>
<p>Almost none&#33; <code>lorenz</code> is a pretty cheap function call, so it&#39;s a good baseline of &quot;something that would have more overhead that larger cases&quot;. In  <a href="https://github.com/SciML/DiffEqBase.jl/pull/736#issuecomment-1221502113">the development PR there were some benchmarks</a></p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq
<span class=hljs-keyword >function</span> f(du, u, p, t)
    du[<span class=hljs-number >1</span>] = <span class=hljs-number >0.2</span>u[<span class=hljs-number >1</span>]
    du[<span class=hljs-number >2</span>] = <span class=hljs-number >0.4</span>u[<span class=hljs-number >2</span>]
<span class=hljs-keyword >end</span>
u0 = ones(<span class=hljs-number >2</span>)
tspan = (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>)
prob = ODEProblem{<span class=hljs-literal >true</span>,<span class=hljs-literal >false</span>}(f, u0, tspan, <span class=hljs-built_in >Float64</span>[])

<span class=hljs-keyword >function</span> lorenz(du, u, p, t)
    du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>] - u[<span class=hljs-number >1</span>])
    du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>] * (<span class=hljs-number >28.0</span> - u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
    du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>] * u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span> / <span class=hljs-number >3</span>) * u[<span class=hljs-number >3</span>]
<span class=hljs-keyword >end</span>
lorenzprob = ODEProblem{<span class=hljs-literal >true</span>,<span class=hljs-literal >false</span>}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), <span class=hljs-built_in >Float64</span>[])
typeof(prob) === typeof(lorenzprob) <span class=hljs-comment ># true</span>

<span class=hljs-meta >@time</span> sol = solve(lorenzprob, Rosenbrock23())
<span class=hljs-comment ># 0.847580 seconds (83.25 k allocations: 3.404 MiB, 99.75% compilation time)</span>

<span class=hljs-meta >@time</span> sol = solve(lorenzprob, Rosenbrock23(autodiff=<span class=hljs-literal >false</span>))
<span class=hljs-comment ># 0.701598 seconds (499.23 k allocations: 28.846 MiB, 99.73% compilation time)</span>

<span class=hljs-meta >@time</span> sol = solve(lorenzprob, Rosenbrock23())
<span class=hljs-comment ># 0.000113 seconds (457 allocations: 39.828 KiB)</span>

<span class=hljs-meta >@time</span> sol = solve(lorenzprob, Rosenbrock23(autodiff=<span class=hljs-literal >false</span>))
<span class=hljs-comment ># 0.000147 seconds (950 allocations: 45.547 KiB)</span>

lorenzprob2 = ODEProblem(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), <span class=hljs-built_in >Float64</span>[])

<span class=hljs-meta >@time</span> sol = solve(lorenzprob2, Rosenbrock23())
<span class=hljs-comment ># 8.587653 seconds (24.77 M allocations: 3.581 GiB, 5.37% gc time, 99.99% compilation time)</span>

<span class=hljs-meta >@time</span> sol = solve(lorenzprob2, Rosenbrock23(autodiff=<span class=hljs-literal >false</span>))
<span class=hljs-comment ># 1.122847 seconds (3.69 M allocations: 211.491 MiB, 2.45% gc time, 99.98% compilation time)</span>

<span class=hljs-meta >@time</span> sol = solve(lorenzprob2, Rosenbrock23())
<span class=hljs-comment ># 0.000120 seconds (455 allocations: 39.531 KiB)</span>

<span class=hljs-meta >@time</span> sol = solve(lorenzprob2, Rosenbrock23(autodiff=<span class=hljs-literal >false</span>))
<span class=hljs-comment ># 0.000138 seconds (950 allocations: 45.188 KiB)</span></code></pre>
<p><code>lorenzprob2</code> is the full specialization form, and <code>lorenzprob</code> is the function wrapped form. We could not descern a meaningful difference.</p>
<h4 id=small_detail_on_wrapper_performance_with_forwarddiff ><a href="#small_detail_on_wrapper_performance_with_forwarddiff" class=header-anchor >Small Detail on Wrapper Performance with ForwardDiff</a></h4>
<p>Though note that needs a caveat on it. When forward-mode automatic differentiation via ForwardDiff.jl is used, the chunk size is a part of the type. Having a larger chunk size can improve the performance of the method, but the allowed values are dependent on the number of ODEs. Thus if one was only going to pick a single chunk size, the only valid answer is <code>1</code>, which can be less performant than some other cases. We could in theory setup the wrapper for all chunk sizes, though this increases the number of dispatches in the <code>FunctionWrappersWrappers</code> by an order of magnitude, and thus the precompilation time as well. Therefore, the function wrapper that is built sets the allowed chunk sizes to only be <code>1</code>, and takes a bit of a performance &#40;usually no greater than 2x&#41; to cut down on the total precompilation time. This trade-off can then be managed by the user specifying they want <code>SciMLBase.FullSpecialize</code> form instead &#40;which we recommend in any case where top-notch runtime is necessary&#41;.</p>
<p>So conclusion, performance of function wrappers are fine, though there can be edge cases.</p>
<h2 id=whats_left_using_time_llvm_time_and_system_images ><a href="#whats_left_using_time_llvm_time_and_system_images" class=header-anchor >What&#39;s Left? Using Time, LLVM Time, and System Images</a></h2>
<p>Now what we&#39;re left with is 1.5 seconds which is almost all LLVM compile time, since all of the inference time was removed by precompilation, and we still have the <code>using OrdinaryDiffEq</code> time. <code>using OrdinaryDiffEq</code> still takes quite a good chunk of time &#40;I think about 5 seconds on the desktop  that was measuring everything? It&#39;s hard to measure since my laptop is much slower and it&#39;s at about 8 seconds, so to keep the timings consistent, all others used the desktop and that&#39;s my best guess right now&#41;. That <code>using OrdinaryDiffEq</code> will come down considerably since Julia&#39;s Base v1.8.2 has some major invalidation fixes, and <a href="https://github.com/SciML/Static.jl/pull/78">two major</a> invalidation <a href="https://github.com/JuliaDiff/ChainRulesCore.jl/issues/576">sources remain</a> unaddressed. I think the same desktop will get to around 2-3 seconds <code>using</code> time after that. FillArrays.jl needs to split out a core, NonlinearSolve needs to not recompile RecursiveFactorization.jl, etc. but these are all things you know how to do now. So the baseline &quot;user does nothing else to their installation&quot; should soon be at about 3.5 seconds total &#40;according to what the profiles show is easy to drop&#41;.</p>
<p>Thus how do we get down to 0.1 seconds? This last part requires forcing caching the resolution of invalidations and caching the LLVM/native bytecode. While  <a href="https://www.youtube.com/watch?v&#61;GnsONc9DYg0">Tim Holy and Valentin Churavy have plans for how to automate this in upcoming Julia releases</a>, currently there is no way to make precompilation stash these pieces.</p>
<p>However, this is where system images come into play. If Julia&#39;s Base code never stashed any native bytecode, Julia would have a terribly first time to run anything. But it does have a way to do this, and that&#39;s called the system image. The system image is a bundle of the LLVM compiler, Julia&#39;s runtime, and a precompiled binary with &quot;all of the code that existed at system image build time&quot;. By default, Julia&#39;s system image build includes Julia itself and the standard library. However, there is a tool in the package ecosystem, <a href="https://github.com/JuliaLang/PackageCompiler.jl">PackageCompiler.jl</a>, which allows for adding more code to the system image. </p>
<p>PackageCompiler usage normally states that you need to give a representative set of functions etc., but it turns out that, since our precompile files are complete we don&#39;t need to do anything. All we need to do is tell PackageCompiler to compile a list of packages which includes our package. For example:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> PackageCompiler
create_sysimage([<span class=hljs-string >&quot;OrdinaryDiffEq&quot;</span>], sysimage_path=<span class=hljs-string >&quot;DiffEqSysImage.so&quot;</span>)</code></pre>
<p>and then we run it, and tada that&#39;s the 0.1 seconds GIF at the top.</p>
<p><img src="https://user-images.githubusercontent.com/1814174/185794444-34a99f53-646a-4cbb-81a3-678bb2e13a17.gif" alt="" /></p>
<p>Moral of the story, this last step is still being worked on in order to be further automated. But, it&#39;s now only one line of code to get full compilation, so please do it. Like seriously, I&#39;m now using some nice custom system images all of the time. One you have all of your precompilation well-snooped, it&#39;s a beauty. Start using them today.</p>
<h4 id=note_about_system_images ><a href="#note_about_system_images" class=header-anchor >Note About System Images</a></h4>
<p>System images are not the be-all end-all solution here. The function specialization changes, inference improvements, etc. are all needed in order to get this final result. A few months ago the time post system image was still &gt;10 seconds. So while system images do some heavy lifting, all of these &quot;good compile time practices&quot; were required to really get that final system image actually removing all compilation.</p>
<h4 id=note_about_vs_code ><a href="#note_about_vs_code" class=header-anchor >Note About VS Code</a></h4>
<p>VS Code has  <a href="https://www.julia-vscode.org/docs/stable/userguide/compilesysimage/">tools to make building system images easier</a>. Use them.</p>
<h2 id=conclusions_and_lasting_thoughts ><a href="#conclusions_and_lasting_thoughts" class=header-anchor >Conclusions and Lasting Thoughts</a></h2>
<p>Nothing is complete, but huge strides have been made. Major thanks to Tim Holy who put together the tools required to make these changes as part of the CZI work. Also major kudos to Chris Elrod, Jeff Bezanson, and Jameson Nash at Julia Computing who helped complete the story with ambiguity handling and function specialization pieces as part of the &#40;yet to be made public&#41; grant work. All of this was a culmination of package developers working with the compiler developers to get the tools that are needed to solve the real problems.</p>
<p>SciML&#39;s packages can thus serve as a source of inspiration for the Julia community. Here&#39;s a set of packages which had some of the largest compile times just a year ago, and now the REPL feels instantaneous. These steps are reproducible to other packages and just need someone to roll up their sleeves. We have the tools, let&#39;s go for it&#33;</p>
<p>A tl;dr of our current position is as follows:</p>
<ul>
<li><p>Every package should setup a SnoopPrecompile.jl block. While the precompilation ownership changes allow for downstream usage to precompile without such a block, covering the standard cases will allow the ecosystem to reduce the total amount of compilation caches and thus improve precompile times and reduce using times.</p>

<li><p>Julia could use a better conditional module loading system in future versions. That would help lower package loading times. For now, sectioning off &quot;core&quot; portions of a package and making diligent use of subpackaging can be helpful. Avoid Requires.jl when possible.</p>

<li><p>Delay large loads until as late as possible.</p>

<li><p>Help the Julia ecosystem by profiling and identifying major invalidation sources. Most are trivial to fix and give a nice performance boost. Not all invalidations are worth fixing though&#33;</p>

<li><p>Use <code>Test.detect_ambiguities</code> to identify ambiguities. Add it to your CI tests&#33;</p>

<li><p>Better interfaces in Julia Base would reduce the dependency tree requirements downstream as more AbstractArray and Number packages would have consistently compile-time queriable information. Upstreaming of portions of ArrayInterface to Base and changing the  <a href="https://docs.julialang.org/en/v1/manual/interfaces/">Base AbstractArray Interface</a> would thus facilitate better code with more explicit checking of assumptions and reduce package load times. </p>

<li><p>Use <code>inference_triggers</code> to find all inference issues, and fix them.</p>

<li><p>Understand the function specialization behaviors if your package deals with higher order functions.</p>

<li><p>Once precompilation is all setup, PackageCompiler system image building is just one line of code. Thus if packages all setup their compilation practices appropriately, PackageCompiler is a piece of cake and everyone should use it&#33;</p>

</ul>
<p>As for improvements coming soon:</p>
<ul>
<li><p>There are more improvements coming soon to Base, such as LLVM code caching, which will further  reduce the need for PackageCompiler and system images. Until then, you can at least eliminate  everything that is inference time via precompilation, reduce using times, and then use system images.</p>

<li><p>Most of the remaining issues in SciML are due to the JuliaSIMD stack, specifically LoopVectorization.jl and its usage in RecursiveFactorization.jl. It&#39;s just a big generated code and therefore its LLVM time is long. New packages are being developed in the JuliaSIMD stack which alleviate this and further bring down the &quot;no system image first solve time&quot;.</p>

<li><p>Precompilation of uninferred calls and reduction of world-splitting optimizations have been identified as two improvements to Julia&#39;s Base that could further help compile times. We&#39;ve upstreamed these needs and should hopefully hear some good news in the future.</p>

</ul>
<p>And that&#39;s all for now. It&#39;s still on-going work, but there&#39;s no reason to not get started yourself.</p>
<h1 id=appendix ><a href="#appendix" class=header-anchor >Appendix</a></h1>
<h2 id=using_preferences_to_control_local_precompilation_choices ><a href="#using_preferences_to_control_local_precompilation_choices" class=header-anchor >Using Preferences to Control Local Precompilation Choices</a></h2>
<p>While the ability for a <code>SnoopPrecompile.@precompile_all_calls</code> block to precompile all well-inferred calls is a good thing, in some cases users may want to control the amount that is precompiled. Moreso than users, this is helpful to developers who may need to frequently recompile the package. To make the precompilation choices more flexible, <a href="https://github.com/JuliaPackaging/Preferences.jl">Preferences.jl</a> can be used to set compile-time preference controls on what to precompile. For example, with OrdinaryDiffEq.jl there are controls on whether to precompile the non-stiff, stiff, and auto-switching ODE solvers. This is done for example like:</p>
<pre><code class="julia hljs">SnoopPrecompile.<span class=hljs-meta >@precompile_all_calls</span> <span class=hljs-keyword >begin</span>
    <span class=hljs-keyword >function</span> lorenz(du, u, p, t)
        du[<span class=hljs-number >1</span>] = <span class=hljs-number >10.0</span>(u[<span class=hljs-number >2</span>] - u[<span class=hljs-number >1</span>])
        du[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>] * (<span class=hljs-number >28.0</span> - u[<span class=hljs-number >3</span>]) - u[<span class=hljs-number >2</span>]
        du[<span class=hljs-number >3</span>] = u[<span class=hljs-number >1</span>] * u[<span class=hljs-number >2</span>] - (<span class=hljs-number >8</span> / <span class=hljs-number >3</span>) * u[<span class=hljs-number >3</span>]
    <span class=hljs-keyword >end</span>

    nonstiff = [BS3(), Tsit5(), Vern7(), Vern9()]

    stiff = [Rosenbrock23(), Rosenbrock23(autodiff = <span class=hljs-literal >false</span>),
             Rodas4(), Rodas4(autodiff = <span class=hljs-literal >false</span>),
             Rodas5(), Rodas5(autodiff = <span class=hljs-literal >false</span>),
             Rodas5P(), Rodas5P(autodiff = <span class=hljs-literal >false</span>),
             TRBDF2(), TRBDF2(autodiff = <span class=hljs-literal >false</span>),
             KenCarp4(), KenCarp4(autodiff = <span class=hljs-literal >false</span>),
             QNDF(), QNDF(autodiff = <span class=hljs-literal >false</span>)]

    autoswitch = [
        AutoTsit5(Rosenbrock23()), AutoTsit5(Rosenbrock23(autodiff = <span class=hljs-literal >false</span>)),
        AutoTsit5(TRBDF2()), AutoTsit5(TRBDF2(autodiff = <span class=hljs-literal >false</span>)),
        AutoVern9(KenCarp47()), AutoVern9(KenCarp47(autodiff = <span class=hljs-literal >false</span>)),
        AutoVern9(Rodas5()), AutoVern9(Rodas5(autodiff = <span class=hljs-literal >false</span>)),
        AutoVern9(Rodas5P()), AutoVern9(Rodas5P(autodiff = <span class=hljs-literal >false</span>)),
        AutoVern7(Rodas4()), AutoVern7(Rodas4(autodiff = <span class=hljs-literal >false</span>)),
        AutoVern7(TRBDF2()), AutoVern7(TRBDF2(autodiff = <span class=hljs-literal >false</span>))]

    solver_list = []

    <span class=hljs-keyword >if</span> Preferences.<span class=hljs-meta >@load_preference</span>(<span class=hljs-string >&quot;PrecompileNonStiff&quot;</span>, <span class=hljs-literal >true</span>)
        append!(solver_list, nonstiff)
    <span class=hljs-keyword >end</span>

    <span class=hljs-keyword >if</span> Preferences.<span class=hljs-meta >@load_preference</span>(<span class=hljs-string >&quot;PrecompileStiff&quot;</span>, <span class=hljs-literal >true</span>)
        append!(solver_list, stiff)
    <span class=hljs-keyword >end</span>

    <span class=hljs-keyword >if</span> Preferences.<span class=hljs-meta >@load_preference</span>(<span class=hljs-string >&quot;PrecompileAutoSwitch&quot;</span>, <span class=hljs-literal >true</span>)
        append!(solver_list, autoswitch)
    <span class=hljs-keyword >end</span>

    prob_list = []

    <span class=hljs-keyword >if</span> Preferences.<span class=hljs-meta >@load_preference</span>(<span class=hljs-string >&quot;PrecompileDefaultSpecialize&quot;</span>, <span class=hljs-literal >true</span>)
        push!(prob_list, ODEProblem(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>)))
        push!(prob_list, ODEProblem(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), <span class=hljs-built_in >Float64</span>[]))
    <span class=hljs-keyword >end</span>

    <span class=hljs-keyword >if</span> Preferences.<span class=hljs-meta >@load_preference</span>(<span class=hljs-string >&quot;PrecompileAutoSpecialize&quot;</span>, <span class=hljs-literal >false</span>)
        push!(prob_list,
              ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.AutoSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>],
                                                         (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>)))
        push!(prob_list,
              ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.AutoSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>],
                                                         (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), <span class=hljs-built_in >Float64</span>[]))
    <span class=hljs-keyword >end</span>

    <span class=hljs-keyword >if</span> Preferences.<span class=hljs-meta >@load_preference</span>(<span class=hljs-string >&quot;PrecompileFunctionWrapperSpecialize&quot;</span>, <span class=hljs-literal >false</span>)
        push!(prob_list,
              ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.FunctionWrapperSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>],
                                                                    (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>)))
        push!(prob_list,
              ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.FunctionWrapperSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>],
                                                                    (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), <span class=hljs-built_in >Float64</span>[]))
    <span class=hljs-keyword >end</span>

    <span class=hljs-keyword >if</span> Preferences.<span class=hljs-meta >@load_preference</span>(<span class=hljs-string >&quot;PrecompileNoSpecialize&quot;</span>, <span class=hljs-literal >false</span>)
        push!(prob_list,
              ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.NoSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>)))
        push!(prob_list,
              ODEProblem{<span class=hljs-literal >true</span>, SciMLBase.NoSpecialize}(lorenz, [<span class=hljs-number >1.0</span>; <span class=hljs-number >0.0</span>; <span class=hljs-number >0.0</span>], (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>),
                                                       <span class=hljs-built_in >Float64</span>[]))
    <span class=hljs-keyword >end</span>

    <span class=hljs-keyword >for</span> prob <span class=hljs-keyword >in</span> prob_list, solver <span class=hljs-keyword >in</span> solver_list; solve(prob, solver)(<span class=hljs-number >5.0</span>); <span class=hljs-keyword >end</span>
<span class=hljs-keyword >end</span></code></pre>
<p>Then in the user&#39;s startup profile, precompilation amount can be toggled using the UUID of the OrdinaryDiffEq.jl package:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> Preferences, UUIDs
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileNonStiff&quot;</span> =&gt; <span class=hljs-literal >true</span>)
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileStiff&quot;</span> =&gt; <span class=hljs-literal >false</span>)
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileAutoSwitch&quot;</span> =&gt; <span class=hljs-literal >false</span>)
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileAutoSwitch&quot;</span> =&gt; <span class=hljs-literal >false</span>)
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileDefaultSpecialize&quot;</span> =&gt; <span class=hljs-literal >true</span>)
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileAutoSpecialize&quot;</span> =&gt; <span class=hljs-literal >false</span>)
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileFunctionWrapperSpecialize&quot;</span> =&gt; <span class=hljs-literal >false</span>)
set_preferences!(UUID(<span class=hljs-string >&quot;1dea7af3-3e70-54e6-95c3-0bf5283fa5ed&quot;</span>), <span class=hljs-string >&quot;PrecompileNoSpecialize&quot;</span> =&gt; <span class=hljs-literal >false</span>)</code></pre>
<h2 id=bonus_extra_profiling_tool ><a href="#bonus_extra_profiling_tool" class=header-anchor >Bonus Extra Profiling Tool</a></h2>
<p>I couldn&#39;t figure out where else to put this, so if you want to know the compile time contributions per method instance that is getting invalidated, you can print this out via:</p>
<pre><code class="julia hljs">julia&gt; show(<span class=hljs-literal >stdout</span>,<span class=hljs-string >MIME&quot;text/plain&quot;</span>(),staleinstances(tinf))
<span class=hljs-number >45</span>-element <span class=hljs-built_in >Vector</span>{SnoopCompileCore.InferenceTiming}:
 InferenceTiming: <span class=hljs-number >0.000051</span>/<span class=hljs-number >0.010410</span> on ForwardDiff.<span class=hljs-string >var&quot;#s10#33&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000442</span>/<span class=hljs-number >0.010359</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#34::ForwardDiff.var&quot;#34#35&quot;, ::Any)</span>
 InferenceTiming: <span class=hljs-number >0.000990</span>/<span class=hljs-number >0.008745</span> on collect(::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#34#35&quot;</span>}} <span class=hljs-keyword >where</span> _A)
 InferenceTiming: <span class=hljs-number >0.002431</span>/<span class=hljs-number >0.004820</span> on Base.collect_to_with_first!(::<span class=hljs-built_in >AbstractArray</span>, ::<span class=hljs-built_in >Expr</span>, ::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#34#35&quot;</span>}} <span class=hljs-keyword >where</span> _A, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000351</span>/<span class=hljs-number >0.006221</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#34::ForwardDiff.var&quot;#34#35&quot;, ::Int64)</span>
 InferenceTiming: <span class=hljs-number >0.000057</span>/<span class=hljs-number >0.000622</span> on Base.cconvert(::<span class=hljs-built_in >Type</span>{<span class=hljs-built_in >Int32</span>}, ::<span class=hljs-built_in >Enum</span>{T2}) <span class=hljs-keyword >where</span> T2&lt;:<span class=hljs-built_in >Integer</span>
 InferenceTiming: <span class=hljs-number >0.000170</span>/<span class=hljs-number >0.000565</span> on <span class=hljs-built_in >Int32</span>(::<span class=hljs-built_in >Enum</span>)
 InferenceTiming: <span class=hljs-number >0.000114</span>/<span class=hljs-number >0.000175</span> on Static.<span class=hljs-string >var&quot;#s3#1&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000129</span>/<span class=hljs-number >0.000199</span> on Static.<span class=hljs-string >var&quot;#s3#2&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000117</span>/<span class=hljs-number >0.000180</span> on Static.<span class=hljs-string >var&quot;#s3#3&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000129</span>/<span class=hljs-number >0.000193</span> on Static.<span class=hljs-string >var&quot;#s3#5&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.005421</span>/<span class=hljs-number >0.011408</span> on getindex(::Core.SimpleVector, ::<span class=hljs-built_in >AbstractArray</span>)
 InferenceTiming: <span class=hljs-number >0.000917</span>/<span class=hljs-number >0.000917</span> on Base.IteratorSize(::<span class=hljs-built_in >AbstractArray</span>)
 InferenceTiming: <span class=hljs-number >0.009340</span>/<span class=hljs-number >0.014736</span> on ArrayInterface.<span class=hljs-string >var&quot;#s13#18&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000730</span>/<span class=hljs-number >0.002170</span> on (::<span class=hljs-built_in >Colon</span>)(::<span class=hljs-built_in >Int64</span>, ::Static.StaticInt{U}) <span class=hljs-keyword >where</span> U
 InferenceTiming: <span class=hljs-number >0.001248</span>/<span class=hljs-number >0.001248</span> on ArrayInterface.OptionallyStaticUnitRange(::<span class=hljs-built_in >Int64</span>, ::<span class=hljs-built_in >Union</span>{<span class=hljs-built_in >Int64</span>, Static.StaticInt})
 InferenceTiming: <span class=hljs-number >0.000192</span>/<span class=hljs-number >0.000192</span> on ArrayInterface.OptionallyStaticUnitRange(::<span class=hljs-built_in >Int64</span>, ::<span class=hljs-built_in >Integer</span>)
 InferenceTiming: <span class=hljs-number >0.001313</span>/<span class=hljs-number >0.002110</span> on ArrayInterface.<span class=hljs-string >var&quot;#s13#21&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000906</span>/<span class=hljs-number >0.001342</span> on ArrayInterface.<span class=hljs-string >var&quot;#s13#22&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.001090</span>/<span class=hljs-number >0.001946</span> on Static.<span class=hljs-string >var&quot;#s3#27&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.002478</span>/<span class=hljs-number >0.004839</span> on ArrayInterface.<span class=hljs-string >var&quot;#s49#45&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Type</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.001974</span>/<span class=hljs-number >0.002299</span> on ArrayInterface.rank_to_sortperm(::<span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Vararg</span>{Static.StaticInt, N}}) <span class=hljs-keyword >where</span> N
 InferenceTiming: <span class=hljs-number >0.000070</span>/<span class=hljs-number >0.015091</span> on ForwardDiff.<span class=hljs-string >var&quot;#s10#21&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000489</span>/<span class=hljs-number >0.015021</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#22::ForwardDiff.var&quot;#22#23&quot;, ::Any)</span>
 InferenceTiming: <span class=hljs-number >0.001327</span>/<span class=hljs-number >0.013339</span> on collect(::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#22#23&quot;</span>}} <span class=hljs-keyword >where</span> _A)
 InferenceTiming: <span class=hljs-number >0.000279</span>/<span class=hljs-number >0.000279</span> on Base._array_for(::<span class=hljs-built_in >Type</span>{<span class=hljs-built_in >Symbol</span>}, ::<span class=hljs-built_in >Any</span>, Base.HasLength()::Base.HasLength)
 InferenceTiming: <span class=hljs-number >0.002368</span>/<span class=hljs-number >0.007726</span> on Base.collect_to_with_first!(::<span class=hljs-built_in >AbstractArray</span>, ::<span class=hljs-built_in >Symbol</span>, ::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#22#23&quot;</span>}} <span class=hljs-keyword >where</span> 
_A, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.002375</span>/<span class=hljs-number >0.005359</span> on Base.collect_to!(::<span class=hljs-built_in >AbstractArray</span>, ::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#22#23&quot;</span>}} <span class=hljs-keyword >where</span> _A, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)    
 InferenceTiming: <span class=hljs-number >0.002984</span>/<span class=hljs-number >0.002984</span> on Base.setindex_widen_up_to(::<span class=hljs-built_in >AbstractArray</span>, ::<span class=hljs-built_in >Symbol</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000394</span>/<span class=hljs-number >0.006532</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#22::ForwardDiff.var&quot;#22#23&quot;, ::Int64)</span>
 InferenceTiming: <span class=hljs-number >0.000053</span>/<span class=hljs-number >0.010643</span> on ForwardDiff.<span class=hljs-string >var&quot;#s10#42&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000459</span>/<span class=hljs-number >0.010590</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#43::ForwardDiff.var&quot;#43#44&quot;, ::Any)</span>
 InferenceTiming: <span class=hljs-number >0.001090</span>/<span class=hljs-number >0.008975</span> on collect(::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#43#44&quot;</span>}} <span class=hljs-keyword >where</span> _A)
 InferenceTiming: <span class=hljs-number >0.002429</span>/<span class=hljs-number >0.004903</span> on Base.collect_to_with_first!(::<span class=hljs-built_in >AbstractArray</span>, ::<span class=hljs-built_in >Expr</span>, ::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#43#44&quot;</span>}} <span class=hljs-keyword >where</span> _A, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000367</span>/<span class=hljs-number >0.006414</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#43::ForwardDiff.var&quot;#43#44&quot;, ::Int64)</span>
 InferenceTiming: <span class=hljs-number >0.000088</span>/<span class=hljs-number >0.032105</span> on ForwardDiff.<span class=hljs-string >var&quot;#s10#45&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000833</span>/<span class=hljs-number >0.032017</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#46::ForwardDiff.var&quot;#46#47&quot;, ::Any)</span>
 InferenceTiming: <span class=hljs-number >0.001881</span>/<span class=hljs-number >0.011196</span> on collect(::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#46#47&quot;</span>}} <span class=hljs-keyword >where</span> _A)
 InferenceTiming: <span class=hljs-number >0.002700</span>/<span class=hljs-number >0.005390</span> on Base.collect_to_with_first!(::<span class=hljs-built_in >AbstractArray</span>, ::<span class=hljs-built_in >Expr</span>, ::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#46#47&quot;</span>}} <span class=hljs-keyword >where</span> _A, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000511</span>/<span class=hljs-number >0.007142</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#46::ForwardDiff.var&quot;#46#47&quot;, ::Int64)</span>
 InferenceTiming: <span class=hljs-number >0.000059</span>/<span class=hljs-number >0.011278</span> on ForwardDiff.<span class=hljs-string >var&quot;#s10#48&quot;</span>(::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000487</span>/<span class=hljs-number >0.011219</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#49::ForwardDiff.var&quot;#49#50&quot;, ::Any)</span>
 InferenceTiming: <span class=hljs-number >0.001180</span>/<span class=hljs-number >0.009389</span> on collect(::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#49#50&quot;</span>}} <span class=hljs-keyword >where</span> _A)
 InferenceTiming: <span class=hljs-number >0.002482</span>/<span class=hljs-number >0.004950</span> on Base.collect_to_with_first!(::<span class=hljs-built_in >AbstractArray</span>, ::<span class=hljs-built_in >Expr</span>, ::Base.Generator{_A, ForwardDiff.<span class=hljs-string >var&quot;#16#17&quot;</span>{ForwardDiff.<span class=hljs-string >var&quot;#49#50&quot;</span>}} <span class=hljs-keyword >where</span> _A, ::<span class=hljs-built_in >Any</span>)
 InferenceTiming: <span class=hljs-number >0.000393</span>/<span class=hljs-number >0.007056</span> on ForwardDiff.tupexpr(<span class=hljs-comment >#49::ForwardDiff.var&quot;#49#50&quot;, ::Int64)</span></code></pre>
<h2 id=constant_type_handling_for_automatic_differentiation_with_forwarddiffjl ><a href="#constant_type_handling_for_automatic_differentiation_with_forwarddiffjl" class=header-anchor >Constant Type Handling for Automatic Differentiation with ForwardDiff.jl</a></h2>
<p>If one naively uses ForwardDiff.jl inside of their solver package, then there are two things that will not be easily handled in the wrapper: the tag type and the chunk size. I already mentioned above that one simple hack is to always force chunk size equal to one. For the tag type, you will need to setup a <a href="https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/">package tag</a> so that the automatic differentiation is not dependent on the the function type and is instead constant for the package. I highly recommend using  <a href="https://github.com/JuliaDiff/SparseDiffTools.jl">SparseDiffTools.jl instead</a>, since its higher level calls allow for setting the tag and chunk sizes more directly.</p>
<p>Once that is done, you can directly compute all of the possible method dispatchs with and without automatic differentiation, and generate the FunctionWrappersWrapper to handle all of the combinations. For DiffEqBase this looks like:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> wrapfun_iip(ff,
                     inputs::<span class=hljs-built_in >Tuple</span>{T1, T2, T3, T4}) <span class=hljs-keyword >where</span> {T1, T2, T3, T4}
    T = eltype(T2)
    dualT = dualgen(T)
    dualT1 = ArrayInterfaceCore.promote_eltype(T1, dualT)
    dualT2 = ArrayInterfaceCore.promote_eltype(T2, dualT)
    dualT4 = dualgen(promote_type(T, T4))

    iip_arglists = (<span class=hljs-built_in >Tuple</span>{T1, T2, T3, T4},
                    <span class=hljs-built_in >Tuple</span>{dualT1, dualT2, T3, T4},
                    <span class=hljs-built_in >Tuple</span>{dualT1, T2, T3, dualT4},
                    <span class=hljs-built_in >Tuple</span>{dualT1, dualT2, T3, dualT4})

    iip_returnlists = ntuple(x -&gt; <span class=hljs-built_in >Nothing</span>, <span class=hljs-number >4</span>)

    fwt = map(iip_arglists, iip_returnlists) <span class=hljs-keyword >do</span> A, R
        FunctionWrappersWrappers.FunctionWrappers.FunctionWrapper{R, A}(Void(ff))
    <span class=hljs-keyword >end</span>
    FunctionWrappersWrappers.FunctionWrappersWrapper{typeof(fwt), <span class=hljs-literal >false</span>}(fwt)
<span class=hljs-keyword >end</span></code></pre>
<p>Note that we use the <code>ArrayInterfaceCore.promote_eltype&#40;T1, dualT&#41;</code> function to find out how to promote <code>Vector&#123;Float64&#125;</code> to <code>Vector&#123;Dual&#123;...&#125;&#125;</code> in a generic way. Given the promotions that have to happen for automatic differentation support, we need to safeguard this dispatch by requiring that the promotion rules exist. We check for these method dispatches at compile time using <a href="https://github.com/oxinabox/Tricks.jl">Tricks.jl</a>, and thus the safe version does not  wrap if these don&#39;t exist:</p>
<pre><code class="julia hljs">f = <span class=hljs-keyword >if</span> f <span class=hljs-keyword >isa</span> ODEFunction &amp;&amp; isinplace(f) &amp;&amp; !(f.f <span class=hljs-keyword >isa</span> AbstractDiffEqOperator) &amp;&amp;
        <span class=hljs-comment ># Some reinitialization code still uses NLSolvers stuff which doesn&#x27;t</span>
        <span class=hljs-comment ># properly tag, so opt-out if potentially a mass matrix DAE</span>
        f.mass_matrix <span class=hljs-keyword >isa</span> UniformScaling &amp;&amp;
        <span class=hljs-comment ># Jacobians don&#x27;t wrap, so just ignore those cases</span>
        f.jac === <span class=hljs-literal >nothing</span> &amp;&amp;
        ((specialize === SciMLBase.AutoSpecialize &amp;&amp; eltype(u0) !== <span class=hljs-built_in >Any</span> &amp;&amp;
          RecursiveArrayTools.recursive_unitless_eltype(u0) === eltype(u0) &amp;&amp;
          one(t) === oneunit(t) &amp;&amp;
          Tricks.static_hasmethod(ArrayInterfaceCore.promote_eltype,
                                  <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Type</span>{typeof(u0)}, <span class=hljs-built_in >Type</span>{dualgen(eltype(u0))}}) &amp;&amp;
          Tricks.static_hasmethod(promote_rule,
                                  <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Type</span>{eltype(u0)}, <span class=hljs-built_in >Type</span>{dualgen(eltype(u0))}}) &amp;&amp;
          Tricks.static_hasmethod(promote_rule,
                                  <span class=hljs-built_in >Tuple</span>{<span class=hljs-built_in >Type</span>{eltype(u0)}, <span class=hljs-built_in >Type</span>{typeof(t)}})) ||
        (specialize === SciMLBase.FunctionWrapperSpecialize &amp;&amp;
          !(f.f <span class=hljs-keyword >isa</span> FunctionWrappersWrappers.FunctionWrappersWrapper)))
    <span class=hljs-keyword >return</span> unwrapped_f(f, wrapfun_iip(f.f, (u0, u0, p, t)))
<span class=hljs-keyword >else</span>
    <span class=hljs-keyword >return</span> f
<span class=hljs-keyword >end</span></code></pre>
<p>Note that since we are not wrapping the <code>jac</code> type, there&#39;s no reason to wrap <code>f</code> since it will recompile anyways. That&#39;s just a current limitation of the design which can get lifted after I&#39;m done spending too much time writing blog posts.</p>
<p>Tada&#33; Take care all.</p>
<!-- Footer-->
<footer class="footer bg-light">
  <div class=container >
      <div class=row >
          <div class="col-lg-6 h-100 text-center text-lg-start my-auto">
              <ul class="list-inline mb-2">
                  
                  <li class=list-inline-item ><a href="/community">Contact</a>
                  <!-- <li class=list-inline-item ><a href="#!">Terms of Use</a>
                  <li class=list-inline-item ><a href="#!">Privacy Policy</a> -->
              </ul>
              <p class="text-muted small mb-4 mb-lg-0">Website powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia</a> programming language. &copy; SciML 2022. All Rights Reserved.</p>
              <p class="text-muted small mb-4 mb-lg-0">Edit on <a href="https://github.com/SciML/sciml.ai">GitHub</a></p>

            </div>
          <div class="col-lg-6 h-100 text-center text-lg-end my-auto">
              <ul class="list-inline mb-0">
                  <li class="list-inline-item me-4">
                      <a href="https://github.com/SciML"><i class="bi-github fs-3"></i></a>
                  
                  <li class="list-inline-item me-4">
                      <a href="https://twitter.com/SciML_Org"><i class="bi-twitter fs-3"></i></a>
                  
                  <li class=list-inline-item >
                      <a href="https://www.linkedin.com/company/the-julia-language"><i class="bi-linkedin fs-3"></i></a>
                  
              </ul>
          </div>
      </div>
  </div>
</footer>
</div>