<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta property="og:title" content="SciML: Open Source Software for Scientific Machine Learning"> <meta property="og:description" content="Open Source Software for Scientific Machine Learning"> <meta property="og:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta property="og:url" content="https://sciml.ai"> <meta name="twitter:title" content=SciML > <meta name="twitter:description" content="Open Source Software for Scientific Machine Learning"> <meta name="twitter:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <!-- Favicon--> <link rel=icon  type="image/x-icon" href="assets/favicon.png" /> <!-- Bootstrap icons--> <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel=stylesheet  type="text/css" /> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css"> <!-- Google fonts--> <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet  type="text/css" /> <!-- Core theme CSS (includes Bootstrap)--> <link href="./css/styles.css" rel=stylesheet  /> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/styles.css"> <link rel=stylesheet  href="/css/hypertext.css"> <link rel=icon  href="/assets/favicon.png"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-2"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-90474609-2'); </script> <div class=container-fluid  id=top-alert > <div class="alert alert-dark alert-dismissible mb-0" role=alert > <p class=text-center > <a href="https://www.youtube.com/playlist?list=PLP8iPy9hna6QglWLQM02jcVjEBjaamzvw">Watch the recordings of SciMLCon 2022</a> </p> <!-- <button type=button  class=close  data-dismiss=alert  aria-label=Close > <span aria-hidden=true >&times;</span> --> </button> </div> </div> <header> <nav class="navbar navbar-light bg-light static-top"> <div class=container > <a class="btn btn-primary" href="/" class=current >Home</a> <a class=navbar-brand  href="/news/">News</a> <a class=navbar-brand  href="/roadmap/">Roadmap</a> <a class=navbar-brand  href="/citing/">Citing</a> <a class=navbar-brand  href="/showcase/">Showcase</a> <a class=navbar-brand  href="/challenge/">Challenge Problems</a> <a class=navbar-brand  href="/community/">Community</a> <hr/> <a class=navbar-brand  href="/documentation/">Documentation</a> <a class=navbar-brand  href="/dev/">Dev Programs</a> <a class=navbar-brand  href="/governance/">Governance</a> <a class=navbar-brand  href="https://benchmarks.sciml.ai/">Benchmarks</a> <a class=navbar-brand  href="https://github.com/SciML/">Source Code</a> <a class=navbar-brand  href="https://numfocus.org/donate-to-sciml">Donate</a> </div> </nav> </header> <div class=franklin-content ><h1 id=sciml_ecosystem_update_neural_pdes_lie_groups_and_stochastic_delay_differential_equations ><a href="#sciml_ecosystem_update_neural_pdes_lie_groups_and_stochastic_delay_differential_equations" class=header-anchor >SciML Ecosystem Update: Neural PDEs, Lie Groups, and Stochastic Delay Differential Equations</a></h1> <p>Another week, another SciML release&#33; Yes, this has been a crazy productive summer so thanks to everyone who has been involved. Let&#39;s dive in.</p> <h2 id=neuralpdejl_physics-informed_neural_networks_for_automated_pde_solving_and_high_dimensional_pdes ><a href="#neuralpdejl_physics-informed_neural_networks_for_automated_pde_solving_and_high_dimensional_pdes" class=header-anchor >NeuralPDE.jl: Physics-Informed Neural Networks for Automated PDE Solving and High Dimensional PDEs</a></h2> <p>We have officially released <a href="https://neuralpde.sciml.ai/dev/">NeuralPDE.jl</a>. This is a domain-specific library for gathering all of the neural network based PDE solver methods. Its main focus are on the two big branches of scientific machine learning:</p> <ol> <li><p>Physics-informed neural networsk &#40;PINNs&#41;</p> <li><p>Forward-Backwards Stochastic Differential Equations for high dimensional PDEs</p> </ol> <p>On the PINN front, the library is all about <strong>automated solution of PDEs</strong> we want users who have no experience in partial differential equations to be able to slap down a symbolic description of the partial differential equation and get a reasonable result without having to know about details like discretization. To see this in action, let&#39;s look at solving the 2-dimensional Poisson equation with this library. We start by describing the PDE:</p> <pre><code class="julia hljs"><span class=hljs-meta >@parameters</span> x y θ
<span class=hljs-meta >@variables</span> u(..)
<span class=hljs-meta >@derivatives</span> Dxx&#x27;&#x27;~x
<span class=hljs-meta >@derivatives</span> Dyy&#x27;&#x27;~y

<span class=hljs-comment ># 2D PDE</span>
eq  = Dxx(u(x,y,θ)) + Dyy(u(x,y,θ)) ~ -sin(<span class=hljs-literal >pi</span>*x)*sin(<span class=hljs-literal >pi</span>*y)

<span class=hljs-comment ># Boundary conditions</span>
bcs = [u(<span class=hljs-number >0</span>,y,θ) ~ <span class=hljs-number >0.f0</span>, u(<span class=hljs-number >1</span>,y,θ) ~ -sin(<span class=hljs-literal >pi</span>*<span class=hljs-number >1</span>)*sin(<span class=hljs-literal >pi</span>*y),
       u(x,<span class=hljs-number >0</span>,θ) ~ <span class=hljs-number >0.f0</span>, u(x,<span class=hljs-number >1</span>,θ) ~ -sin(<span class=hljs-literal >pi</span>*x)*sin(<span class=hljs-literal >pi</span>*<span class=hljs-number >1</span>)]
<span class=hljs-comment ># Space and time domains</span>
domains = [x ∈ IntervalDomain(<span class=hljs-number >0.0</span>,<span class=hljs-number >1.0</span>),
           y ∈ IntervalDomain(<span class=hljs-number >0.0</span>,<span class=hljs-number >1.0</span>)]</code></pre> <p>Here we described the PDE by its Julia code because, why not: it&#39;s as informative and refined as mathematical notation itself&#33; Now let&#39;s tell the system to discretize and solve this PDE with a neural network:</p> <pre><code class="julia hljs">dx = <span class=hljs-number >0.1</span> <span class=hljs-comment ># Discretization size for sampling purposes</span>
discretization = PhysicsInformedNN(dx)

<span class=hljs-comment ># Neural network and optimizer</span>
opt = Flux.ADAM(<span class=hljs-number >0.02</span>)
dim = <span class=hljs-number >2</span> <span class=hljs-comment ># number of dimensions</span>
chain = FastChain(FastDense(dim,<span class=hljs-number >16</span>,Flux.σ),FastDense(<span class=hljs-number >16</span>,<span class=hljs-number >16</span>,Flux.σ),FastDense(<span class=hljs-number >16</span>,<span class=hljs-number >1</span>))

pde_system = PDESystem(eq,bcs,domains,[x,y],[u])
prob = discretize(pde_system,discretization)
alg = NNDE(chain,opt,autodiff=<span class=hljs-literal >false</span>)</code></pre> <p>and then we solve it:</p> <pre><code class="julia hljs">phi,res  = solve(prob,alg,verbose=<span class=hljs-literal >true</span>, maxiters=<span class=hljs-number >5000</span>)</code></pre>
<p><img src="https://user-images.githubusercontent.com/12683885/88482882-cbc00c80-cf6c-11ea-91bb-47a477f38af6.png" alt="" /></p>
<p>And boom, that&#39;s the solution to the PDE. We are continuing to improve this framework, and refactor some of the pieces so that it better connects to more ML and scientific computing library ecosystems, but it&#39;s achieving its general goal today so we&#39;ve decided to release it. Major thanks to @KirillZubov for these developments.</p>
<p>Along with this symbolic form, there are FBSDE methods specifically written for parabolic equations and Kolmogorov backwards equations. This means that high dimensional PDEs that show up in finance, biology, and beyond can now quickly be solved with a neural network. Here&#39;s the code to solve a 100 dimensional Hamilton-Jacobi-Bellman equation for LQG optimal control:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> NeuralPDE
<span class=hljs-keyword >using</span> Flux
<span class=hljs-keyword >using</span> DifferentialEquations
<span class=hljs-keyword >using</span> LinearAlgebra
d = <span class=hljs-number >100</span> <span class=hljs-comment ># number of dimensions</span>
X0 = fill(<span class=hljs-number >0.0f0</span>, d) <span class=hljs-comment ># initial value of stochastic control process</span>
tspan = (<span class=hljs-number >0.0f0</span>, <span class=hljs-number >1.0f0</span>)
λ = <span class=hljs-number >1.0f0</span>

g(X) = log(<span class=hljs-number >0.5f0</span> + <span class=hljs-number >0.5f0</span> * sum(X.^<span class=hljs-number >2</span>))
f(X,u,σᵀ∇u,p,t) = -λ * sum(σᵀ∇u.^<span class=hljs-number >2</span>)
μ_f(X,p,t) = zero(X)  <span class=hljs-comment ># Vector d x 1 λ</span>
σ_f(X,p,t) = Diagonal(sqrt(<span class=hljs-number >2.0f0</span>) * ones(<span class=hljs-built_in >Float32</span>, d)) <span class=hljs-comment ># Matrix d x d</span>
prob = TerminalPDEProblem(g, f, μ_f, σ_f, X0, tspan)
hls = <span class=hljs-number >10</span> + d <span class=hljs-comment ># hidden layer size</span>
opt = Flux.ADAM(<span class=hljs-number >0.01</span>)  <span class=hljs-comment ># optimizer</span>
<span class=hljs-comment ># sub-neural network approximating solutions at the desired point</span>
u0 = Flux.Chain(Dense(d, hls, relu),
                Dense(hls, hls, relu),
                Dense(hls, <span class=hljs-number >1</span>))
<span class=hljs-comment ># sub-neural network approximating the spatial gradients at time point</span>
σᵀ∇u = Flux.Chain(Dense(d + <span class=hljs-number >1</span>, hls, relu),
                  Dense(hls, hls, relu),
                  Dense(hls, hls, relu),
                  Dense(hls, d))
pdealg = NNPDENS(u0, σᵀ∇u, opt=opt)
<span class=hljs-meta >@time</span> ans = solve(prob, pdealg, verbose=<span class=hljs-literal >true</span>, maxiters=<span class=hljs-number >100</span>, trajectories=<span class=hljs-number >100</span>,
                            alg=EM(), dt=<span class=hljs-number >1.2</span>, pabstol=<span class=hljs-number >1f-2</span>)</code></pre>
<p>Boom: that&#39;s all there is to it. <a href="https://neuralpde.sciml.ai/dev/">Check out the documentation for more details</a>. There&#39;s still a lot of active development here so if you&#39;re a student who&#39;s interested in this topic, please get in touch.</p>
<h2 id=lie_group_integrators_magnus_rungekuttamunthe-kaas_and_crouchgrossman_methods ><a href="#lie_group_integrators_magnus_rungekuttamunthe-kaas_and_crouchgrossman_methods" class=header-anchor >Lie Group Integrators: Magnus, Runge–Kutta–Munthe-Kaas, and Crouch–Grossman Methods</a></h2>
<p>There are many differential equations which specifically fall under the form of <code>u&#39; &#61; A&#40;t&#41;u</code>, or <code>u&#39; &#61; A&#40;u&#41;u</code>. In these cases, you have geometric properties, like Lie groups, that you can exploit in the solution of the ODE. These Lie group methods are commonly embedded in domain-specific software, usually in robotics, so they are not generally seen except by practitioners of specific scientific areas trying to get the most robust and performant methods in these cases.</p>
<p>Well, SciML wants the most robust and performant methods, so we have now included these methods as part of the standard DifferentialEquations.jl suite thanks to Biswajit Ghosh &#40;@Biswajitghosh98&#41; and <a href="https://mlh.io/">Major League Hacking &#40;MLH&#41;</a>. To use these methods, you have to define your ODE via a DiffEqOperator. For example:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> update_func(A,u,p,t)
    A[<span class=hljs-number >1</span>,<span class=hljs-number >1</span>] = cos(t)
    A[<span class=hljs-number >2</span>,<span class=hljs-number >1</span>] = sin(t)
    A[<span class=hljs-number >1</span>,<span class=hljs-number >2</span>] = -sin(t)
    A[<span class=hljs-number >2</span>,<span class=hljs-number >2</span>] = cos(t)
<span class=hljs-keyword >end</span>
A = DiffEqArrayOperator(ones(<span class=hljs-number >2</span>,<span class=hljs-number >2</span>),update_func=update_func)
prob = ODEProblem(A, ones(<span class=hljs-number >2</span>), (<span class=hljs-number >1.0</span>, <span class=hljs-number >6.0</span>))
sol = solve(prob,MagnusGL6(),dt=<span class=hljs-number >1</span>/<span class=hljs-number >10</span>)</code></pre>
<p>that is a quick and easy way to utilize a 6th order Magnus integrator for the <code>u&#39; &#61; A&#40;t&#41;u</code> equation. We have high order methods and adaptive methods, and these all utilize as much mutation as possible to try and be efficient. There&#39;s still some optimization that can be done, but the methods are well-tested for correctness and ready to be used where you see fit&#33;</p>
<h2 id=stochastic_delay_differential_equations_and_stochastic_differential-algebraic_equations ><a href="#stochastic_delay_differential_equations_and_stochastic_differential-algebraic_equations" class=header-anchor >Stochastic Delay Differential Equations and Stochastic Differential-Algebraic Equations</a></h2>
<p>It&#39;s finally here&#33; A lot of people had found <a href="http://real.mtak.hu/106039/1/ENOC_StochasticDelayDIffEq.pdf">our ENOC 2020 paper on StochasticDelayDiffEq.jl</a>, but we had to spend some time getting the library to our continuous integration testing and documentation standard before releasing. Well, now it&#39;s finally here. StochasticDelayDiffEq.jl allows for solving stochastic differential equations with delayed components and includes higher order and adaptive integrators. It&#39;s built on StochasticDiffEq.jl so the methods that you know and love have been transferred to this new domain. It uses all of the development from DelayDiffEq.jl to give robust SDDE solving. SDDEs are very difficult equations to solve, but we try to make it as efficient as possible. Thanks to everyone who was involved, including Henrik Sykora &#40;@HTSykora&#41;, for making this possible. For the reveal, here&#39;s an SDDE solved with a Milstein method with adaptive time stepping:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >function</span> hayes_modelf(du,u,h,p,t)
    τ,a,b,c,α,β,γ = p
    du .= a.*u .+ b .* h(p,t-τ) .+ c
<span class=hljs-keyword >end</span>
<span class=hljs-keyword >function</span> hayes_modelg(du,u,h,p,t)
    τ,a,b,c,α,β,γ = p
    du .= α.*u .+ γ
<span class=hljs-keyword >end</span>
h(p,t) = (ones(<span class=hljs-number >1</span>) .+ t);
tspan = (<span class=hljs-number >0.</span>,<span class=hljs-number >10.</span>)

pmul = [<span class=hljs-number >1.0</span>,-<span class=hljs-number >4.</span>,-<span class=hljs-number >2.</span>,<span class=hljs-number >10.</span>,-<span class=hljs-number >1.3</span>,-<span class=hljs-number >1.2</span>, <span class=hljs-number >1.1</span>]
padd = [<span class=hljs-number >1.0</span>,-<span class=hljs-number >4.</span>,-<span class=hljs-number >2.</span>,<span class=hljs-number >10.</span>,-<span class=hljs-number >0.0</span>,-<span class=hljs-number >0.0</span>, <span class=hljs-number >0.1</span>]

prob = SDDEProblem(hayes_modelf, hayes_modelg, [<span class=hljs-number >1.</span>], h, tspan, pmul; constant_lags = (pmul[<span class=hljs-number >1</span>],));
sol = solve(prob,RKMil())</code></pre>
<p>In this same vein, SDAEs are possible via singular mass matrices. These have proper testing and now have the official release along with documentation in the latest docs.</p>
<p>With these two announcements, note that because the SciML software composes, you can solve SDDAEs. And yes, these are compatible with neural networks and DiffEqFlux. Go have fun.</p>
<h2 id=automated_ensemble_parallelism_and_multiple_gpus_in_diffeqflux ><a href="#automated_ensemble_parallelism_and_multiple_gpus_in_diffeqflux" class=header-anchor >Automated Ensemble Parallelism and Multiple GPUs in DiffEqFlux</a></h2>
<p>You can now mix ensemble parallelism, and thus multi-GPU computation, with DiffEqFlux and reverse-mode automatic differentiation. An example of the multithreaded computation of an ensemble which is then trained is as follows:</p>
<pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq, DiffEqSensitivity, Flux, Test
pa = [<span class=hljs-number >1.0</span>]
u0 = [<span class=hljs-number >3.0</span>]

<span class=hljs-keyword >function</span> model2()
  prob = ODEProblem((u, p, t) -&gt; <span class=hljs-number >1.01</span>u .* p, u0, (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), pa)

  <span class=hljs-keyword >function</span> prob_func(prob, i, repeat)
    remake(prob, u0 = <span class=hljs-number >0.5</span> .+ i/<span class=hljs-number >100</span> .* prob.u0)
  <span class=hljs-keyword >end</span>

  ensemble_prob = EnsembleProblem(prob, prob_func = prob_func)
  sim = solve(ensemble_prob, Tsit5(), EnsembleThreads(), saveat = <span class=hljs-number >0.1</span>, trajectories = <span class=hljs-number >100</span>).u
<span class=hljs-keyword >end</span>
loss() = sum(abs2,[sum(abs2,<span class=hljs-number >1.0</span>.-u) <span class=hljs-keyword >for</span> u <span class=hljs-keyword >in</span> model2()])

pa = [<span class=hljs-number >1.0</span>]
u0 = [<span class=hljs-number >3.0</span>]
opt = ADAM(<span class=hljs-number >0.1</span>)
println(<span class=hljs-string >&quot;Starting to train&quot;</span>)
l1 = loss()
Flux.<span class=hljs-meta >@epochs</span> <span class=hljs-number >10</span> Flux.train!(loss, params([pa,u0]), data, opt; cb = cb)
l2 = loss()
<span class=hljs-meta >@test</span> <span class=hljs-number >10</span>l2 &lt; l1</code></pre>
<h2 id=major_performance_improvements_to_parallelized_extrapolation_methods ><a href="#major_performance_improvements_to_parallelized_extrapolation_methods" class=header-anchor >Major performance improvements to parallelized extrapolation methods</a></h2>
<p>Thanks to Utkarsh &#40;@utkarsh530&#41;, we now have fast parallelized implicit extrapolation in OrdinaryDiffEq.jl. You&#39;ll find these <a href="https://diffeq.sciml.ai/dev/solvers/ode_solve/#Parallelized-Implicit-Extrapolation-Methods">in the documentation</a> as <code>ImplicitEulerExtrapolation</code>, <code>ImplicitDeuflhardExtrapolation</code>, and <code>ImplicitHairerWannerExtrapolation</code>. For those ODE-inclined, these are pure Julia implementations of SEULEX and SODEX which include automated multithreaded parallelization of the <code>f</code> calls.</p>
<h2 id=new_surrogates_gradient-enhanced_kriging ><a href="#new_surrogates_gradient-enhanced_kriging" class=header-anchor >New surrogates: Gradient-Enhanced Kriging</a></h2>
<p>Surrogates.jl continues to march forward. If you have not seen the documentation recently, do <a href="https://surrogates.sciml.ai/latest">check it out</a> as it has undergone many major improvements, including showing differences between surrogates on many benchmark problems. One of the latest enhancements is Gradient-Enhanced Kriging, which is an extension to Kriging that can utilize derivative information &#40;from automatic differentiation&#41; to improve the convergence of the surrogate with less samples. Thank Ludovico Bessi &#40;@ludoro&#41; for driving this surrogate project.</p>
<h1 id=next_directions ><a href="#next_directions" class=header-anchor >Next Directions</a></h1>
<p>The next directions are going to be highly tied to the directions that we are going with the latest Google Summer of Code, so here are a few things to look forward to:</p>
<ul>
<li><p>Higher efficiency low-storage Runge-Kutta methods with a demonstration of optimality in a large-scale climate model &#40;&#33;&#33;&#33;&#41;.</p>

<li><p>Continued improvements to parallel and sparse automatic differentiation.</p>

<li><p>More SDE solvers and adjoints</p>

<li><p>More performance</p>

</ul>
<!-- Footer-->
<footer class="footer bg-light">
  <div class=container >
      <div class=row >
          <div class="col-lg-6 h-100 text-center text-lg-start my-auto">
              <ul class="list-inline mb-2">
                  
                  <li class=list-inline-item ><a href="/community">Contact</a>
                  <!-- <li class=list-inline-item ><a href="#!">Terms of Use</a>
                  <li class=list-inline-item ><a href="#!">Privacy Policy</a> -->
              </ul>
              <p class="text-muted small mb-4 mb-lg-0">Website powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia</a> programming language. &copy; SciML 2022. All Rights Reserved.</p>
              <p class="text-muted small mb-4 mb-lg-0">Edit on <a href="https://github.com/SciML/sciml.ai">GitHub</a></p>

            </div>
          <div class="col-lg-6 h-100 text-center text-lg-end my-auto">
              <ul class="list-inline mb-0">
                  <li class="list-inline-item me-4">
                      <a href="https://github.com/SciML"><i class="bi-github fs-3"></i></a>
                  
                  <li class="list-inline-item me-4">
                      <a href="https://twitter.com/SciML_Org"><i class="bi-twitter fs-3"></i></a>
                  
                  <li class=list-inline-item >
                      <a href="https://www.linkedin.com/company/the-julia-language"><i class="bi-linkedin fs-3"></i></a>
                  
              </ul>
          </div>
      </div>
  </div>
</footer>
</div>