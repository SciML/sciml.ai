<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta property="og:title" content="SciML: Open Source Software for Scientific Machine Learning"> <meta property="og:description" content="Open Source Software for Scientific Machine Learning"> <meta property="og:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta property="og:url" content="https://sciml.ai"> <meta name="twitter:title" content=SciML > <meta name="twitter:description" content="Open Source Software for Scientific Machine Learning"> <meta name="twitter:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <!-- Favicon--> <link rel=icon  type="image/x-icon" href="assets/favicon.png" /> <!-- Bootstrap icons--> <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel=stylesheet  type="text/css" /> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css"> <!-- Google fonts--> <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet  type="text/css" /> <!-- Core theme CSS (includes Bootstrap)--> <link href="./css/styles.css" rel=stylesheet  /> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/styles.css"> <link rel=stylesheet  href="/css/hypertext.css"> <link rel=icon  href="/assets/favicon.png"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-2"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-90474609-2'); </script> <title>Mixed Precision Linear Solvers and Enhanced BLAS Integration in LinearSolve.jl</title> <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.3/dist/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin=anonymous ></script> <div class=container-fluid  id=top-alert > <div class="alert alert-dark alert-dismissible mb-0" role=alert > <p class=text-center > <a href="https://youtu.be/yHiyJQdWBY8">Check out the latest talk: "The Continuing Advancements of Scientific Machine Learning (SciML)"</a> </p> <!-- <button type=button  class=close  data-dismiss=alert  aria-label=Close > <span aria-hidden=true >&times;</span> --> </button> </div> </div> <header> <nav class="navbar navbar-expand-lg navbar-light"> <a class=navbar-brand  href="/">Home</a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class="nav-item active"> <a class=nav-link  href="https://docs.sciml.ai/">Documentation</a> <li class=nav-item > <a class=nav-link  href="/news/">News</a> <li class=nav-item > <a class=nav-link  href="/roadmap/">Roadmap</a> <li class=nav-item > <a class=nav-link  href="/citing/">Citing</a> <li class=nav-item > <a class=nav-link  href="/showcase/">Showcase</a> <li class=nav-item > <a class=nav-link  href="https://benchmarks.sciml.ai/">Benchmarks</a> <li class=nav-item > <a class=nav-link  href="https://github.com/SciML/">GitHub</a> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id=navbarDropdownMenuLink  role=button  data-toggle=dropdown  aria-haspopup=true  aria-expanded=false > Community </a> <div class=dropdown-menu  aria-labelledby=navbarDropdownMenuLink > <a class=dropdown-item  href="/community/">Community Home</a> <a class=dropdown-item  href="/governance/">Governance</a> <a class=dropdown-item  href="/coc/">Code of Conduct</a> <a class=dropdown-item  href="/challenge/">Challenges</a> <a class=dropdown-item  href="/dev/">Developer Programs</a> </div> <li class=nav-item > <a class=nav-link  href="https://juliahub.com/company/contact-us-sciml"> Commercial Support </a> <li class=nav-item > <a class=nav-link  href="https://numfocus.org/donate-to-sciml"><i class="bi bi-heart"></i> Donate</a> </ul> </div> </nav> </header> <div class=franklin-content ><h1 id=mixed_precision_linear_solvers_and_enhanced_blas_integration_in_linearsolvejl ><a href="#mixed_precision_linear_solvers_and_enhanced_blas_integration_in_linearsolvejl" class=header-anchor >Mixed Precision Linear Solvers and Enhanced BLAS Integration in LinearSolve.jl</a></h1> <p>LinearSolve.jl has received a major expansion of its solver capabilities over the summer of 2025, with the introduction of comprehensive mixed precision linear solvers and enhanced BLAS library integration. These developments provide significant performance improvements for memory-bandwidth limited problems while expanding hardware support across different platforms.</p> <h2 id=mixed_precision_linear_solvers ><a href="#mixed_precision_linear_solvers" class=header-anchor >Mixed Precision Linear Solvers</a></h2> <p>The centerpiece of these developments is a comprehensive suite of mixed precision LU factorization methods that perform computations in Float32 precision while maintaining Float64 interfaces. This approach provides significant performance benefits for well-conditioned, memory-bandwidth limited problems.</p> <h3 id=new_mixed_precision_factorization_methods ><a href="#new_mixed_precision_factorization_methods" class=header-anchor >New Mixed Precision Factorization Methods</a></h3> <p><strong>Core Mixed Precision Solvers &#40;PR #746&#41;:</strong></p> <ul> <li><p><code>MKL32MixedLUFactorization</code>: CPU-based mixed precision using Intel MKL</p> <li><p><code>AppleAccelerate32MixedLUFactorization</code>: CPU-based mixed precision using Apple Accelerate</p> <li><p><code>CUDAOffload32MixedLUFactorization</code>: GPU-accelerated mixed precision for NVIDIA GPUs</p> <li><p><code>MetalOffload32MixedLUFactorization</code>: GPU-accelerated mixed precision for Apple Metal</p> </ul> <p><strong>Extended Mixed Precision Support &#40;PR #753&#41;:</strong></p> <ul> <li><p><code>OpenBLAS32MixedLUFactorization</code>: Mixed precision using OpenBLAS for cross-platform support</p> <li><p><code>RF32MixedLUFactorization</code>: Mixed precision using RecursiveFactorization.jl, optimized for small to medium matrices</p> </ul> <h3 id=how_mixed_precision_works ><a href="#how_mixed_precision_works" class=header-anchor >How Mixed Precision Works</a></h3> <p>All mixed precision solvers follow the same pattern:</p> <ol> <li><p><strong>Input</strong>: Accept Float64/ComplexF64 matrices and vectors</p> <li><p><strong>Conversion</strong>: Automatically convert to Float32/ComplexF32 for factorization</p> <li><p><strong>Computation</strong>: Perform LU factorization in reduced precision</p> <li><p><strong>Solution</strong>: Convert results back to original precision &#40;Float64/ComplexF64&#41;</p> </ol> <p>This approach reduces memory bandwidth requirements and can provide up to <strong>2x speedups</strong> for large, well-conditioned matrices while maintaining reasonable accuracy &#40;typically within 1e-5 relative error&#41;.</p> <h2 id=enhanced_blas_library_integration ><a href="#enhanced_blas_library_integration" class=header-anchor >Enhanced BLAS Library Integration</a></h2> <p>Alongside mixed precision capabilities, LinearSolve.jl has significantly expanded its direct BLAS library support, providing users with more high-performance options.</p> <h3 id=openblas_direct_integration_pr_745 ><a href="#openblas_direct_integration_pr_745" class=header-anchor >OpenBLAS Direct Integration &#40;PR #745&#41;</a></h3> <p><strong>OpenBLASLUFactorization</strong>: A new high-performance solver that directly calls OpenBLAS_jll routines without going through libblastrampoline:</p> <ul> <li><p><strong>Optimal performance</strong>: Direct calls to OpenBLAS for maximum efficiency</p> <li><p><strong>Cross-platform</strong>: Works on all platforms where OpenBLAS is available</p> <li><p><strong>Open source alternative</strong>: Provides MKL-like performance without proprietary dependencies</p> <li><p><strong>Pre-allocated workspace</strong>: Avoids allocations during solving</p> </ul> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> LinearSolve

A = rand(<span class=hljs-number >1000</span>, <span class=hljs-number >1000</span>)
b = rand(<span class=hljs-number >1000</span>)
prob = LinearProblem(A, b)

<span class=hljs-comment ># Direct OpenBLAS usage</span>
sol = solve(prob, OpenBLASLUFactorization())</code></pre> <h3 id=blis_integration_enhancement_pr_733 ><a href="#blis_integration_enhancement_pr_733" class=header-anchor >BLIS Integration Enhancement &#40;PR #733&#41;</a></h3> <p><strong>BLISLUFactorization</strong> has been integrated into the default algorithm selection system:</p> <ul> <li><p><strong>Automatic availability</strong>: Included in autotune benchmarking when BLIS is available</p> <li><p><strong>Smart selection</strong>: Can be automatically chosen as optimal solver for specific hardware</p> <li><p><strong>Fallback support</strong>: Graceful degradation when BLIS extension isn&#39;t loaded</p> </ul> <h2 id=enhanced_autotune_integration ><a href="#enhanced_autotune_integration" class=header-anchor >Enhanced Autotune Integration</a></h2> <p>The autotune system has been significantly enhanced to incorporate the new mixed precision and BLAS solvers with intelligent algorithm selection.</p> <h3 id=smart_algorithm_selection_pr_730_733 ><a href="#smart_algorithm_selection_pr_730_733" class=header-anchor >Smart Algorithm Selection &#40;PR #730, #733&#41;</a></h3> <p><strong>Availability Checking</strong>: The system now verifies that algorithms are actually usable before selecting them:</p> <ul> <li><p>Checks if required libraries &#40;MKL, OpenBLAS, BLIS&#41; are available</p> <li><p>Verifies GPU functionality for CUDA/Metal solvers</p> <li><p>Gracefully falls back to always-available methods when extensions aren&#39;t loaded</p> </ul> <p><strong>Dual Preference System</strong>: Autotune can now store both:</p> <ul> <li><p><code>best_algorithm_&#123;type&#125;_&#123;size&#125;</code>: Overall fastest algorithm &#40;may require extensions&#41;</p> <li><p><code>best_always_loaded_&#123;type&#125;_&#123;size&#125;</code>: Fastest among always-available methods</p> </ul> <p><strong>Intelligent Fallback Chain</strong>:</p> <ol> <li><p>Try best overall algorithm → if available, use it</p> <li><p>Fall back to best always-loaded → if available, use it </p> <li><p>Fall back to existing heuristics → guaranteed available</p> </ol> <p>This ensures optimal performance when extensions are available while maintaining robustness when they&#39;re not.</p> <h3 id=algorithm_integration ><a href="#algorithm_integration" class=header-anchor >Algorithm Integration</a></h3> <p>All new solvers are now integrated into the default algorithm selection:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> LinearSolve, LinearSolveAutotune

<span class=hljs-comment ># Benchmark includes all new mixed precision and BLAS methods</span>
benchmark_and_set_preferences!()

<span class=hljs-comment ># Default solver automatically uses best available algorithm</span>
A = rand(<span class=hljs-number >1000</span>, <span class=hljs-number >1000</span>)  
b = rand(<span class=hljs-number >1000</span>)
prob = LinearProblem(A, b)
sol = solve(prob)  <span class=hljs-comment ># May internally use OpenBLAS32MixedLUFactorization, BLISLUFactorization, etc.</span></code></pre> <h2 id=practical_examples ><a href="#practical_examples" class=header-anchor >Practical Examples</a></h2> <h3 id=direct_linear_system_solving ><a href="#direct_linear_system_solving" class=header-anchor >Direct Linear System Solving</a></h3> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> LinearSolve

<span class=hljs-comment ># Create a large, well-conditioned linear system</span>
A = rand(<span class=hljs-number >2000</span>, <span class=hljs-number >2000</span>) + <span class=hljs-number >5.0</span>I  <span class=hljs-comment ># Well-conditioned matrix</span>
b = rand(<span class=hljs-number >2000</span>)
prob = LinearProblem(A, b)

<span class=hljs-comment ># Mixed precision solvers - up to 2x speedup for memory-bandwidth limited problems</span>
sol_mkl = solve(prob, MKL32MixedLUFactorization())                    <span class=hljs-comment ># Intel MKL</span>
sol_apple = solve(prob, AppleAccelerate32MixedLUFactorization())      <span class=hljs-comment ># Apple Accelerate  </span>
sol_openblas = solve(prob, OpenBLAS32MixedLUFactorization())          <span class=hljs-comment ># OpenBLAS</span>
sol_rf = solve(prob, RF32MixedLUFactorization())                      <span class=hljs-comment ># RecursiveFactorization</span>

<span class=hljs-comment ># GPU acceleration (if available)</span>
sol_cuda = solve(prob, CUDAOffload32MixedLUFactorization())           <span class=hljs-comment ># NVIDIA</span>
sol_metal = solve(prob, MetalOffload32MixedLUFactorization())         <span class=hljs-comment ># Apple Silicon</span>

<span class=hljs-comment ># Direct BLAS integration (full precision)</span>
sol_openblas_direct = solve(prob, OpenBLASLUFactorization())          <span class=hljs-comment ># Direct OpenBLAS calls</span>
sol_blis = solve(prob, BLISLUFactorization())                         <span class=hljs-comment ># BLIS high-performance library</span></code></pre> <h3 id=mixed_precision_newton_methods_with_nonlinearsolvejl ><a href="#mixed_precision_newton_methods_with_nonlinearsolvejl" class=header-anchor >Mixed Precision Newton Methods with NonlinearSolve.jl</a></h3> <p>The mixed precision linear solvers integrate seamlessly with NonlinearSolve.jl to provide mixed precision Newton methods. This approach, as demonstrated by C.T. Kelley in &quot;Newton&#39;s Method in Mixed Precision&quot; &#40;SIAM Review, 2022&#41;, shows that using single precision for Newton step linear solves has minimal impact on nonlinear convergence rates while providing significant performance benefits.</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> NonlinearSolve, LinearSolve

<span class=hljs-comment ># Define a nonlinear system</span>
<span class=hljs-keyword >function</span> nonlinear_system!(F, u, p)
    F[<span class=hljs-number >1</span>] = u[<span class=hljs-number >1</span>]^<span class=hljs-number >2</span> + u[<span class=hljs-number >2</span>]^<span class=hljs-number >2</span> - <span class=hljs-number >1</span>
    F[<span class=hljs-number >2</span>] = u[<span class=hljs-number >1</span>] - u[<span class=hljs-number >2</span>]^<span class=hljs-number >3</span>
<span class=hljs-keyword >end</span>

u0 = [<span class=hljs-number >0.5</span>, <span class=hljs-number >0.5</span>]
prob = NonlinearProblem(nonlinear_system!, u0)

<span class=hljs-comment ># Use mixed precision linear solver for Newton steps</span>
<span class=hljs-comment ># The Jacobian factorization uses Float32, but maintains Float64 accuracy</span>
sol = solve(prob, NewtonRaphson(linsolve=MKL32MixedLUFactorization()))

<span class=hljs-comment ># For larger systems where GPU acceleration helps</span>
sol_gpu = solve(prob, NewtonRaphson(linsolve=CUDAOffload32MixedLUFactorization()))</code></pre> <p><strong>Key Benefits of Mixed Precision Newton Methods:</strong></p> <ul> <li><p><strong>Preserved convergence</strong>: Kelley&#39;s analysis shows that nonlinear convergence rates remain essentially unchanged when using single precision for the linear solve</p> <li><p><strong>Memory efficiency</strong>: Reduced memory bandwidth for Jacobian factorization</p> <li><p><strong>Scalability</strong>: Performance benefits increase with problem dimension</p> </ul> <h3 id=mixed_precision_in_ode_solving ><a href="#mixed_precision_in_ode_solving" class=header-anchor >Mixed Precision in ODE Solving</a></h3> <p>Mixed precision linear solvers are particularly effective in ODE solvers for stiff problems where Jacobian factorization dominates computational cost:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq, LinearSolve

<span class=hljs-comment ># Stiff ODE system</span>
<span class=hljs-keyword >function</span> stiff_ode!(du, u, p, t)
    k1, k2, k3 = p
    du[<span class=hljs-number >1</span>] = -k1*u[<span class=hljs-number >1</span>] + k2*u[<span class=hljs-number >2</span>]
    du[<span class=hljs-number >2</span>] = k1*u[<span class=hljs-number >1</span>] - k2*u[<span class=hljs-number >2</span>] - k3*u[<span class=hljs-number >2</span>]
    du[<span class=hljs-number >3</span>] = k3*u[<span class=hljs-number >2</span>]
<span class=hljs-keyword >end</span>

u0 = [<span class=hljs-number >1.0</span>, <span class=hljs-number >0.0</span>, <span class=hljs-number >0.0</span>]
prob = ODEProblem(stiff_ode!, u0, (<span class=hljs-number >0.0</span>, <span class=hljs-number >10.0</span>), [<span class=hljs-number >10.0</span>, <span class=hljs-number >5.0</span>, <span class=hljs-number >1.0</span>])

<span class=hljs-comment ># Use mixed precision for internal linear systems</span>
sol = solve(prob, Rodas5P(linsolve=MKL32MixedLUFactorization()))</code></pre> <h2 id=performance_characteristics ><a href="#performance_characteristics" class=header-anchor >Performance Characteristics</a></h2> <h3 id=mixed_precision_benefits ><a href="#mixed_precision_benefits" class=header-anchor >Mixed Precision Benefits</a></h3> <ul> <li><p><strong>Memory bandwidth limited problems</strong>: Up to 2x speedup</p> <li><p><strong>Large matrices</strong>: Significant memory usage reduction during factorization</p> <li><p><strong>Well-conditioned systems</strong>: Maintains accuracy within 1e-5 relative error</p> <li><p><strong>Complex number support</strong>: Works with both real and complex matrices</p> </ul> <h3 id=openblasblas_integration_benefits ><a href="#openblasblas_integration_benefits" class=header-anchor >OpenBLAS/BLAS Integration Benefits</a></h3> <ul> <li><p><strong>Cross-platform performance</strong>: High-performance computing without proprietary dependencies</p> <li><p><strong>Direct library calls</strong>: Bypasses intermediate layers for optimal efficiency </p> <li><p><strong>Automatic selection</strong>: Integrated into autotune benchmarking system</p> <li><p><strong>Fallback support</strong>: Graceful degradation when libraries aren&#39;t available</p> </ul> <h3 id=gpu_offloading_performance_thresholds ><a href="#gpu_offloading_performance_thresholds" class=header-anchor >GPU Offloading Performance Thresholds</a></h3> <p>Based on community-contributed LinearSolveAutotune benchmark data, GPU acceleration shows distinct performance characteristics:</p> <p><strong>Metal GPU Performance &#40;Apple Silicon&#41;:</strong></p> <ul> <li><p><strong>Below 500×500 matrices</strong>: CPU algorithms dominate performance</p> <li><p><strong>500×500 to 5000×5000</strong>: Competitive performance between CPU and GPU</p> <li><p><strong>Above 5000×5000</strong>: GPU delivers <strong>2-3x speedup</strong>, reaching over <strong>1 TFLOP</strong></p> </ul> <p><strong>CUDA GPU Performance:</strong></p> <ul> <li><p>Similar threshold behavior, with GPU acceleration becoming advantageous for larger matrices</p> <li><p>Mixed precision &#40;32-bit&#41; GPU solvers often outperform 64-bit CPU LU factorization at lower matrix size thresholds than full precision GPU solvers</p> </ul> <h2 id=gpu_offloading_for_large_stiff_ode_systems ><a href="#gpu_offloading_for_large_stiff_ode_systems" class=header-anchor >GPU Offloading for Large Stiff ODE Systems</a></h2> <p>For large stiff ODE systems where Jacobian factorization dominates computational cost, GPU offloading with mixed precision can provide substantial performance improvements:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> OrdinaryDiffEq, LinearSolve

<span class=hljs-comment ># Large stiff system (e.g., discretized PDE)</span>
<span class=hljs-keyword >function</span> large_stiff_system!(du, u, p, t)
    <span class=hljs-comment ># Example: 2D heat equation discretization</span>
    n = <span class=hljs-built_in >Int</span>(sqrt(length(u)))  <span class=hljs-comment ># Assume square grid</span>
    Δx = <span class=hljs-number >1.0</span> / (n - <span class=hljs-number >1</span>)
    α = p[<span class=hljs-number >1</span>]
    
    <span class=hljs-comment ># Interior points with finite difference</span>
    <span class=hljs-keyword >for</span> i <span class=hljs-keyword >in</span> <span class=hljs-number >2</span>:n-<span class=hljs-number >1</span>
        <span class=hljs-keyword >for</span> j <span class=hljs-keyword >in</span> <span class=hljs-number >2</span>:n-<span class=hljs-number >1</span>
            idx = (i-<span class=hljs-number >1</span>)*n + j
            du[idx] = α * ((u[idx-<span class=hljs-number >1</span>] - <span class=hljs-number >2</span>u[idx] + u[idx+<span class=hljs-number >1</span>]) / Δx^<span class=hljs-number >2</span> +
                          (u[idx-n] - <span class=hljs-number >2</span>u[idx] + u[idx+n]) / Δx^<span class=hljs-number >2</span>)
        <span class=hljs-keyword >end</span>
    <span class=hljs-keyword >end</span>
    <span class=hljs-comment ># Boundary conditions (simplified)</span>
    du[<span class=hljs-number >1</span>:n] .= <span class=hljs-number >0.0</span>    <span class=hljs-comment ># Bottom</span>
    du[<span class=hljs-keyword >end</span>-n+<span class=hljs-number >1</span>:<span class=hljs-keyword >end</span>] .= <span class=hljs-number >0.0</span>  <span class=hljs-comment ># Top</span>
<span class=hljs-keyword >end</span>

<span class=hljs-comment ># Large system (10000 unknowns)</span>
n = <span class=hljs-number >100</span>
u0 = rand(n*n)
prob = ODEProblem(large_stiff_system!, u0, (<span class=hljs-number >0.0</span>, <span class=hljs-number >1.0</span>), [<span class=hljs-number >0.01</span>])

<span class=hljs-comment ># For systems &gt; 5000×5000 Jacobians, GPU mixed precision excels</span>
sol_metal = solve(prob, Rodas5P(linsolve=MetalOffload32MixedLUFactorization()))
sol_cuda = solve(prob, Rodas5P(linsolve=CUDAOffload32MixedLUFactorization()))

<span class=hljs-comment ># CPU fallback for smaller systems or when GPU unavailable  </span>
sol_cpu = solve(prob, Rodas5P(linsolve=MKL32MixedLUFactorization()))</code></pre> <p><strong>Performance Guidelines from AutoTune Data:</strong></p> <ul> <li><p><strong>Small systems &#40;&lt; 500×500 Jacobians&#41;</strong>: Use RecursiveFactorization mixed precision</p> <li><p><strong>Medium systems &#40;500×500 to 5000×5000&#41;</strong>: Platform-specific BLAS libraries &#40;MKL, Apple Accelerate&#41; </p> <li><p><strong>Large systems &#40;&gt; 5000×5000&#41;</strong>: GPU offloading with mixed precision provides optimal performance</p> </ul> <h3 id=note_from_autotune_performance_results_julias_performance_leadership_in_small_matrix_factorization ><a href="#note_from_autotune_performance_results_julias_performance_leadership_in_small_matrix_factorization" class=header-anchor >Note from AutoTune Performance Results: Julia&#39;s Performance Leadership in Small Matrix Factorization</a></h3> <p>A remarkable finding from the LinearSolveAutotune results across hundreds of different CPUs is that <strong>RecursiveFactorization.jl consistently outperforms all BLAS implementations for small matrices &#40;256×256 and smaller&#41;</strong>. This pure Julia implementation beats optimized libraries like Intel MKL, OpenBLAS, and vendor-specific implementations, demonstrating that the Julia ecosystem is well ahead of traditional BLAS tools in this domain.</p> <p>This performance advantage stems from:</p> <ul> <li><p><strong>Reduced call overhead</strong>: Pure Julia avoids FFI costs for small matrices</p> <li><p><strong>Optimized blocking strategies</strong>: RecursiveFactorization uses cache-optimal algorithms</p> <li><p><strong>Julia compiler optimizations</strong>: LLVM can optimize the entire computation path</p> <li><p><strong>Elimination of memory layout conversions</strong>: Direct operation on Julia arrays</p> </ul> <p>This trend reflects the broader evolution of scientific computing, where high-level languages with sophisticated compilers can outperform traditional low-level libraries in specific domains. The SciML ecosystem continues to push these boundaries, developing native Julia algorithms that leverage the language&#39;s performance characteristics while maintaining the productivity benefits of high-level programming.</p> <h2 id=looking_forward ><a href="#looking_forward" class=header-anchor >Looking Forward</a></h2> <p>The introduction of mixed precision linear solvers and enhanced BLAS integration represents a significant expansion of LinearSolve.jl&#39;s capabilities:</p> <ul> <li><p><strong>Performance</strong>: New algorithmic approaches for memory-bandwidth limited problems</p> <li><p><strong>Hardware support</strong>: Broader platform coverage with OpenBLAS and BLIS integration</p> <li><p><strong>Usability</strong>: Intelligent algorithm selection reduces user burden</p> <li><p><strong>Ecosystem integration</strong>: Seamless integration with ODE solvers and other SciML packages</p> </ul> <p>These developments provide both immediate performance benefits and establish a foundation for future mixed precision innovations across the SciML ecosystem.</p> <hr /> <p><em>For detailed technical information and examples, visit the <a href="https://docs.sciml.ai/">SciML documentation</a> and join discussions on <a href="https://discourse.julialang.org/c/domain/models/21">Julia Discourse</a>.</em></p> <!-- Footer--> <footer class="footer bg-light"> <div class=container > <div class=row > <div class="col-lg-6 h-100 text-center text-lg-start my-auto"> <ul class="list-inline mb-2"> <li class=list-inline-item ><a href="/community">Contact</a> <!-- <li class=list-inline-item ><a href="#!">Terms of Use</a> <li class=list-inline-item ><a href="#!">Privacy Policy</a> --> </ul> <p class="text-muted small mb-4 mb-lg-0">Website powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia</a> programming language. &copy; SciML 2022. All Rights Reserved.</p> <p class="text-muted small mb-4 mb-lg-0">Edit on <a href="https://github.com/SciML/sciml.ai">GitHub</a></p> </div> <div class="col-lg-6 h-100 text-center text-lg-end my-auto"> <ul class="list-inline mb-0"> <li class="list-inline-item me-4"> <a href="https://github.com/SciML"><i class="bi-github fs-3"></i></a> <li class="list-inline-item me-4"> <a href="https://twitter.com/SciML_Org"><i class="bi-twitter fs-3"></i></a> <li class=list-inline-item > <a href="https://www.linkedin.com/company/the-julia-language"><i class="bi-linkedin fs-3"></i></a> </ul> </div> </div> </div> </footer> </div>