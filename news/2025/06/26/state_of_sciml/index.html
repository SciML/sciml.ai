<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta property="og:title" content="SciML: Open Source Software for Scientific Machine Learning"> <meta property="og:description" content="Open Source Software for Scientific Machine Learning"> <meta property="og:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta property="og:url" content="https://sciml.ai"> <meta name="twitter:title" content=SciML > <meta name="twitter:description" content="Open Source Software for Scientific Machine Learning"> <meta name="twitter:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <!-- Favicon--> <link rel=icon  type="image/x-icon" href="assets/favicon.png" /> <!-- Bootstrap icons--> <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel=stylesheet  type="text/css" /> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css"> <!-- Google fonts--> <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet  type="text/css" /> <!-- Core theme CSS (includes Bootstrap)--> <link href="./css/styles.css" rel=stylesheet  /> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/styles.css"> <link rel=stylesheet  href="/css/hypertext.css"> <link rel=icon  href="/assets/favicon.png"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-2"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-90474609-2'); </script> <title>State of the SciML Open Source Software Ecosystem, 2025</title> <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.3/dist/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin=anonymous ></script> <div class=container-fluid  id=top-alert > <div class="alert alert-dark alert-dismissible mb-0" role=alert > <p class=text-center > <a href="https://youtu.be/yHiyJQdWBY8">Check out the latest talk: "The Continuing Advancements of Scientific Machine Learning (SciML)"</a> </p> <!-- <button type=button  class=close  data-dismiss=alert  aria-label=Close > <span aria-hidden=true >&times;</span> --> </button> </div> </div> <header> <nav class="navbar navbar-expand-lg navbar-light"> <a class=navbar-brand  href="/">Home</a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class="nav-item active"> <a class=nav-link  href="https://docs.sciml.ai/">Documentation</a> <li class=nav-item > <a class=nav-link  href="/news/">News</a> <li class=nav-item > <a class=nav-link  href="/roadmap/">Roadmap</a> <li class=nav-item > <a class=nav-link  href="/citing/">Citing</a> <li class=nav-item > <a class=nav-link  href="/showcase/">Showcase</a> <li class=nav-item > <a class=nav-link  href="https://benchmarks.sciml.ai/">Benchmarks</a> <li class=nav-item > <a class=nav-link  href="https://github.com/SciML/">GitHub</a> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id=navbarDropdownMenuLink  role=button  data-toggle=dropdown  aria-haspopup=true  aria-expanded=false > Community </a> <div class=dropdown-menu  aria-labelledby=navbarDropdownMenuLink > <a class=dropdown-item  href="/community/">Community Home</a> <a class=dropdown-item  href="/governance/">Governance</a> <a class=dropdown-item  href="/challenge/">Challenges</a> <a class=dropdown-item  href="/dev/">Developer Programs</a> </div> <li class=nav-item > <a class=nav-link  href="https://juliahub.com/company/contact-us-sciml"> Commercial Support </a> <li class=nav-item > <a class=nav-link  href="https://numfocus.org/donate-to-sciml"><i class="bi bi-heart"></i> Donate</a> </ul> </div> </nav> </header> <div class=franklin-content ><h1 id=state_of_the_sciml_open_source_software_ecosystem_2025 ><a href="#state_of_the_sciml_open_source_software_ecosystem_2025" class=header-anchor >State of the SciML Open Source Software Ecosystem, 2025</a></h1> <p><em>This blog post is a comprehensive summary of the presentation <a href="https://www.youtube.com/watch?v&#61;SZZ0lT8DVRo">&quot;State of SciML&quot;</a> given at JuliaCon 2024. The original slides can be found at <a href="https://figshare.com/articles/presentation/State_of_SciML_JuliaCon2024_/26299429">https://figshare.com/articles/presentation/State<em>of</em>SciML<em>JuliaCon2024</em>/26299429</a>.</em></p> <p>As we enter 2025, the SciML Open Source organization has evolved into the world&#39;s most comprehensive ecosystem for scientific machine learning and differential equation solving. This technical report provides a detailed overview of our current state, recent achievements, and roadmap for the future.</p> <h2 id=what_is_the_sciml_open_source_organization ><a href="#what_is_the_sciml_open_source_organization" class=header-anchor >What is the SciML Open Source Organization?</a></h2> <p>The SciML Open Source organization is a non-profit organization, part of the NumFOCUS affiliate libraries, which builds and supports the development of packages in Julia, Python, and R for scientific simulation and scientific machine learning.</p> <h3 id=current_scale_and_impact ><a href="#current_scale_and_impact" class=header-anchor >Current Scale and Impact</a></h3> <ul> <li><p><strong>Over 100 GitHub repositories</strong>, many containing 10&#43; packages themselves</p> <li><p><strong>Totals ~200 Julia packages</strong>, all MIT open source licensed</p> <li><p><strong>20,000&#43; GitHub stars</strong> across the package ecosystem</p> <li><p><strong>~100&#43; unique contributors monthly</strong> &#40;variable month to month&#41;</p> <li><p><strong>~20 core maintainers</strong> owning specific aspects of the project</p> <li><p><strong>~5-10 summer students and other trainees per year</strong></p> </ul> <h3 id=community_structure ><a href="#community_structure" class=header-anchor >Community Structure</a></h3> <p>Our community includes many maintainers from the ~20 folks associated with the MIT Julia Lab &#40;and alumni&#41;. Multiple companies have spun off &#40;PumasAI, JuliaHub, Neuroblox, etc.&#41; which &quot;house&quot; maintainers with full-time jobs related to their contributions. We submit ~10 grant applications per year related to expanding the SciML organization, usually to MIT though sometimes facilitated through contributor commercial entities or to the non-profit itself.</p> <p><strong>Scope Note:</strong> This report covers approximately ¼ of the organization&#39;s work due to the breadth of our activities. We hope to start a recurring series that better tracks these updates.</p> <h2 id=sciml_ecosystem_architecture ><a href="#sciml_ecosystem_architecture" class=header-anchor >SciML Ecosystem Architecture</a></h2> <p>The SciML ecosystem is built around composable interfaces with comprehensive documentation for differentiable simulation. Our organization spans multiple interconnected areas:</p> <p><strong>Domain-Specific Modeling Tools:</strong> QuantumCumulants, OrbitalTrajectories, Catalyst, ModelingToolkitStandardLibrary, ModelingToolkit, Symbolics</p> <p><strong>Analysis Capabilities:</strong> GlobalSensitivity, EasyModelAnalysis, SciMLExpectation, StructuralIdentifiability, BifurcationKit</p> <p><strong>Machine Learning Integration:</strong> ReservoirComputing, Surrogates, QuasiMonteCarlo for sampling methods</p> <p><strong>Core Equation Solving:</strong> NonlinearSolve, Integrals, LinearSolve, StochasticDelayDiffEq, JumpProcesses, DifferentialEquations, DelayDiffEq, StochasticDiffEq, OrdinaryDiffEq</p> <p><strong>Inverse Problems and Simulation:</strong> DiffEqBayes, SciMLSensitivity, DiffEqParamEstim, DiffEqGPU, Sundials, MethodsofLines, DataDrivenDiffEq, NeuralPDE, DiffEqFlux</p> <p><strong>Third-Party PDE Solvers Integration:</strong> Trixi, Gridap, ApproxFun, VoronoiFVM</p> <p><strong>Automatic Differentiation:</strong> Enzyme, ReverseDiff, ForwardDiff</p> <p><strong>External Ecosystem Connections:</strong> Evolutionary, JuMP, BlackBoxOptim, MathOptInterface, Flux, Lux</p> <p><em>Comprehensive ecosystem diagram created by maintainers Torkel Lohman and Vaibhav Dixit</em></p> <h2 id=common_interface_for_julia_equation_solvers ><a href="#common_interface_for_julia_equation_solvers" class=header-anchor >Common Interface for Julia Equation Solvers</a></h2> <p>SciML provides a unified interface across all major equation types:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Linear Systems</span>
LinearSolve.jl          <span class=hljs-comment ># A(p)x = b</span>

<span class=hljs-comment ># Nonlinear Systems  </span>
NonlinearSolve.jl       <span class=hljs-comment ># f(u,p) = 0</span>

<span class=hljs-comment ># Differential Equations</span>
DifferentialEquations.jl <span class=hljs-comment ># u&#x27; = f(u,p,t)</span>

<span class=hljs-comment ># Integration</span>
Integrals.jl            <span class=hljs-comment ># ∫[lb to ub] f(t,p)dt</span>

<span class=hljs-comment ># Optimization</span>
Optimization.jl         <span class=hljs-comment ># minimize f(u,p)</span>
                        <span class=hljs-comment ># subject to g(u,p) ≤ 0, h(u,p) = 0</span></code></pre> <h2 id=maturity_level_classification_system ><a href="#maturity_level_classification_system" class=header-anchor >Maturity Level Classification System</a></h2> <p>SciML practices open science, with all development and research done in the open. While some libraries are nearly 10 years old and battle-hardened by hundreds of thousands of researchers, others are brand-new research projects. We&#39;ve established a maturity classification:</p> <p><strong>High Maturity</strong> - Battle-hardened libraries with well-defined interfaces, comprehensive error handling, extensive real-world testing, full documentation, and training materials.</p> <p><strong>Medium Maturity</strong> - Recently mature libraries with full feature sets, robust numerical testing against classical implementations, and well-documented core methods, but potentially lacking refined error messages or advanced documentation.</p> <p><strong>Low Maturity</strong> - Libraries in heavy development with major version updates planned. Core methods documented with tutorials, but extensive features may be under-documented with known edge cases.</p> <p><strong>Research</strong> - Effectively unreleased libraries under development for early adopters and contributors.</p> <h2 id=organization_timeline_from_2015_to_2025 ><a href="#organization_timeline_from_2015_to_2025" class=header-anchor >Organization Timeline: From 2015 to 2025</a></h2> <h3 id=the_early_years_2015-2017 ><a href="#the_early_years_2015-2017" class=header-anchor >The Early Years &#40;2015-2017&#41;</a></h3> <p><strong>2015</strong>: First versions of differential equation solvers written as MATLAB&#43;C code, targeting Intel Xeon Phi accelerators</p> <p><strong>2016</strong>: First translation into Julia, established as the JuliaDiffEq organization with DifferentialEquations.jl</p> <p><strong>2017</strong>: First workshop at JuliaCon Berkeley, start of Pumas. Catalyst.jl &#40;then DiffEqBiological.jl&#41;, ParameterizedFunctions.jl &#40;early ModelingToolkit&#41; created. SciMLBenchmarks established.</p> <h3 id=growth_and_academic_integration_2018-2019 ><a href="#growth_and_academic_integration_2018-2019" class=header-anchor >Growth and Academic Integration &#40;2018-2019&#41;</a></h3> <p><strong>2018</strong>: Analysis tools launched &#40;global sensitivity analysis, SciMLExpectations, uncertainty quantification&#41;. Reverse-mode automatic differentiation and adjoint method integration. Tools spun out into widely-used Julia packages &#40;RecursiveArrayTools.jl, FiniteDiff.jl, PreallocationTools.jl&#41;</p> <p><strong>2019</strong>: Chris joins MIT Mathematics, growing the MIT Julia Lab. Universal differential equations and differentiable programming popularized. DiffEqFlux.jl released. PumasAI founded. ChainRules.jl integrated throughout SciML.</p> <h3 id=organizational_expansion_2020-2021 ><a href="#organizational_expansion_2020-2021" class=header-anchor >Organizational Expansion &#40;2020-2021&#41;</a></h3> <p><strong>2020</strong>: Renamed to SciML, extending beyond differential equations. SciML Book released from MIT 18.337. Non-differential equation solver packages started. Development practices refined through PumasAI regulatory compliance work.</p> <p><strong>2021</strong>: Chris becomes VP at JuliaHub, JuliaSim project starts. ModelingToolkit achieves hierarchical acausal model capability. Symbolics.jl spun out as full CAS. Early Enzyme integration.</p> <h3 id=maturation_and_performance_2022-2023 ><a href="#maturation_and_performance_2022-2023" class=header-anchor >Maturation and Performance &#40;2022-2023&#41;</a></h3> <p><strong>2022</strong>: Organization reworked for static compilability, reducing first-time-to-solve from 30 seconds to 0.1 seconds. Documentation and error messages improved. Lux.jl spun out. Interface definitions standardized.</p> <p><strong>2023</strong>: Global SciML documentation released. Core solver packages reach DifferentialEquations.jl completeness level. PDE tooling reaches relative maturity on structured grids. Focus restructured towards Lux.jl.</p> <h3 id=current_state_2024-2025 ><a href="#current_state_2024-2025" class=header-anchor >Current State &#40;2024-2025&#41;</a></h3> <p><strong>2024</strong>: SciML Small Grants program launched. Kernel-based GPU solvers released. EnzymeRules integration established. JuliaSim GUI released. Global allocation-free and anti-dynamism checking for AOT compilation.</p> <p><strong>2025 Planned</strong>: Symbolic-Numeric PDE tooling for unstructured grids. Better Trixi.jl/Ferrite.jl integration. Full Makie integration. Major Optimization.jl improvements including automated convex optimization. Enhanced embedded device and WebAssembly support.</p> <h2 id=numerical_solvers_the_foundation ><a href="#numerical_solvers_the_foundation" class=header-anchor >Numerical Solvers: The Foundation</a></h2> <h3 id=differentialequationsjl_battle-tested_excellence ><a href="#differentialequationsjl_battle-tested_excellence" class=header-anchor >DifferentialEquations.jl: Battle-Tested Excellence</a></h3> <p><strong>Current Maturity Level: High</strong></p> <p>DifferentialEquations.jl remains our cornerstone, providing unified interfaces for ODEs, SDEs, DDEs, DAEs, and hybrid systems.</p> <h4 id=key_milestones ><a href="#key_milestones" class=header-anchor >Key Milestones</a></h4> <p><strong>2016</strong>: Initial release with major features &#40;event handling, type-generic, high-performance&#41;. High-order adaptive SDE algorithms.</p> <p><strong>2017</strong>: Split into separate solver packages &#40;OrdinaryDiffEq.jl, StochasticDiffEq.jl, DelayDiffEq.jl, Sundials.jl&#41;. Core Julia solvers win non-stiff ODE benchmarks using new Tsit and Vern methods &#43; PI adaptivity.</p> <p><strong>2018-2019</strong>: Large-scale stiff ODE functionality. SDIRK, IMEX, Radau, exponential integrators. AutoSwitch stiffness detection. Automatic sparsity support for large-scale PDEs.</p> <p><strong>2021-2022</strong>: FBDF and QNDF competitive with Sundials. Parallel extrapolation methods achieve top performance on &lt;200 equation stiff ODEs. DAE initialization with specialized event handling.</p> <p><strong>2023</strong>: Kernel GPU methods outperform JAX and PyTorch by 20x-100x for GPU ensemble parallelization.</p> <h4 id=2024_achievements ><a href="#2024_achievements" class=header-anchor >2024 Achievements</a></h4> <ul> <li><p>Major performance improvements for implicit methods on large equations</p> <li><p>NonlinearSolve.jl integration into core solvers</p> <li><p>OrdinaryDiffEq.jl restructuring for improved compile times</p> <li><p>Default ODE solver reworked for full type-stability with maintained dynamism</p> </ul> <h4 id=2025_roadmap ><a href="#2025_roadmap" class=header-anchor >2025 Roadmap</a></h4> <ul> <li><p>New solvers research targeting 2x-6x non-stiff equation performance improvements</p> <li><p>NonlinearSolve.jl becomes default nonlinear solving method</p> <li><p>OrdinaryDiffEq v7 release with improved AOT compilation</p> <li><p>AllocCheck and DispatchDoctor/JET enforcement for runtime-free compilation</p> <li><p>DifferentiationInterface.jl integration with forward-mode Enzyme as default</p> </ul> <h3 id=ordinarydiffeqjl_v7_addressing_scale_challenges ><a href="#ordinarydiffeqjl_v7_addressing_scale_challenges" class=header-anchor >OrdinaryDiffEq.jl v7: Addressing Scale Challenges</a></h3> <p>A critical issue: OrdinaryDiffEq.jl has grown to ~10MB of code. Our solution:</p> <p><strong>Modularization Strategy:</strong></p> <ul> <li><p>Split into solver sub-libraries not instantiated by default</p> <li><p>Example: <code>ImplicitEulerBarycentricExtrapolation</code> requires <code>using OrdinaryDiffEqImplicitExtrapolation</code> &#40;<strong>only breaking change</strong>&#41;</p> <li><p>Solver-specific dependencies for faster installation/loading</p> <li><p>Precompilation on specific sub-libraries</p> <li><p>Common algorithms &#40;Tsit5, VernX&#41; get dedicated sub-libraries</p> <li><p>Restructure: OrdinaryDiffEqCore.jl, OrdinaryDiffEqImplicitCore.jl, with OrdinaryDiffEq.jl as default collection</p> </ul> <h3 id=other_core_solver_packages ><a href="#other_core_solver_packages" class=header-anchor >Other Core Solver Packages</a></h3> <p><strong>Current Maturity Level: Transitioning Medium-High</strong></p> <p>Our unified solver ecosystem includes LinearSolve.jl, NonlinearSolve.jl, Integrals.jl, and Optimization.jl.</p> <h4 id=historical_development ><a href="#historical_development" class=header-anchor >Historical Development</a></h4> <p><strong>2016-2017</strong>: Started with external dependencies &#40;Roots.jl, NLsolve.jl&#41;, then developed bespoke internal systems.</p> <p><strong>2019</strong>: RecursiveFactorization.jl becomes fastest pure Julia LU factorization for matrices &lt;200x200. Suitesparse KLU integration for sparse ODEs.</p> <p><strong>2020-2021</strong>: Individual packages established &#40;Integrals.jl as Quadrature.jl, NonlinearSolve.jl, LinearSolve.jl&#41;. Organization scope expanded.</p> <p><strong>2022-2023</strong>: Packages matured with ChainRules integration for specialized automatic differentiation.</p> <h4 id=2024_progress ><a href="#2024_progress" class=header-anchor >2024 Progress</a></h4> <ul> <li><p>NonlinearSolve.jl gained iteration functionality for DifferentialEquations.jl backend</p> <li><p>EnzymeRules interfaces added &#40;Integrals.jl pending&#41;</p> <li><p>SimpleX solvers made GPU kernel compilation compatible</p> </ul> <h4 id=2025_goals ><a href="#2025_goals" class=header-anchor >2025 Goals</a></h4> <ul> <li><p>DifferentiationInterface.jl integration for improved sparsity handling</p> <li><p>ParU integration into LinearSolve.jl</p> <li><p>Higher-order nonlinear solvers research</p> <li><p>Static compilation guarantees across all solvers</p> </ul> <h3 id=optimizationjl_advancing_toward_maturity ><a href="#optimizationjl_advancing_toward_maturity" class=header-anchor >Optimization.jl: Advancing Toward Maturity</a></h3> <p><strong>Current Maturity Level: Low → Medium &#40;2025 target&#41;</strong></p> <h4 id=evolution_timeline ><a href="#evolution_timeline" class=header-anchor >Evolution Timeline</a></h4> <p><strong>2017-2020</strong>: From DiffEqParamEstim.jl targeting multiple packages, through DiffEqFlux.jl polyalgorithm, to GalacticOptim.jl unified interface.</p> <p><strong>2021-2023</strong>: Renamed to Optimization.jl. MathOptInterface.jl bindings for JuMP ecosystem. IPOPT support. ModelingToolkit integration. NonlinearSolve.jl integration for least squares. PRIMA derivative-free methods. Sparse Hessian and Enzyme support.</p> <h4 id=2024_achievements__2 ><a href="#2024_achievements__2" class=header-anchor >2024 Achievements</a></h4> <ul> <li><p>Automated structure analysis &#40;DCP and DGCP&#41; for convex optimization</p> <li><p>ModelingToolkit tearing optimizations for symbolic-numeric simplifications</p> <li><p>Augmented-Lagrangian with LBFGS-B implementation &#40;no solver subpackages required&#41;</p> <li><p>Interface stabilization with improved testing and error messages</p> </ul> <h4 id=2025_medium_maturity_goals ><a href="#2025_medium_maturity_goals" class=header-anchor >2025 Medium Maturity Goals</a></h4> <ul> <li><p>Complete convex optimization interface leveraging MOI&#39;s set interface</p> <li><p>Multi-objective problems interface</p> <li><p>JuliaSimCompiler targeting for large-scale symbolic code generation</p> <li><p>Enhanced ML workflow support with minibatching and GPU compatibility</p> <li><p>SciML-native nonlinear optimizers based on LineSearch.jl and NonlinearSolve.jl</p> <li><p>ChainRules and EnzymeRules interfaces for differentiable optimization</p> </ul> <h2 id=symbolic_modeling_interfaces ><a href="#symbolic_modeling_interfaces" class=header-anchor >Symbolic Modeling Interfaces</a></h2> <h3 id=modelingtoolkitjl_ecosystem_symbolic-numeric_integration ><a href="#modelingtoolkitjl_ecosystem_symbolic-numeric_integration" class=header-anchor >ModelingToolkit.jl Ecosystem: Symbolic-Numeric Integration</a></h3> <p><strong>Current Maturity Level: Low-Medium</strong></p> <p>ModelingToolkit.jl serves as our acausal modeling framework for automatically parallelized scientific machine learning, featuring a computer algebra system for physics-informed machine learning and automated differential equation transformations.</p> <h4 id=development_history ><a href="#development_history" class=header-anchor >Development History</a></h4> <p><strong>2016-2018</strong>: From ParameterizedFunctions.jl with SymEngine backend, through DiffEqBiological with macro-based DSL, to first ModelingToolkit.jl versions trying multiple symbolic backends before building internal engine.</p> <p><strong>2019-2020</strong>: Solved staged-compilation issues with GeneralizedGenerated.jl/RuntimeGeneratedFunctions.jl. PDESystem added for MethodOfLines and NeuralPDE.jl. Became backend for Catalyst, Pumas, and other tools.</p> <p><strong>2021</strong>: Acausal model support with Pantelides algorithm for DAE index reduction, tearing, alias elimination. Chosen as JuliaSim backend. Symbolics.jl spun out. StructuralIdentifiability.jl created. SBML, CellML, BioNetGen import added.</p> <p><strong>2022</strong>: NonlinearSystem, OptimizationSystem, SDESystem support. General System type for automatic mathematical form detection. Partial state selection algorithms. ModelOrderReduction.jl prototypes. EasyModelAnalysis.jl created.</p> <p><strong>2023</strong>: JuliaSimCompiler released with loop regeneration optimizations. Direct LLVM and C compilation for embedded devices. Catalyst reaches high maturity.</p> <h4 id=2024_major_release_-_modelingtoolkit_v9 ><a href="#2024_major_release_-_modelingtoolkit_v9" class=header-anchor >2024 Major Release - ModelingToolkit v9</a></h4> <ul> <li><p>SymbolicIndexingInterface established and deployed organization-wide</p> <li><p>Initialization interface and clocking system for hybrid systems</p> <li><p>Neural network integration via array functions in symbolic interfaces</p> <li><p>JuliaSimCompiler optimizations for multibody systems</p> <li><p>BaseModelica import capabilities</p> </ul> <h4 id=2025_technical_priorities ><a href="#2025_technical_priorities" class=header-anchor >2025 Technical Priorities</a></h4> <ul> <li><p>ModelOrderReduction.jl completion for symbolic-numeric PDE optimizations</p> <li><p>Unstructured grid PDE interface support with NeuralPDE.jl integration</p> <li><p>Ferrite.jl and Trixi.jl integration as PDE auto-discretizers</p> <li><p>Major JuliaSimCompiler performance improvements</p> <li><p>Advanced symbolic-numeric projects for enhanced performance</p> </ul> <h3 id=symbolicsjl_the_cas_foundation ><a href="#symbolicsjl_the_cas_foundation" class=header-anchor >Symbolics.jl: The CAS Foundation</a></h3> <p><strong>Current Maturity Level: Low</strong></p> <h4 id=development_timeline ><a href="#development_timeline" class=header-anchor >Development Timeline</a></h4> <p><strong>2020</strong>: Bespoke symbolic tooling developed in ModelingToolkit.jl after SymEngine, SymPy, and REDUCE couldn&#39;t meet SciML&#39;s performance and feature requirements.</p> <p><strong>2021</strong>: Symbolics.jl spun out as independent library. Rule-based simplifiers with sophisticated type system for automating commutative/distributive optimizations. Metadata system for lightweight symbolic variables.</p> <p><strong>2022</strong>: Unityper sum type representation rewrite reduced dynamism and improved performance. SymbolicNumericIntegration.jl created. Symbolic array machinery developed.</p> <p><strong>2023</strong>: Array function registration. Groebner.jl extensions and other CAS tooling connections for polynomial reductions. SymPy translation extensions.</p> <h4 id=2024_performance_revolution ><a href="#2024_performance_revolution" class=header-anchor >2024 Performance Revolution</a></h4> <ul> <li><p>Major performance improvements via reduced dynamism</p> <li><p>Closed sum type representation optimization</p> <li><p>Eliminated default sorting overhead </p> <li><p>Hash consing implementation</p> </ul> <h4 id=20252026_medium_maturity_goals ><a href="#20252026_medium_maturity_goals" class=header-anchor >2025/2026 Medium Maturity Goals</a></h4> <ul> <li><p>Symbolic tensor calculus implementation</p> <li><p>Symbolic equation solvers &#40;nonlinear equations, integrals, basic ODEs&#41;</p> <li><p>Enhanced numerical method integration</p> </ul> <h2 id=machine_learning_integration ><a href="#machine_learning_integration" class=header-anchor >Machine Learning Integration</a></h2> <h3 id=sciml_differentiable_programming ><a href="#sciml_differentiable_programming" class=header-anchor >SciML Differentiable Programming</a></h3> <p><strong>Current Maturity Level: Medium</strong></p> <p>We&#39;ve established ourselves as the leader in differentiable programming for scientific computing, with comprehensive adjoint method support and automatic differentiation.</p> <h4 id=development_history__2 ><a href="#development_history__2" class=header-anchor >Development History</a></h4> <p><strong>2018-2019</strong>: DiffEqSensitivity.jl established with forward/adjoint differentiation for ODEs. Tracker.jl and ReverseDiff.jl integration for vector-Jacobian product optimizations.</p> <p><strong>2020</strong>: DiffEqFlux.jl released with concrete_solve interface automating adjoint method use when solvers detected in loss functions. Dispatching adjoint system for multiple optimization types created.</p> <p><strong>2021</strong>: DiffEqFlux.jl&#39;s bespoke interface removed as ChainRules support enabled &quot;normal&quot; ODE solver code automatic handling. Event handling and shadowing methods for chaotic systems. Early Enzyme integration and ReverseDiff tape-compilation for JIT optimizations.</p> <p><strong>2022</strong>: Renamed to SciMLSensitivity.jl with NonlinearProblem, SteadyStateProblem differentiation support. Documentation overhaul. Enzyme as behind-the-scenes default vjp choice.</p> <p><strong>2023</strong>: GaussAdjoint created for improved memory performance. Enhanced error handling and interface checking.</p> <h4 id=2024_milestone_achievement ><a href="#2024_milestone_achievement" class=header-anchor >2024 Milestone Achievement</a></h4> <ul> <li><p>Published comprehensive review article: &quot;Differentiable Programming for Differential Equations&quot;</p> <li><p>EnzymeRules integration enabling automatic Enzyme to SciMLSensitivity.jl routing</p> <li><p>Enhanced performance and memory optimization</p> </ul> <p>The review article provides comprehensive adjoint method analysis with detailed performance comparisons across ReverseDiffAdjoint, TrackerAdjoint, Forward sensitivity equations, Backsolve adjoint, Interpolating adjoint, Quadrature adjoint, and Gauss adjoint methods, comparing stability, performance complexity, and memory usage.</p> <h4 id=2025_priorities ><a href="#2025_priorities" class=header-anchor >2025 Priorities</a></h4> <ul> <li><p>Documentation overhaul favoring Enzyme and DifferentiationInterface</p> <li><p>Asynchronous checkpointing for large-scale problems</p> <li><p>DAE and SDE adjoint performance improvements</p> </ul> <h3 id=deep_learning_libraries_the_luxjl_revolution ><a href="#deep_learning_libraries_the_luxjl_revolution" class=header-anchor >Deep Learning Libraries: The Lux.jl Revolution</a></h3> <p><strong>Current Maturity Level: Medium-High</strong></p> <h4 id=strategic_migration_from_fluxjl_to_luxjl ><a href="#strategic_migration_from_fluxjl_to_luxjl" class=header-anchor >Strategic Migration from Flux.jl to Lux.jl</a></h4> <p>SciML completed a major transition, fully replacing Flux.jl with Lux.jl in all documentation and examples, delivering significant improvements in error messages, correctness testing, and performance.</p> <h4 id=timeline_of_innovation ><a href="#timeline_of_innovation" class=header-anchor >Timeline of Innovation</a></h4> <p><strong>2018</strong>: Flux.jl adoption as core deep learning library</p> <p><strong>2020</strong>: FastChain development for specialized SciML performance needs in DiffEqFlux.jl</p> <p><strong>2021</strong>: SimpleChains.jl creation by PumasAI for small neural networks in UDE applications. Outperforms JAX by 15x with multithreading for sufficiently small systems.</p> <p><strong>2022</strong>: Lux.jl and LuxDL organization established based on FastChain fully explicit function interface, extending to all NNlib/Flux layers.</p> <p><strong>2023</strong>: Lux.jl maturation and full Flux replacement in SciML documentation, leading to major improvements in error messages, correctness testing, and performance.</p> <p><strong>2024</strong>: Joint Lux.jl/Enzyme.jl development for neural network support.</p> <h4 id=2024-2025_integrations ><a href="#2024-2025_integrations" class=header-anchor >2024-2025 Integrations</a></h4> <ul> <li><p>NeuralOperators.jl rewrite using Lux-based architectures</p> <li><p>Planned Lux.jl integration with Reactant.jl for automated kernel fusion</p> </ul> <h2 id=what_was_intentionally_skipped ><a href="#what_was_intentionally_skipped" class=header-anchor >What Was Intentionally Skipped</a></h2> <p>Due to time and scope constraints, many significant areas were not covered in this report:</p> <p><strong>Analysis libraries</strong> &#40;GlobalSensitivity.jl, StructuralIdentifiability.jl, EasyModelAnalysis.jl&#41;</p> <p><strong>Uncertainty quantification libraries</strong> &#40;SciMLExpectations.jl, PolyChaos.jl&#41;</p> <p><strong>Jump processes</strong> &#40;JumpProcesses.jl, Catalyst.jl, integrations in StochasticDiffEq.jl for tau-leaping&#41;</p> <p><strong>SciML PDE solvers</strong> &#40;HighDimPDE.jl, NeuralPDE.jl, NeuralOperators.jl&#41;</p> <p><strong>SciML architecture tools</strong> &#40;Surrogates.jl, DiffEqFlux.jl, DeepEquilibriumNetworks.jl, etc.&#41;</p> <p><strong>Core numerical libraries</strong> &#40;DataInterpolations.jl, ExponentialUtilities.jl, QuasiMonteCarlo.jl, etc.&#41;</p> <p><strong>High level interfaces</strong> &#40;SymbolicIndexingInterface, TermInterface, ArrayInterface, SciMLOperators, etc.&#41;</p> <p><strong>Parallelism / GPUs</strong></p> <p>Every library received only brief highlights for a year&#39;s worth of changes. Many connection and &quot;partner libraries&quot; &#40;BifurcationKit.jl, ControlSystems.jl, DynamicalSystems.jl, etc.&#41; that we work extensively with were not included.</p> <p><strong>Coverage</strong>: Approximately ¼ of the organization&#39;s state is represented here. Follow-up discussions will cover additional areas.</p> <hr /> <h2 id=looking_forward ><a href="#looking_forward" class=header-anchor >Looking Forward</a></h2> <p>As we progress through 2025, SciML continues pushing the boundaries of scientific computing, combining Julia&#39;s performance with cutting-edge research in scientific machine learning, symbolic computation, and differentiable programming. Our commitment to open science ensures these advanced capabilities remain accessible to the global research community while maintaining the performance and reliability required for production use.</p> <p>The SciML ecosystem has successfully demonstrated that high-performance scientific computing, modern machine learning, and user-friendly interfaces can coexist in a single, coherent framework. We remain at the forefront of computational science, driving innovation in scientific simulation and machine learning integration.</p> <!-- Footer--> <footer class="footer bg-light"> <div class=container > <div class=row > <div class="col-lg-6 h-100 text-center text-lg-start my-auto"> <ul class="list-inline mb-2"> <li class=list-inline-item ><a href="/community">Contact</a> <!-- <li class=list-inline-item ><a href="#!">Terms of Use</a> <li class=list-inline-item ><a href="#!">Privacy Policy</a> --> </ul> <p class="text-muted small mb-4 mb-lg-0">Website powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia</a> programming language. &copy; SciML 2022. All Rights Reserved.</p> <p class="text-muted small mb-4 mb-lg-0">Edit on <a href="https://github.com/SciML/sciml.ai">GitHub</a></p> </div> <div class="col-lg-6 h-100 text-center text-lg-end my-auto"> <ul class="list-inline mb-0"> <li class="list-inline-item me-4"> <a href="https://github.com/SciML"><i class="bi-github fs-3"></i></a> <li class="list-inline-item me-4"> <a href="https://twitter.com/SciML_Org"><i class="bi-twitter fs-3"></i></a> <li class=list-inline-item > <a href="https://www.linkedin.com/company/the-julia-language"><i class="bi-linkedin fs-3"></i></a> </ul> </div> </div> </div> </footer> </div>