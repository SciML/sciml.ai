<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <meta property="og:title" content="SciML: Open Source Software for Scientific Machine Learning"> <meta property="og:description" content="Open Source Software for Scientific Machine Learning"> <meta property="og:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta property="og:url" content="https://sciml.ai"> <meta name="twitter:title" content=SciML > <meta name="twitter:description" content="Open Source Software for Scientific Machine Learning"> <meta name="twitter:image" content="https://sciml.ai/assets/SciMLGitHubPreview.png"> <meta name="twitter:card" content=summary_large_image > <!-- Favicon--> <link rel=icon  type="image/x-icon" href="assets/favicon.png" /> <!-- Bootstrap icons--> <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel=stylesheet  type="text/css" /> <link rel=stylesheet  href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css"> <!-- Google fonts--> <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic" rel=stylesheet  type="text/css" /> <!-- Core theme CSS (includes Bootstrap)--> <link href="./css/styles.css" rel=stylesheet  /> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/styles.css"> <link rel=stylesheet  href="/css/hypertext.css"> <link rel=icon  href="/assets/favicon.png"> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-2"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-90474609-2'); </script> <title>LinearSolve.jl Autotuning: Community-Driven Algorithm Selection for Optimal Performance</title> <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.3/dist/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin=anonymous ></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin=anonymous ></script> <div class=container-fluid  id=top-alert > <div class="alert alert-dark alert-dismissible mb-0" role=alert > <p class=text-center > <a href="https://youtu.be/yHiyJQdWBY8">Check out the latest talk: "The Continuing Advancements of Scientific Machine Learning (SciML)"</a> </p> <!-- <button type=button  class=close  data-dismiss=alert  aria-label=Close > <span aria-hidden=true >&times;</span> --> </button> </div> </div> <header> <nav class="navbar navbar-expand-lg navbar-light"> <a class=navbar-brand  href="/">Home</a> <button class=navbar-toggler  type=button  data-toggle=collapse  data-target="#navbarSupportedContent" aria-controls=navbarSupportedContent  aria-expanded=false  aria-label="Toggle navigation"> <span class=navbar-toggler-icon ></span> </button> <div class="collapse navbar-collapse" id=navbarSupportedContent > <ul class="navbar-nav mr-auto"> <li class="nav-item active"> <a class=nav-link  href="https://docs.sciml.ai/">Documentation</a> <li class=nav-item > <a class=nav-link  href="/news/">News</a> <li class=nav-item > <a class=nav-link  href="/roadmap/">Roadmap</a> <li class=nav-item > <a class=nav-link  href="/citing/">Citing</a> <li class=nav-item > <a class=nav-link  href="/showcase/">Showcase</a> <li class=nav-item > <a class=nav-link  href="https://benchmarks.sciml.ai/">Benchmarks</a> <li class=nav-item > <a class=nav-link  href="https://github.com/SciML/">GitHub</a> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id=navbarDropdownMenuLink  role=button  data-toggle=dropdown  aria-haspopup=true  aria-expanded=false > Community </a> <div class=dropdown-menu  aria-labelledby=navbarDropdownMenuLink > <a class=dropdown-item  href="/community/">Community Home</a> <a class=dropdown-item  href="/governance/">Governance</a> <a class=dropdown-item  href="/challenge/">Challenges</a> <a class=dropdown-item  href="/dev/">Developer Programs</a> </div> <li class=nav-item > <a class=nav-link  href="https://juliahub.com/company/contact-us-sciml"> Commercial Support </a> <li class=nav-item > <a class=nav-link  href="https://numfocus.org/donate-to-sciml"><i class="bi bi-heart"></i> Donate</a> </ul> </div> </nav> </header> <div class=franklin-content ><h1 id=linearsolvejl_autotuning_community-driven_algorithm_selection_for_optimal_performance ><a href="#linearsolvejl_autotuning_community-driven_algorithm_selection_for_optimal_performance" class=header-anchor >LinearSolve.jl Autotuning: Community-Driven Algorithm Selection for Optimal Performance</a></h1> <p>Linear algebra operations form the computational backbone of scientific computing, yet choosing the optimal algorithm for a given problem and hardware configuration remains a persistent challenge. Today, we&#39;re excited to introduce <strong>LinearSolveAutotune.jl</strong>, a new community-driven autotuning system that automatically benchmarks and selects the best linear solver algorithms for your specific hardware configuration.</p> <h2 id=the_challenge_one_size_doesnt_fit_all ><a href="#the_challenge_one_size_doesnt_fit_all" class=header-anchor >The Challenge: One Size Doesn&#39;t Fit All</a></h2> <p>LinearSolve.jl provides a unified interface to over 20 different linear solving algorithms, from generic Julia implementations to highly optimized vendor libraries like Intel MKL, Apple Accelerate, and GPU-accelerated solvers. Each algorithm excels in different scenarios:</p> <ul> <li><p><strong>Small matrices &#40;&lt; 100×100&#41;</strong>: Pure Julia implementations like <code>RFLUFactorization</code> often outperform BLAS due to lower overhead</p> <li><p><strong>Medium matrices &#40;100-1000×1000&#41;</strong>: Vendor-optimized libraries like Apple Accelerate and MKL shine</p> <li><p><strong>Large matrices &#40;&gt; 1000×1000&#41;</strong>: GPU acceleration through Metal or CUDA becomes dominant</p> <li><p><strong>Sparse matrices</strong>: Specialized algorithms like KLU and UMFPACK are essential</p> </ul> <p>The optimal choice depends on matrix size, sparsity, numerical type, and critically, your specific hardware. An M2 MacBook Pro has very different performance characteristics than an AMD Threadripper workstation with an NVIDIA GPU.</p> <h2 id=enter_linearsolveautotune_community-powered_performance ><a href="#enter_linearsolveautotune_community-powered_performance" class=header-anchor >Enter LinearSolveAutotune: Community-Powered Performance</a></h2> <p>LinearSolveAutotune addresses this challenge through a unique approach: <strong>collaborative benchmarking with optional telemetry sharing</strong>. Here&#39;s how it works:</p> <h3 id=local_benchmarking ><a href="#local_benchmarking" class=header-anchor ><ol> <li><p>Local Benchmarking</p> </ol> </a></h3> <p>Run comprehensive benchmarks on your machine with a simple command:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> LinearSolve, LinearSolveAutotune

<span class=hljs-comment ># Run benchmarks across different matrix sizes and types</span>
results = autotune_setup()

<span class=hljs-comment ># View performance summary</span>
display(results)

<span class=hljs-comment ># Generate performance visualization</span>
plot(results)</code></pre> <p>The system automatically:</p> <ul> <li><p>Tests algorithms across matrix sizes from 5×5 to 15,000×15,000</p> <li><p>Benchmarks Float32, Float64, Complex, and BigFloat types</p> <li><p>Detects available hardware acceleration &#40;GPUs, vendor libraries&#41;</p> <li><p>Measures performance in GFLOPS for easy comparison</p> </ul> <h3 id=ol_start2_smart_recommendations ><a href="#ol_start2_smart_recommendations" class=header-anchor ><ol start=2 > <li><p>Smart Recommendations</p> </ol> </a></h3> <p>Based on your benchmarks, LinearSolveAutotune generates tailored recommendations for each scenario:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Example output from an Apple M2 system:</span>
<span class=hljs-comment ># ┌─────────────┬──────────────────────────────┐</span>
<span class=hljs-comment ># │ Size Range  │ Best Algorithm               │</span>
<span class=hljs-comment ># ├─────────────┼──────────────────────────────┤</span>
<span class=hljs-comment ># │ tiny (5-20) │ RFLUFactorization            │</span>
<span class=hljs-comment ># │ small       │ RFLUFactorization            │</span>
<span class=hljs-comment ># │ medium      │ AppleAccelerateLUFactorization │</span>
<span class=hljs-comment ># │ large       │ AppleAccelerateLUFactorization │</span>
<span class=hljs-comment ># │ huge        │ MetalLUFactorization         │</span>
<span class=hljs-comment ># └─────────────┴──────────────────────────────┘</span></code></pre> <h3 id=ol_start3_community_telemetry_optional ><a href="#ol_start3_community_telemetry_optional" class=header-anchor ><ol start=3 > <li><p>Community Telemetry &#40;Optional&#41;</p> </ol> </a></h3> <p>The real innovation lies in <strong>opt-in community telemetry</strong>. By sharing your benchmark results, you contribute to a growing database that helps improve algorithm selection heuristics for everyone:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Share your results with the community</span>
share_results(results)</code></pre> <p>This creates an automatic GitHub comment on our <a href="https://github.com/SciML/LinearSolve.jl/issues/725">results collection issue</a> with:</p> <ul> <li><p>Your hardware configuration &#40;CPU, GPU, available libraries&#41;</p> <li><p>Performance measurements across all algorithms</p> <li><p>System-specific recommendations</p> <li><p>Beautiful performance visualizations</p> </ul> <p><strong>Privacy First</strong>: The telemetry system:</p> <ul> <li><p>Only shares benchmark performance data</p> <li><p>Never collects personal information</p> <li><p>Requires explicit opt-in via <code>share_results&#40;&#41;</code></p> <li><p>Uses GitHub authentication for transparency</p> <li><p>All shared data is publicly visible on GitHub</p> </ul> <h2 id=real-world_impact_performance_gains_in_the_wild ><a href="#real-world_impact_performance_gains_in_the_wild" class=header-anchor >Real-World Impact: Performance Gains in the Wild</a></h2> <p>The community has already contributed benchmarks from diverse hardware configurations, revealing fascinating insights:</p> <h3 id=apple_silicon_optimization ><a href="#apple_silicon_optimization" class=header-anchor >Apple Silicon Optimization</a></h3> <p>On Apple M2 processors, we discovered that Apple&#39;s Accelerate framework delivers exceptional performance for medium-sized matrices, achieving <strong>750&#43; GFLOPS</strong> for large Float32 matrices. However, for tiny matrices &#40;&lt; 20×20&#41;, the pure Julia <code>RFLUFactorization</code> is <strong>3-5x faster</strong> due to lower call overhead.</p> <h3 id=gpu_acceleration_patterns ><a href="#gpu_acceleration_patterns" class=header-anchor >GPU Acceleration Patterns</a></h3> <p>Metal acceleration on Apple Silicon shows interesting threshold behavior:</p> <ul> <li><p>Below 500×500: CPU algorithms dominate</p> <li><p>500-5000×5000: Competitive performance</p> <li><p>Above 5000×5000: GPU delivers <strong>2-3x speedup</strong>, reaching over 1 TFLOP</p> </ul> <h3 id=complex_number_performance ><a href="#complex_number_performance" class=header-anchor >Complex Number Performance</a></h3> <p>For complex arithmetic, we found that specialized algorithms matter even more:</p> <ul> <li><p><code>LUFactorization</code> outperforms vendor libraries by <strong>2x</strong> for ComplexF32</p> <li><p>Apple Accelerate struggles with complex numbers, making pure Julia implementations preferable</p> </ul> <h2 id=using_the_results_automatic_algorithm_selection ><a href="#using_the_results_automatic_algorithm_selection" class=header-anchor >Using the Results: Automatic Algorithm Selection</a></h2> <p>The beauty of LinearSolve.jl&#39;s autotuning system is that you don&#39;t need to manually specify algorithms. The benchmark results from the community directly improve the default heuristics, so you simply use:</p> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> LinearSolve

<span class=hljs-comment ># Create your linear problem</span>
A = rand(<span class=hljs-number >100</span>, <span class=hljs-number >100</span>)
b = rand(<span class=hljs-number >100</span>)
prob = LinearProblem(A, b)

<span class=hljs-comment ># Just solve - LinearSolve automatically picks the best algorithm!</span>
sol = solve(prob)  <span class=hljs-comment ># Uses optimized heuristics based on community benchmarks</span></code></pre> <p>The autotuning results you and others share help LinearSolve.jl make intelligent decisions about:</p> <ul> <li><p>When to use pure Julia implementations vs vendor libraries</p> <li><p>Matrix size thresholds for GPU acceleration</p> <li><p>Special handling for complex numbers and sparse matrices</p> </ul> <p>By contributing your benchmark results with <code>share_results&#40;&#41;</code>, you&#39;re directly improving the default algorithm selection for everyone. The more diverse hardware configurations we collect, the smarter the automatic selection becomes.</p> <h2 id=performance_visualization_a_picture_worth_1000_benchmarks ><a href="#performance_visualization_a_picture_worth_1000_benchmarks" class=header-anchor >Performance Visualization: A Picture Worth 1000 Benchmarks</a></h2> <p>LinearSolveAutotune generates comprehensive performance visualizations showing:</p> <ul> <li><p><strong>Algorithm comparison plots</strong>: GFLOPS vs matrix size for each algorithm</p> <li><p><strong>Heatmaps</strong>: Performance across different size ranges and types</p> <li><p><strong>System information</strong>: Hardware details and available acceleration</p> </ul> <p>Here&#39;s an example from recent community submissions showing the dramatic performance differences across algorithms:</p> <pre><code class="julia hljs">Metal GPU vs CPU Performance (Apple M2)
┌────────────────────────────────────────────┐
│ <span class=hljs-number >1000</span> ┤ ▁▁▁▁▁▂▂▃▄▅▆▇█ Metal GPU        │
│      │                                      │
│  <span class=hljs-number >500</span> ┤     ▅▆▇██████ Apple Accelerate     │
│      │   ▂▄████▅▃▂▁                        │
│  <span class=hljs-number >100</span> ┤ ▆████▃▁      Generic LU            │
│      │████▁                                │
│   <span class=hljs-number >10</span> ┤██            RF Factorization      │
│      │                                     │
│    <span class=hljs-number >1</span> └────────────────────────────────────┘
│       <span class=hljs-number >10</span>   <span class=hljs-number >100</span>   <span class=hljs-number >1000</span>   <span class=hljs-number >10000</span>              │
│            <span class=hljs-built_in >Matrix</span> Size (n×n)               │
└────────────────────────────────────────────┘</code></pre> <h2 id=how_the_telemetry_system_works ><a href="#how_the_telemetry_system_works" class=header-anchor >How the Telemetry System Works</a></h2> <p>The telemetry system is designed with transparency and user control at its core:</p> <ol> <li><p><strong>Local Execution</strong>: All benchmarks run locally on your machine</p> <li><p><strong>Data Generation</strong>: Results are formatted as markdown tables and plots</p> <li><p><strong>Authentication</strong>: Uses GitHub OAuth for secure, transparent submission</p> <li><p><strong>Public Sharing</strong>: Creates a comment on a public GitHub issue</p> <li><p><strong>Community Analysis</strong>: Results feed into improved algorithm selection heuristics</p> </ol> <p>The collected data helps us:</p> <ul> <li><p>Identify performance patterns across different hardware</p> <li><p>Improve default algorithm selection</p> <li><p>Discover optimization opportunities</p> <li><p>Guide future development priorities</p> </ul> <h2 id=getting_started ><a href="#getting_started" class=header-anchor >Getting Started</a></h2> <p>Ready to optimize your linear algebra performance? Here&#39;s how to get started:</p> <pre><code class="julia hljs"><span class=hljs-comment ># Install the packages</span>
<span class=hljs-keyword >using</span> Pkg
Pkg.add([<span class=hljs-string >&quot;LinearSolve&quot;</span>, <span class=hljs-string >&quot;LinearSolveAutotune&quot;</span>])

<span class=hljs-comment ># Run comprehensive benchmarks</span>
<span class=hljs-keyword >using</span> LinearSolve, LinearSolveAutotune
results = autotune_setup(
    sizes = :all,           <span class=hljs-comment ># Test all size categories</span>
    types = [<span class=hljs-built_in >Float32</span>, <span class=hljs-built_in >Float64</span>, <span class=hljs-built_in >ComplexF64</span>],
    quality = :high,        <span class=hljs-comment ># Thorough benchmarking</span>
    time_limit = <span class=hljs-number >60.0</span>      <span class=hljs-comment ># Limit per-algorithm time</span>
)

<span class=hljs-comment ># Analyze your results</span>
display(results)
plot(results)

<span class=hljs-comment ># Optional: Share with the community</span>
share_results(results)</code></pre> <h2 id=the_road_ahead ><a href="#the_road_ahead" class=header-anchor >The Road Ahead</a></h2> <p>LinearSolveAutotune represents a new paradigm in scientific computing: <strong>community-driven performance optimization</strong>. By aggregating performance data across diverse hardware configurations, we can:</p> <ul> <li><p>Build better default heuristics that work well for everyone</p> <li><p>Identify performance regressions quickly</p> <li><p>Guide optimization efforts where they matter most</p> <li><p>Create hardware-specific algorithm recommendations</p> </ul> <p>We envision expanding this approach to other SciML packages, creating a comprehensive performance knowledge base that benefits the entire Julia scientific computing ecosystem.</p> <h2 id=join_the_community_effort ><a href="#join_the_community_effort" class=header-anchor >Join the Community Effort</a></h2> <p>The success of LinearSolveAutotune depends on community participation. Whether you&#39;re running on a laptop, workstation, or HPC cluster, your benchmarks provide valuable data that helps improve performance for everyone.</p> <p>Visit our <a href="https://github.com/SciML/LinearSolve.jl/issues/725">results collection issue</a> to see community submissions, and consider running the autotuning suite on your hardware. Together, we&#39;re building a faster, smarter linear algebra ecosystem for Julia.</p> <h2 id=acknowledgments ><a href="#acknowledgments" class=header-anchor >Acknowledgments</a></h2> <p>LinearSolveAutotune was developed as part of the SciML ecosystem with contributions from the Julia community. Special thanks to all early adopters who have shared their benchmark results and helped refine the system.</p> <hr /> <p><em>For more information, see the <a href="https://docs.sciml.ai/LinearSolve/stable/tutorials/autotune/">LinearSolve.jl documentation</a> and join the discussion on <a href="https://discourse.julialang.org/c/domain/models/21">Julia Discourse</a>.</em></p> <!-- Footer--> <footer class="footer bg-light"> <div class=container > <div class=row > <div class="col-lg-6 h-100 text-center text-lg-start my-auto"> <ul class="list-inline mb-2"> <li class=list-inline-item ><a href="/community">Contact</a> <!-- <li class=list-inline-item ><a href="#!">Terms of Use</a> <li class=list-inline-item ><a href="#!">Privacy Policy</a> --> </ul> <p class="text-muted small mb-4 mb-lg-0">Website powered by <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia</a> programming language. &copy; SciML 2022. All Rights Reserved.</p> <p class="text-muted small mb-4 mb-lg-0">Edit on <a href="https://github.com/SciML/sciml.ai">GitHub</a></p> </div> <div class="col-lg-6 h-100 text-center text-lg-end my-auto"> <ul class="list-inline mb-0"> <li class="list-inline-item me-4"> <a href="https://github.com/SciML"><i class="bi-github fs-3"></i></a> <li class="list-inline-item me-4"> <a href="https://twitter.com/SciML_Org"><i class="bi-twitter fs-3"></i></a> <li class=list-inline-item > <a href="https://www.linkedin.com/company/the-julia-language"><i class="bi-linkedin fs-3"></i></a> </ul> </div> </div> </div> </footer> </div>